\section{Kernel Discriminant Analysis against Masking}


\subsection{Linear Discriminant Analysis}

%\begin{frame}
%\frametitle{Contents}
%\begin{itemize}
%\item \important{Introduction to LDA: as a classifier, and as a feature extractor}
%\item Introduction to masking countermeasure and Kernel Discriminant Analysis as a feature extractor
%\item Motivations to apply deep learning techniques
%\item Convolutional Neural Networks and Data Augmentation to attack jitter-based countermeasure
%\end{itemize}
%\end{frame}



\begin{frame}
\frametitle{Model-based SNR and Fisher's Criterion}
\vspace*{-12pt}
\begin{block}{Signal-to-Noise Ratio (SNR)}
\begin{itemize}
\item Independent Noise Assumption: $\vaLeakVec[t] = \varphi_t(\sensRandVar)+B$

\item $\mmmXclass= \esperEst[\given{\vaLeakVec}{\sensRandVar = \sensVarGenValue}] = \frac{1}{\nbTracesPerClass}\sum_{i\colon \sensVar_i=\sensVarGenValue} \vLeakVec_i $ sample mean per class ($\approx \varphi_t(\sensRandVar)$)
\item $\varXclass = \varEst(\given{\vaLeakVec}{\sensRandVar = \sensVarGenValue}) = \frac{1}{\nbTracesPerClass-1}\sum_{i\colon \sensVar_i=\sensVarGenValue} (\vLeakVec_i - \mmmXclass)^2 $ sample variance per class ($\approx \mathrm{var}(\vec{B})$) 
\item \begin{equation*}\mathrm{SNR}(t) = \frac{\varEst(\mmmXclass[Z](t))}{\esperEst[\varXclass[Z](t)]} \qquad \textcolor{grey}{\frac{\text{\emph{variance inter-class}}}{\text{\emph{variance intra-class}}}} \end{equation*}
\end{itemize}
\end{block}

\begin{block}{Fisher's Criterion for Linear Dimensionality Reduction}

\begin{itemize}
\item $\SB = \sum_{\sensVarGenValue\in\sensVarSet}\nbTracesPerClass(\mmmXclass-\mmmX)(\mmmXclass-\mmmX)^\intercal $ (inter-class scatter matrix)
\item $\SW = \sum_{\sensVarGenValue\in\sensVarSet}\sum_{i=1\colon \sensVar_i=\sensVarGenValue}(\vLeakVec_i-\mmmXclass)(\vLeakVec_i-\mmmXclass)^\intercal$ (intra-class scatter matrix)
\item Fisher's criterion
 \begin{equation}\label{eq:LDA}
\hat{ \AAlpha}=\mathrm{argmax}_{\AAlpha} \frac{\AAlpha^\intercal \SB \AAlpha}{\AAlpha^\intercal \SW \AAlpha} \mbox{ ,}
 \end{equation}
 $\extract(\vLeakVec) = A\vLeakVec $ where $A=[\AAlpha_1, \dots, \AAlpha_{|\sensVarSet|-1}]$ eigenvectors of $\SW^{-1}\SB$
\end{itemize}

\end{block}
\end{frame}

\begin{frame}
\frametitle{LDA: an optimal binary linear classifier}
\begin{columns}
\begin{column}{.5\linewidth}
\includegraphics[width=\textwidth]{figures/dataNoProjection.pdf} 
\end{column}
\begin{column}{.6\linewidth}
\begin{itemize}
\item Classify data $\vLeakVec$ into 2 classes $\sensVarSet = \{\sensVarValue{1}, \sensVarValue{2}\}$
\pause
\item Generative model: $\pdf_{\given{\vaLeakVec}{\sensRandVar = \sensVarValue{j}}}(\vLeakVec)$, $\pdf_{\sensRandVar}(\sensVarValue{j})$ and $\pdf_{\vaLeakVec}(\vLeakVec)$
\pause
\item Posterior probabilities (via Bayes' theorem), then classify through the \emph{log-likelihood ratio}: $a = \log\left[\frac{\prob(\given{\sensVarValue{1}}{\vLeakVec})}{\prob(\given{\sensVarValue{2}}{\vLeakVec})}\right]]$ (boundary surface $a=0$)
\end{itemize}
\end{column}
\end{columns}
\pause
\begin{itemize}
\item Two assumptions about class-conditional densities: 
\begin{itemize}
\item Gaussian distributions with parameters $\mu_j, \Sigma_j$
\item Homoscedasticity: $\Sigma_j=\Sigma$ for all $j$
\end{itemize}
\pause
\item $\Rightarrow a = \vec{w}^\intercal \vLeakVec + w_0$ (linear decision boundary, $\vec{w}$ and $w_0$ functions of $\Sigma, \mu_j$)
\end{itemize}
\pause
\begin{block}{Generalised linear discriminative model}
\begin{equation}\label{eq:binary_linear_classifier}
\prob(\given{\sensVarValue{1}}{\vLeakVec}) = \sigma(\www^\intercal \vLeakVec + w_0)\mbox{ ,where }  \sigma(a)= \frac{1}{1+e^{-a}} \text{\emph{logistic sigmoid}}
\end{equation}
\end{block}
\end{frame}

\begin{frame}
\frametitle{LDA and Fisher Criterion}
\begin{columns}
\begin{column}{.5\linewidth}
\only<1>{\includegraphics[width=\textwidth]{figures/LDA_boundary.pdf}}
\only<2->{\includegraphics[width=\textwidth]{figures/LDAprojection.pdf} }
\end{column}
\begin{column}{.6\linewidth}
\begin{itemize}
\item LDA: linear decision boundary $a = \vec{w}^\intercal \vLeakVec + w_0$
\pause
\item Equivalently, project data onto $\vec{w}^\intercal \vLeakVec$ (orthogonally to the decision boudary), than classify by a real threshold (optimally $w_0$). \\
\end{itemize}
\end{column}
\end{columns}
\pause
\begin{itemize}
\item Two assumptions about class-conditional densities: 
\begin{itemize}
\item Gaussian distributions with parameters $\mu_j, \Sigma_j$
\item Homoscedasticity: $\Sigma_j=\Sigma$ for all $j$
\end{itemize}
\end{itemize}

\begin{block}{Fact, abuse and preference for the dimensionality reduction formulation}
\begin{itemize}
\item When LDA assumptions are met, the solution $\AAlpha_1$ of the Fisher's criterion is orthogonal to $\vec{w}$. 
\item assumption not required
\item naturally multi-class
\item optimal dimensionality reduction for template attack
\end{itemize}
\end{block}

\end{frame}

\begin{frame}
\frametitle{Linear separability}

\only<1>{\includegraphics[width=0.5\textwidth]{figures/LDA_non_linearly.pdf}\\ }
LDA: linear decision boundary $a = \vec{w}^\intercal \vLeakVec + w_0$ ($\vec{w} = \Sigma^{-1}(\mu_1-\mu_2)$)
%
\begin{block}{}
\begin{huge}
\centering What if $\mu_1 = \mu_2$? 
\end{huge}
\end{block}


%\only<2>{\includegraphics[width=.4\textwidth]{figures/D2.png}\\}
%\only<3>{\includegraphics[width=.4\textwidth]{figures/D2.png}\includegraphics[width=.4\textwidth]{figures/D2_useless1.png}\\}
%\only<4>{\includegraphics[width=.4\textwidth]{figures/D2.png}\includegraphics[width=.4\textwidth]{figures/D2_useless2.png}\\}
%\only<5>{\includegraphics[width=.4\textwidth]{figures/D2.png}\includegraphics[width=.4\textwidth]{figures/D2_useless3.png}\\}
%\only<6>{\includegraphics[width=.4\textwidth]{figures/D2.png}\includegraphics[width=.4\textwidth]{figures/D2_useless4.png}\\}
%\uncover<2->{$f(z) = \esper[\vaLeakVec\lvert Z=z]$ is constant\\}
%\uncover<3->{$\esper[A\vaLeakVec\lvert Z=z]$ is constant as well $\Rightarrow$ Projecting extractors can't help! }


\end{frame}


\begin{frame}
\frametitle{Dimensionality reduction in presence of masking}
\begin{block}{$(d-1)$th-order Sharing (or Masking)} 
Split each sensitive $Z$ into shares  $Z = M_1 \star \dots \star M_d$ \\
with \only<1-2>{\textcolor{black}{$M_1, \dots , M_{d-1}$}}\only<3->{\textcolor{red}{$M_1, \dots , M_{d-1}$}} random shares (or masks) \only<3->{\textcolor{red}{UNKNOWN}} \\ and $M_d = Z \star M_1^{-1}\star \dots \star M_{d-1}^{-1}$ \\
Software implementations: shares are handled at different time samples $$t_1,\dots, t_d \qquad \only<1-2>{\mbox{\textcolor{white}{UNKNOWN}}} \only<3->{\mbox{\textcolor{red}{UNKNOWN}}} $$\\
$\Rightarrow$ each time sample is independent from $Z$.\\
\end{block}
\uncover<2->{
\begin{columns}
\begin{column}{.5\textwidth}
\includegraphics[width=\textwidth]{figures/D2.pdf}
\end{column}
\begin{column}{.5\textwidth}
\begin{block}{}
Toy example: 2 time samples, 1-bit data
\begin{itemize}
\item[$t_1$:] $M + n$, $n\sim \mathcal{N}(0,0.1)$ 
\item[$t_2$:] $M\oplus Z + n$ (Boolean masking)
\end{itemize}
\end{block}
\end{column}
\end{columns}
}
\end{frame}



\begin{frame}
\frametitle{In high-dimensional traces}
$f(z) = \esper[\vaLeakVec\lvert Z=z]$ is constant $\Rightarrow$ SNR, SOD, t-statistic are null!
\includegraphics[width = 0.9\textwidth]{figures/SNR_2order_new.png} \\
\pause
$\esper[A\vaLeakVec\lvert Z=z]$ is constant as well 

\begin{block}{Higher-Order Side-Channel Attacks}
Exploit $f(z) = \esper\left[\vaLeakVec[t_1]\vaLeakVec[t_2]\dots\vaLeakVec[t_d]\lvert Z=z\right]$ non-constant.
 \end{block}

\begin{block}{Necessary condition}
\cite{carlet2014achieving} The statistic extracted from measurements must contain  $$\vaLeakVec[t_1]\vaLeakVec[t_2]\dots\vaLeakVec[t_d]$$
\end{block} 
\end{frame}

\begin{frame}
\frametitle{PoIs Research}
How to detect the $d$-tuple $t_1,\dots, t_d$? 
\begin{block}{A lacking literature}
\begin{itemize}
\item many HO attacks papers assume the knowledge of $t_1,\dots, t_d$
\item PoI research exploiting the random shares knowledge (back to unprotected case using $M_1,\dots , M_d$ instead of $Z$)
\item naive strategy: infer over all possible $d$-tuples 
\item Hand selection via educated guess \cite{Oswald2006}
\end{itemize}
\end{block}

\begin{block}{Generalizing extractors for higher-order context}
\begin{itemize}
\item Selecting extractors $\longrightarrow$ Projection Pursuits \cite{PP}
\item Projecting extractors $\longrightarrow$ Kernel Discriminant Analysis [CARDIS '16]
\end{itemize}
\end{block}
\end{frame}

\subsection{Kernel Discriminant Analysis}

\begin{frame}
[fragile]
\frametitle{KDA: the purpose}

\begin{block}{Problem}
Useful statistics lie in a high-dimensional \emph{feature} space: 
$$\featureSpace = \mathbb{R}^{{D+d-1}\choose{d}}$$
(all $d$th-degree monomials in the trace coordinates)
\end{block}

\begin{figure}
\centering
{
\begin{tikzpicture}
\matrix (m) [matrix of math nodes, row sep=3em,
column sep=3em, text height=1.5ex, text depth=0.25ex]
{ \mathbb{R}^\traceLength & \featureSpace & \mathbb{R}^\newTraceLength \\};
\path[->]
(m-1-1) edge node[above] {$\Phi$} (m-1-2);
         %edge [bend left=30] (m-2-2)
         %edge [bend right=15] (m-2-2);
\path[->]
($(m-1-2.north east)-(0,0.1)$) edge node[above] {$\extract^{\mathrm{PCA}}$} ($(m-1-3.north west)-(0,0.1)$);
\path[->]
($(m-1-2.south east)+(0,0.15)$) edge node[below] {$\extract^{\mathrm{LDA}}$} ($(m-1-3.south west)+(0,0.15)$);

\path[->]
(m-1-1) edge [bend left=50] node[above] {$\extract^{\mathrm{KPCA}}$} (m-1-3)
(m-1-1) edge [bend right=50] node[below] {$\extract^{\mathrm{KDA}}$} (m-1-3);

\node[text width=4cm] at (4,0) {\begin{footnotesize}
$\Phi$ non-linear function
\end{footnotesize}};
\end{tikzpicture} 
}


\end{figure}

\begin{block}{What KDA provides}
KDA allows performing LDA in $\featureSpace$, remaining in $\mathbb{R}^{D}$.
\end{block}

\end{frame}



\begin{frame}
%[fragile]
\frametitle{KDA: an intuition}
%\vspace{-30pt}
%\begin{figure}
%\centering
%{
%\begin{tikzpicture}
%\matrix (m) [matrix of math nodes, row sep=3em,
%column sep=3em, text height=1.5ex, text depth=0.25ex]
%{ \mathbb{R}^\traceLength & \featureSpace & \mathbb{R}^\newTraceLength \\};
%\path[->]
%(m-1-1) edge node[above] {$\Phi$} (m-1-2);
%         %edge [bend left=30] (m-2-2)
%         %edge [bend right=15] (m-2-2);
%\path[->]
%($(m-1-2.north east)-(0,0.1)$) edge node[above] {$\extract^{\mathrm{PCA}}$} ($(m-1-3.north west)-(0,0.1)$);
%\path[->]
%($(m-1-2.south east)+(0,0.15)$) edge node[below] {$\extract^{\mathrm{LDA}}$} ($(m-1-3.south west)+(0,0.15)$);
%
%\path[->]
%(m-1-1) edge [bend left=50] node[above] {$\extract^{\mathrm{KPCA}}$} (m-1-3)
%(m-1-1) edge [bend right=50] node[below] {$\extract^{\mathrm{KDA}}$} (m-1-3);
%
%\node[text width=4cm] at (4,0) {\begin{footnotesize}
%$\Phi$ non-linear function: all $d$th-degree monomials of time samples
%\end{footnotesize}};
%\end{tikzpicture} 
%}
%
%
%\end{figure}
\vspace{-15pt}
\begin{block}{}
Toy example: 2 time samples, 1-bit data
\begin{itemize}
\item[$t_1$:] $M + n$, $n\sim \mathcal{N}(0,0.1)$ 
\item[$t_2$:] $M\oplus Z + n$ (Boolean masking)
\end{itemize}
\end{block}

\begin{columns}
\begin{column}{.5\textwidth}
\uncover<1->{
\only<1-3>{
\begin{figure}
\includegraphics[width=\textwidth]{figures/D2.pdf}
\end{figure}}

\only<4>{
\begin{figure}
\includegraphics[width=\textwidth]{figures/D2_proj.pdf}
\end{figure}
\centering
\textcolor{violet}{\Large{KDA} \\ \large{remains in $\mathbb{R}^D$}}
}
%\only<5->{
%\begin{figure}
%\includegraphics[width=\textwidth]{figures/new_example_KDA.pdf} 
%\end{figure}}
}

\end{column}

\begin{column}{.5\textwidth}
\uncover<2->{
\only<1-2>{
\begin{figure}
\includegraphics[width=\textwidth]{figures/D3.pdf} 
\end{figure}
\centering
\textcolor{violet}{\Large{ $\Phi\colon \mathbb{R}^D \rightarrow \mathbb{R}^{{D+d-1}\choose{d}}$  \\ $\Phi(t_1,t2) = (t_1^2, t_2^2, kt_1t_2)$ }}
}
\only<3>{
\begin{figure}
\includegraphics[width=\textwidth]{figures/D3_proj.pdf} 
\end{figure}
\centering
\textcolor{violet}{\Large{$\Phi \longrightarrow$ LDA}}
}
\only<4>{
\begin{figure}
\includegraphics[width=\textwidth]{figures/D3_proj.jpg} 
\end{figure}
}}

%\only<5->{
%\begin{block}{KDA procedure}
%\begin{itemize}
%\item Training $\rightarrow \textcolor{violet}{\boldsymbol{\nu}_\ell}$ , $C$ vectors of coefficients ($\ell = 1,\dots , C$)
%\item<6-> \emph{Compare} the new trace with all training traces $K(\textcolor{red}{\sss[z_i]{i}}, \textcolor{green}{\sss[]{}})$, $K(\textcolor{blue}{\sss[z_i]{i}}, \textcolor{green}{\sss[]{}})$
%\item<7-> \emph{KDA projection} \begin{equation}\label{eq:projection}
%\extract^{\mathrm{KDA}}_{\ell}(\vec{x}) = \sum_{i=1}^{\NPoI}\textcolor{violet}{\boldsymbol{\nu}_\ell[i]}K(\sss[z_i]{i}, \textcolor{green}{\sss[]{}}) \mbox{ .}
%\end{equation} 
%\end{itemize}
%
%\end{block}
%}

\end{column}
\end{columns}
%\uncover<5>{
%\begin{block}{Apply KDA extractor}
%\begin{equation}\label{eq:projection}
%\extract^{\mathrm{KDA}}_{\ell}(\vec{x}) = \sum_{i=1}^{\NPoI}\boldsymbol{\nu}_\ell[i]K(\textcolor{red}{\sss[z_i]{i}}, \textcolor{green}{\sss[]{}}) \mbox{ .}
%\end{equation} 
%
%
%\end{block}
%}
\end{frame}


\begin{frame}
\frametitle{Kernel Function}

\begin{block}{Kernel Function}
$K\colon\mathbb{R}^D\times \mathbb{R}^D \rightarrow \mathbb{R} \nonumber$
\begin{equation}\label{eq:kernelProperty}
K(\sss[]{i},\sss[]{j}) = \Phi(\sss[]{i})\cdot \Phi(\sss[]{j})
\end{equation}
\end{block}

\begin{block}{Polynomial Kernel Function}
$K(\sss[]{i},\sss[]{j}) = (\sss[]{i} \cdot \sss[]{j})^d \qquad \leftrightarrow \qquad\Phi:\mathbb{R}^D \rightarrow \mathcal{F}\subset \mathbb{R}^{{D+d-1}\choose{d}} $ all $d$th-degree monomials


\end{block}
\uncover<2->{
\begin{block}{Example: $2$nd Degree Polynomial Kernel Function}
Toy example: $D=2 \longrightarrow \sss[]{i} = [a,b]$ , $\sss[]{j} = [c,d]$
\begin{equation*}
K(\sss[]{i},\sss[]{j}) = (ac + bd)^2 = a^2c^2 + 2abcd + b^2d^2
\end{equation*}
\only<2>{
\textcolor{white}{$K \longleftrightarrow$ $\Phi\colon \Bbb{R}^2\rightarrow\Bbb{R}^3$
\begin{equation*}
\Phi(u,v) =  [u^2, \sqrt{2}uv, v^2]
\end{equation*}}
}
\uncover<3->{
$K \longleftrightarrow$ $\Phi\colon \Bbb{R}^2\rightarrow\Bbb{R}^3$
%\begin{align}
%&\Phi \colon \mathbb{R}^2 \rightarrow \mathbb{R}^3\\
%&\Phi \colon [a,b]\mapsto [a^2, \sqrt{2}ab, b^2]\\
%&\Phi \colon [c,d]\mapsto [c^2, \sqrt{2}cd, d^2].
%\end{align}
\vspace{-5pt}
\only<3>{
\begin{equation*}
\Phi(u,v) =  [u^2, \sqrt{2}uv, v^2]
\end{equation*}}
\only<4>{
\begin{equation*}
\Phi(a,b) =  \textcolor{magenta}{[a^2, \sqrt{2}ab, b^2]}
\end{equation*}
}
\only<5>{
\begin{equation*}
\Phi(c,d) =  \textcolor{blue}{[c^2, \sqrt{2}cd, d^2]}
\end{equation*}
}
}

\only<3>{
\textcolor{white}{$\Phi(\sss[]{i})\cdot \Phi(\sss[]{j}) = {a^2}{c^2} + {\sqrt{2}ab}{\sqrt{2}cd} + {b^2}{d^2} = K(\sss[]{i},\sss[]{j})$}}

\only<4>{
$\Phi(\sss[]{i})\cdot \Phi(\sss[]{j}) = \textcolor{magenta}{a^2}\textcolor{white}{c^2} + \textcolor{magenta}{\sqrt{2}ab}\textcolor{white}{\sqrt{2}cd} + \textcolor{magenta}{b^2}\textcolor{white}{d^2} = K(\sss[]{i},\sss[]{j})$}

\only<5>{
$\Phi(\sss[]{i})\cdot \Phi(\sss[]{j}) = \textcolor{magenta}{a^2}\textcolor{blue}{c^2} + \textcolor{magenta}{\sqrt{2}ab}\textcolor{blue}{\sqrt{2}cd} + \textcolor{magenta}{b^2}\textcolor{blue}{d^2} = K(\sss[]{i},\sss[]{j})$}

\end{block}
}

\end{frame}



\begin{frame}

\frametitle{KDA - the training}
\begin{center}
\only<1>{
Between-class (inter-class) Scatter Matrix }
\only<2>{
Within-class (inter-class) Scatter Matrix }
\only<3>{
Eigenvector problem}
\only<4>{
New trace projection}

\end{center}

\begin{columns}
\begin{column}{.5\textwidth}
\uncover<3->{Computational Complexity $O(D^3)$}
\begin{block}{LDA}
\begin{small}
\begin{itemize}
\item $\SB = \sum_{\sensVar\in\sensVarSet}\numTraces(\mmmXclass-\mmmX)(\mmmXclass-\mmmX)^\intercal$ 
\uncover<2->{\item $\SW = \sum_{\sensVar\in\sensVarSet}\sum_{i=1}^{\numTraces}(\sss{i}-\mmmXclass)(\sss{i}-\mmmXclass)^\intercal$ }
\uncover<3->{\item $\AAlpha_i$ eigenvectors of $\SW^{-1}\SB$ [$D\times D$]}
\uncover<4->{\item $\extract^{LDA}_{\ell}(\xxx) = \sum_{i=1}^D \AAlpha_{\ell}[i]\xxx[i] $}
\end{itemize}
\end{small}
\end{block}
\end{column}

\begin{column}{.55\textwidth}
\uncover<3->{\only<3-4>{Computational Complexity $O(N^3)$}}
\begin{block}{KDA}
\begin{small}
\begin{itemize}

\item $\MMM = \sum_{\sensVar\in\sensVarSet}\numTraces(\MMMclass - \MMMT)(\MMMclass-\MMMT)^\intercal$ \footnotemark

\uncover<2->{\item $\NNN = \sum_{\sensVar\in\sensVarSet}\kernelMatrix_\sensVar(\III - \III_{\numTraces})\kernelMatrix_\sensVar^\intercal$ \only<2-4>{\footnotemark} }

\uncover<3->{\item $\nununu_i$ eigenvectors of $\NNN^{-1}\MMM$ [$N\times N$]}
\uncover<4->{\item \only<4>{$\extract^{\mathrm{KDA}}_{\ell}(\vec{x}) = \sum_{i=1}^{\NPoI}\nununu_\ell[i]K(\sss[z_i]{i}, \sss[]{}) $}}
\end{itemize}
\end{small}
\end{block}
\end{column}
\end{columns}
\centering
\vspace{8pt}

\footnotetext[1]{$\MMMclass$ and $\MMMT$ are two $N$-sized column vectors whose entries are given by: $\MMMclass[z][j] = \frac{1}{\numTraces}\sum_{i:z_i=z}^{\numTraces}K(\sss[z_j]{j},\sss[z_i]{i}), \qquad
\MMMT[j] = \frac{1}{\NPoI}\sum_{i=1}^{\NPoI}K(\sss[z_j]{j},\sss[z_i]{i}) \mbox{ .}$}

\uncover<2->{\footnotetext[2]{$\III$ is a $\numTraces\times \numTraces$ identity matrix, $\III_{\numTraces}$ is a $\numTraces\times \numTraces$ matrix with all entries equal to $\frac{1}{\numTraces}$ and $\kernelMatrix_{\sensVar}$ is the $\NPoI\times \numTraces$ sub-matrix of $\kernelMatrix = (K(\sss[z_i]{i},\sss[z_j]{j}))_{\substack{i=1,\dots,\numTraces[] \\ j=1,\dots,\numTraces[]}}$ storing only columns indexed by the indices $i$ such that $z_i=z$}}
\end{frame}


\subsection{Experimental Results}


\begin{frame}
\frametitle{Experimental setup}
Target device and acquisitions: 

\begin{itemize}
\item 8-bit AVR microprocessor Atmega328P
\item power-consumption acquired via the ChipWhisperer \cite{o2014chipwhisperer} platform
\item $D = 200$, $4$ clock-cycles are selected
\end{itemize}

Sensitive variable: $Z = \mathrm{Sbox_{AES}}(P\oplus K^{\star})$\\
%Leakage:
%\begin{footnotesize}
%
%\begin{itemize}
%\item[$d=2$] $M_1$, $Z\oplus M_1$
%\item[$d=3$] $M_1$, $M_2$, $Z\oplus M_1\oplus M_2$  
%\item[$d=4$] $M_1$,$M_2$,$M_3$, $Z\oplus M_1\oplus M_2 \oplus M_3$  
%\end{itemize}
%\end{footnotesize}
\begin{figure}
\includegraphics[width=0.8\textwidth]{figures/one_trace.pdf}
\end{figure}
%Attack: 
%\begin{itemize}
%\item \emph{KDA profiling traces} ($\sim 9000$) $\rightarrow \extract^{KDA}\colon \mathbb{R}^{200}\rightarrow \mathbb{R}^2$ separating classes
%\item \emph{TA profiling traces} ($\sim 20000$) $\rightarrow$ Gaussian templates per class
%\item \emph{attack traces} $\rightarrow$ attack via maximum likelihood
%\end{itemize}


\end{frame}

%
%
%\begin{frame}
%[fragile]
%\frametitle{Attack Scenario}
%\begin{block}{KDA training phase}
%Build the KDA extractor $\extract^{KDA}$
%\end{block}
%
%\uncover<2->{
%\begin{block}{Gaussian Template Attack}
%\begin{itemize}
%\item<2-> Preprocess profiling and attack traces
%\item<3-> Compute Gaussian templates
%\item<4-> Attack via maximum likelihood
%\end{itemize}
%\end{block}
%}
%
%\begin{figure}
%\centering
%{
%\begin{tikzpicture}[xscale=0.8,yscale=0.4]
%\uncover<2->{
%\node at (1,4.5) [text width=2cm] {\begin{footnotesize}TA profiling traces\end{footnotesize}};
%\node at (4,4.5)[text width=2cm] {\begin{footnotesize}TA attack traces\end{footnotesize}};
%\draw [->] (1,4) -- (1,3);
%\draw [->] (4,4) --(4,3);
%\node [left] at (1,3.5) {$\mathbb{R}^{200}$};
%\node [right] at (4,3.5) {$\mathbb{R}^{200}$};}
%\draw [blue] (0,2) rectangle(5,3);
%\node at (2.5,2.5) {\begin{large}$\extract^{KDA}$\end{large}};
%\uncover<2->{
%\draw [->] (1,2) -- (1,1);
%\draw [->] (4,2) --(4,1);
%\node [left] at (1,1.5) {$\mathbb{R}^{2}$};
%\node [right] at (4,1.5) {$\mathbb{R}^{2}$};
%\node at (1,0.5)[text width=2cm] {\begin{footnotesize}TA profiling traces\end{footnotesize}};
%\node at (4,0.5)[text width=2cm] {\begin{footnotesize}TA attack traces\end{footnotesize}};}
%\uncover<3->{
%\draw [->] (1,0) -- (1,-2);
%\draw [yellow] (0,-3) rectangle(2,-2);
%\node at (1.5,-2.5)[text width=2cm] {\begin{footnotesize}Templates\end{footnotesize}};}
%\uncover<4>{
%\draw [->] (2,-2.5) -- (3,-2.5);
%\draw [->] (4,0) -- (4,-1.5);
%\draw [red] (4,-2.5) circle(1);
%\node at (4.6,-2.5)[text width=2cm] {\begin{footnotesize}Matching\end{footnotesize}};
%\draw [->] (5,-2.5) -- (6,-2.5);
%\node at (7.5,-2.5)[text width=2cm] {$K^\star$};}
%\end{tikzpicture} 
%}
%
%
%\end{figure}
%
%
%\end{frame}



\begin{frame}
\frametitle{Efficiency/Accuracy trade-off}

KDA training set size fixed ($\sim$ 9000 traces) to control efficiency\\

\begin{block}{Adjust the number of classes to gain in accuracy}
$Z = 0,\dots, 255 $\\
\centering
\begin{tabular}{|c|c|c|}
\hline 
\textbf{Model} & \textbf{number of classes} & \textbf{traces per class} \\
Value & 256 & 35\\
HW & 9 & 1000 \\
HW $<,>,= 4$ & 3 & 3000 \\
HW $\leq,> 4$ & 2 & 4500 \\
\hline
\end{tabular}

\end{block}
\end{frame}







\begin{frame}
\frametitle{Attack of second order}
\vspace{-10pt}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2016/2order_classes_TA.pdf} 
\uncover<2->{\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2016/good_coeffs.png} }
\begin{block}{Implicit coefficients}

\begin{equation}
\extract^{\mathrm{KDA}}_{\ell}(\sss[]{}) = \sum_{i=1}^{\NPoI}\nununu_\ell[i]K(\sss[z_i]{i}, \sss[]{})= \sum_{j=1}^\traceLength \sum_{k=1}^\traceLength[ \underbrace{(\sss[]{}[j]\sss[]{}[k])}_{\parbox[c]{2cm}{\centering{multiplied \\ time samples}}}\underbrace{(\sum_{i=1}^{\NPoI}\boldsymbol{\nu}_{\ell}[i] \sss[]{i}[j]\sss[]{i}[k])}_{\mbox{implicit  coefficients}}]
\end{equation}


\end{block}
$d=2 \quad \longrightarrow \quad  {{200+2-1}\choose{2}} = 20.100$ implicit coefficients
\end{frame}

\begin{frame}
\frametitle{Third and Fourth Order}
\begin{figure}
\includegraphics[width=0.5\textwidth]{figures/3order_only_9.pdf}
\includegraphics[width=0.5\textwidth]{figures/4order_only_9.pdf}
\end{figure}


\begin{itemize}
\item $d=3 \quad \longrightarrow \quad {{200+3-1}\choose{3}} = 1.353.400$ implicit coefficients
\item  $d=4 \quad \longrightarrow \quad {{200+4-1}\choose{4}} = 68.685.050$ implicit coefficients
\end{itemize}

Same time of execution of the KDA algorithm!

\end{frame}

%\begin{frame}
%\frametitle{Multi-class trade-off}
%
%With a fixed training set size, reducing the number of classes to separate (via non injective models like Hamming Weight, ...) decreases the class approximation error
%\begin{figure}
%\includegraphics[width=0.7\textwidth]{figures/3order_new.pdf} 
%\end{figure}
%
%%\begin{itemize}
%%\item $d=2$, features extracted from a ${{200}\choose{2}} = 19.900$-sized space
%%\item $d=3$, features extracted from a ${{200}\choose{3}} = 1.313.400$-sized space
%%\end{itemize}
%\end{frame}


%\begin{frame}
%\frametitle{Asymmetric KDA/profiling approach}
%
%If KDA separates 2 classes, it has localised the PoIs. \\
%TA will work and will be more efficient if performed over more classes \\(\emph{e.g.} 9 classes, Hamming Weight)
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{figures/3order_2_9.pdf}
%\includegraphics[width=0.5\textwidth]{figures/4order_2_9.pdf}
%\end{figure}
%
%
%\begin{itemize}
%\item $d=3$, features extracted from a ${{200}\choose{3}} = 1.313.400$-sized space
%\item  $d=4$, features extracted from a ${{200}\choose{4}} = 64.684.950$-sized space
%\end{itemize}
%\end{frame}

\begin{frame}
\frametitle{Qualitative comparison with PP}
\vspace{-15pt}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering 
Second Order
\end{column}
\begin{column}{0.5\textwidth}
\centering 
Third Order
\end{column}
\end{columns}
\begin{figure}
\includegraphics[width=0.45\textwidth]{figures/secondOrderPP.pdf} 
\includegraphics[width=0.45\textwidth]{figures/thirdOrderPP.pdf}\\
\only<1>{\includegraphics[width=0.45\textwidth]{../Figures/CARDIS2016/good_coeffs.png}}
\end{figure}
%\only<2>{
%\begin{block}{Fair/Unfair Comparison}
%\hfill
%\begin{itemize}
%\item \textbf{Fair:} the same \emph{KDA profiling traces} are used to run PP algorithm
%\item \textbf{Unfair:} PP does not suffer (but takes advantage of) the increase of the number of profiling traces 
%\end{itemize}
%
%\end{block}
%}
\end{frame}
\begin{frame}
\frametitle{Conclusions on KDA}
\begin{block}{Strong points}
\begin{itemize}
\item KDA is suitable to attack $(d-1)$th-order masking
\item Choice of the $d$-th degree polynomial kernel function
\item KDA computational complexity is independent from the order $d$
\item Tested and effective on a real case, positively compared to PP 
\end{itemize}
\end{block}

\begin{block}{Limits and drawbacks}
%\begin{itemize}
%\item Memory-based $+$ $O(N^3)$ complexity $\rightarrow$ Non-scalability to big training set $\rightarrow$ not suitable for highly-noisy higher-order masked signals
%\item Regularization hyper-parameter $\mu$ (and slow \emph{validation})
%\end{itemize}
\end{block}



\end{frame}
\begin{frame}

\frametitle{KDA - limits and drawbacks}


\begin{columns}
\begin{column}{.5\textwidth}
Computational Complexity $O(D^3)$
\begin{block}{LDA}
\begin{small}
\begin{itemize}
\item $\SB = \sum_{\sensVar\in\sensVarSet}\numTraces(\mmmXclass-\mmmX)(\mmmXclass-\mmmX)^\intercal$ 
\item $\SW = \sum_{\sensVar\in\sensVarSet}\sum_{i=1}^{\numTraces}(\sss{i}-\mmmXclass)(\sss{i}-\mmmXclass)^\intercal$ 
\item $\AAlpha_i$ eigenvectors of $\SW^{-1}\SB$ [$D\times D$]
\item $\extract^{LDA}_{\ell}(\xxx) = \sum_{i=1}^D \AAlpha_{\ell}[i]\xxx[i] $
\end{itemize}
\end{small}
\end{block}
\end{column}

\begin{column}{.55\textwidth}
\textcolor{red}{Computational Complexity $O(N^3)$}
\begin{block}{KDA}
\begin{small}
\begin{itemize}

\item $\MMM = \sum_{\sensVar\in\sensVarSet}\numTraces(\MMMclass - \MMMT)(\MMMclass-\MMMT)^\intercal$ \footnotemark

\item $\NNN = \sum_{\sensVar\in\sensVarSet}\kernelMatrix_\sensVar(\III - \III_{\numTraces})\kernelMatrix_\sensVar^\intercal$ \textcolor{red}{$+ \mu\III$}

\item $\nununu_i$ eigenvectors of $\NNN^{-1}\MMM$ [$N\times N$]
$\extract^{\mathrm{KDA}}_{\ell}(\vec{x}) =$ \textcolor{red}{$\sum_{i=1}^{\NPoI}$}$\nununu_\ell[i]$\textcolor{red}{$K(\sss[z_i]{i}, \sss[]{})$} 
\end{itemize}
\end{small}
\end{block}
\end{column}
\end{columns}
\centering
\vspace{8pt}
\textcolor{red}{$\mu$ regularization parameter} $\qquad$ \textcolor{red}{\emph{Memory-based} machine}\\
\textcolor{red}{Does not allow the localisation of PoIs}

\footnotetext[1]{$\MMMclass$ and $\MMMT$ are two $N$-sized column vectors whose entries are given by: $\MMMclass[z][j] = \frac{1}{\numTraces}\sum_{i:z_i=z}^{\numTraces}K(\sss[z_j]{j},\sss[z_i]{i}), \qquad
\MMMT[j] = \frac{1}{\NPoI}\sum_{i=1}^{\NPoI}K(\sss[z_j]{j},\sss[z_i]{i}) \mbox{ .}$}

\uncover<2->{\footnotetext[2]{$\III$ is a $\numTraces\times \numTraces$ identity matrix, $\III_{\numTraces}$ is a $\numTraces\times \numTraces$ matrix with all entries equal to $\frac{1}{\numTraces}$ and $\kernelMatrix_{\sensVar}$ is the $\NPoI\times \numTraces$ sub-matrix of $\kernelMatrix = (K(\sss[z_i]{i},\sss[z_j]{j}))_{\substack{i=1,\dots,\numTraces[] \\ j=1,\dots,\numTraces[]}}$ storing only columns indexed by the indices $i$ such that $z_i=z$}}
\end{frame}
