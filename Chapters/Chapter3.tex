% Chapter Template

\chapter{Introduction to Machine Learning} % Main chapter title

\label{ChapterIntroML}


\section{Basic Concepts of Machine Learning}
Machine Learning (ML) is a field of computer science that groups a variety of  methods whose aim is giving computers the ability of \emph{learning} without being explicitly programmed. The more cited definition of \emph{learning} has been provided by Mitchell in 1997 \cite{Mitchell1997}: \enquote{ A computer program is said to learn from experience E with respect to some task T and performance measure P, if its performance on T, as measured by P, improves with experience E.} \\
Machine Learning groups a variety of methods essentially coming from applied statistics, and characterized by an increased emphasis on the use of computers to statistically estimate complicated functions. This allows Machine Learning to tackle tasks that would be too difficult to solve with fixed programs written and designed by human being. A Machine Learning algorithm is often said to \enquote{learn from data}, in the sense that it is able to improve a computer program's performance at some task via a data observation experience.

\subsection{The Task, the Performance and the Experience}
\paragraph*{The task T} is usually described in terms of how the machine learning system should process an \emph{example} (or \emph{data point}). An example is one datum $\vLeakVec\in \mathbb{R}^\traceLength$ , which is in turn a collection of \emph{features} $\vLeakVec[i], \quad i=1,\dots \traceLength$. In SCA context an example might be a side-channel trace, which is in turn a collection of time samples, that are its features. Some common ML tasks include these three examples: 
\begin{itemize}
\item \emph{Regression: } the computer is asked to predict a numerical value, given some input. The learning algorithm is thus asked to construct a function $f\colon \mathbb{R}^\traceLength \rightarrow \mathbb{R}$.
\item \emph{Classification: } the computer is asked to specify which class or category an input belongs to, being $\sensVarSet$ the set of the possible classes. The learning algorithm is thus asked to construct a function $f\colon \mathbb{R}^\traceLength \rightarrow \sensVarSet$. We remark that this task is similar to the regression one, except for the form of the output, since in general $\sensVarSet$ is a discrete finite set. A slightly variant solution to the classification task consists in constructing a function $f\colon \mathbb{R}^\traceLength \rightarrow \{0,1\}^{\numClasses}$, once have assigned to each class $\sensVar^j$ a \emph{one-hot encoding}, \ie  a $\numClasses$-dimensional vector,
with all entries equal to $0$ and the $j$-th entry equal to $1$: $\sensVar^j
\rightarrow \vec{\sensVar}^j = (0,\ldots , 0,\underbrace{1}_{j},0,\dots,0)$. A variant of the classification task consists in finding a function $f$ defining a probability distribution over classes.
\item \emph{Verification}: the computer is asked to state whether or not two given inputs are instances of a same class or category, for example state if two hand-written signatures have been produced by the same person. The learning algorithm is thus asked to construct a function $f\colon \mathbb{R}^\traceLength \times \mathbb{R}^{\traceLength}\rightarrow \{0,1\}$. A variant of such a task consists in finding for a pair of inputs the probability distribution for them being instance or not of a same class. This problem differs from the classification one essentially for the form of the input.
\end{itemize}
The functions constructed by a Machine Learning algorithm somehow describe and characterize the data form and distribution, thus are often referred to as \emph{models}.

\paragraph*{The performance P} designs a quantitative measure of the abilities of the learning algorithm. Depending on the task T, a specific performance measure P can be considered. For tasks as classification or verification the more common measure is the \emph{accuracy} of the model, \ie the proportion of inputs for which the model produces the correct output. Equivalently, the \emph{error rate} may be used as performance measure P, \ie the proportion of inputs for which the model produces an incorrect output. For the regression task the more common performance measure P is the so-called \emph{Mean Squared Error} (MSE): it is computed by averaging over a finite set of examples, the differences raised to the square between the correct output and the one predicted by the model. \\
One of the crucial challenge of Machine Learning is that we are usually interested in how well a learning algorithm performs in producing a model that fits new, unseen data. For this reason, the performances of a Machine Learning algorithm are usually evaluated over a so-called \emph{test set}, \ie a set of examples that have not been used for the learning (or \emph{training}) phase. 

\paragraph*{The experience E} denotes the typology of access to data and information the algorithm is allowed during learning. In this context we principally distinguish two family of learning algorithms: 
\begin{itemize}
\item the \emph{supervised} learning algorithms experience a dataset of examples, each associated in general to a \emph{target} or \emph{label}. The term supervised reflects the fact the the learning is somehow guided by an instructor that knows the right answer over the learning dataset;
\item the \emph{unsupervised} learning algorithms experience a dataset, without any associated target. They tries to learn useful properties of the structure of the dataset. 
\end{itemize}
In general, the nature of the task is strictly related to the kind of experience the learner is allowed, for example the classification or regression tasks are considered as supervised tasks, while examples of unsupervised tasks include \emph{clustering} and \emph{data representation} or \emph{dimensionality reduction}. For example, the Principal Component Analysis, that will be discussed in Chapter~\ref{ChapterLinear} in the context of SCA, is a dimensionality reduction algorithm that might be seen as an unsupervised algorithm that learns a representation of data. We will see that for SCA context a supervised version of PCA has been proposed as well. 


% examples: Classification, Dimensionality Reduction, Verification
% Example method: Neural Network Classifier, Stacked Auto-Encoder, Siamese Network 

\subsection{Example of Linear Regression}
The regression task is not of high interest for the rest of this thesis, but is the most direct example to keep in mind to understand some basic Machine Learning concepts, such as the underfitting and the overfitting (see \ref{sec:overfitting}). Let us introduce a linear regression model to tackle the regression task: we want to construct a linear function $f\colon \mathbb{R}^\traceLength \rightarrow \mathbb{R}$, that takes an input $\vLeakVec$ and outputs $\hat{y} = \www^\intercal \vLeakVec$, where $\www\in \mathbb{R}^\traceLength$ is a vector of \emph{parameters} that have to be learned by a learning algorithm.\footnote{An affine model may be considered as well by adding a \emph{bias} is added to the model, leading to $\hat{y} = \www^\intercal \vLeakVec+w_0$. This model is equivalently obtained by adding an additional entry to $\vLeakVec$, always set to $1$ and writing back $\hat{y} = \www^\intercal \vLeakVec$ with $\www\in \mathbb{R}^{N+1}$. } We want this model well describe some data and we suppose have two available datasets of such data: $\setDataTrain = (\setLeakTrain, \setTargetTrain)$, to let the learning algorithm experience on, and $\setDataTest = (\setLeakTest, \setTargetTest)$ to evaluate the performance of the obtained model over some unseen data. Let us choose the MSE over the test set to evaluate such performances. Let us collect the, let say $N$, examples of a dataset as column of a measure matrix $\measuresMatrix\in \mathbb{R}^{\traceLength \times N}$ and the related target into a vector $\yyy\in \mathbb{R}^N$, and let the learned model predict targets $y$ by outputting $\hat{y} = \www^\intercal\vLeakVec$, then the MSE is given by
\begin{equation}
 \mathrm{MSE_{test}} = \frac{1}{m} \norm{\hat{\yyy}_{\text{test}}-\yyy_{\text{test}}}^2_2 \mbox{ .}
\end{equation}
 The MSE is the performance measure in this example, meaning that we consider the model performs well the most such an MSE is small. So the goal of the learning algorithm is somehow minimize the $\mathrm{MSE_{test}}$. But the learning algorithm only experiences on the $\setDataTrain$ dataset. An intuitive way to act, that can be proven be the maximum likelihood solution to the problem, is to minimize  $\mathrm{MSE_{train}}$ by solving an easy optimization problem. When a learning algorithm behaves as an optimization algorithm that minimizes a given function, such a function is called \emph{cost function}, of \emph{objective function}.
 The solution to such a optimization problem can be given in closed form, by means of the pseudo-inverse matrix of $\measuresMatrix_{\text{train}}$:
\begin{equation}
\www = (\measuresMatrix_{\text{train}}\measuresMatrix_{\text{train}}^\intercal)^{-1}\measuresMatrix_{\text{train}}\yyy_{\text{train}}.
\end{equation}


\subsection{Example of Linear Model for Classification}\label{example:LDA}
In this thesis we point out a strict relationship between the profiling SCAs and the classification task in Machine Learning context. For this reason we introduce here a very brief overview of how classically such a task is tackled, by means of linear models. \\
Classifying means assigning to an example $\vLeakVec\in\mathbb{R}^\traceLength$ a label $\sensVar\in \sensVarSet$, or equivalently divide the input space $\mathbb{R}^\traceLength$ in \emph{decision regions}, whose boundaries are referred to as \emph{decision boundaries}. Making use of a linear model signifies exploiting some hyperplanes as decision boundaries. Data sets whose classes can be separated exactly by linear decision boundaries are said to be \emph{linearly separable}. Following the discussion kept by Bishop in \cite{christopher2006pattern}, two different approaches to tackle the classification task should be distinguished: the direct research for a discriminant function $f$ that assigns to an example a label, or the prior construction of a probabilistic model. This second approach might in turn be distinguished into two options, depending on whether a generative model, or a discriminative model is constructed. For this example we consider a probabilistic approach, constructing a generative model. This example will allow to introduce some interesting functions, such as the \emph{logistic sigmoid} and the \emph{softmax}, that will play a role in the construction of neural network (see Chapter \ref{ChapterCNN}, and to show how adding some assumptions on the data distributions one can justify, in contexts where the goal of the learning algorithm is making decisions directly, dispensing with any probabilistic interpretation, the exploitation of linear discriminant functions. \\
Construct a generative probabilistic model implies modelling the class-conditional probabilities $\pdf(\vLeakVec\mid \sensVar^j)$ for $j\in [1,\dots,\numClasses]$ as well as the class priors $\pdf(\sensVar^i)$. Let us first consider in a 2-class context, \ie $\numClasses = 2$. Then the posterior probability for the class $\sensVar^1$ is the following:
\begin{equation}\label{eq:post_probs}
\pdf(\sensVar^1\mid \vLeakVec) = \frac{\pdf(\vLeakVec\mid \sensVar^1)\pdf(\sensVar^1)}{\pdf(\vLeakVec\mid \sensVar^1)\pdf(\sensVar^1) + \pdf(\vLeakVec\mid \sensVar^2)\pdf(\sensVar^2)}\mbox{ .}
\end{equation}
To compare the two classes, we can look to their \emph{log-likelihood ratio}:
\begin{equation}\label{eq:log-ratio}
a = \log\left[\frac{\pdf(\sensVar^1\mid \vLeakVec)}{\pdf(\sensVar^2\mid \vLeakVec)}\right] =  \log\left[\frac{\pdf(\vLeakVec\mid \sensVar^1)\pdf(\sensVar^1)}{\pdf(\vLeakVec\mid \sensVar^2)\pdf(\sensVar^2)}\right].
\end{equation}
Then the discriminative criteria can be assigning to $\vLeakVec$ the class $\sensVar^1$ iff $a>0$. The decision boundary is given by the surface defined by $\pdf(\vLeakVec\mid \sensVar^1)\pdf(\sensVar^1) = \pdf(\vLeakVec\mid \sensVar^2)\pdf(\sensVar^2)$.
We remark that we Eq.~\eqref{eq:post_probs} rewrites as
\begin{equation}\label{eq:post_probs_sigmoid}
\pdf(\sensVar^1\mid \vLeakVec) = \frac{1}{1+e^{-a}} = \sigma(a)\mbox{ ,}
\end{equation}
where the function $\sigma$ is the so-called \emph{logistic sigmoid}.\\
In the multi-class case, \ie $\numClasses >2$, the posterior probability for each class $\sensVar^j$ is given by
\begin{equation}\label{eq:post_probs_multi-class}
\pdf(\sensVar^j\mid \vLeakVec) = \frac{\pdf(\vLeakVec\mid \sensVar^j)\pdf(\sensVar^1)}{\sum_k\pdf(\vLeakVec\mid \sensVar^k)\pdf(\sensVar^k )} = \softmax (\aaa)[k]\mbox{ ,}
\end{equation}
where $\aaa$ is a $\numClasses$-dimensional vector, whose entries are given by
\begin{equation}
\aaa[j] = \log\left[ \pdf(\vLeakVec\mid \sensVar^j)\pdf(\sensVar^j) \right] \mbox{ ,}
\end{equation}
and $\softmax$ is the so-called \emph{softmax} function, or \emph{normalized exponential}, that is defined, entry-wise by:
\begin{equation}\label{eq:softmax}
\softmax(\aaa)[k] = \frac{e^{\aaa[k]}}{\sum_{j=1}^{\numClasses}e^{\aaa[j]}}\mbox{ .}
\end{equation}

Let us now introduce two assumptions about class-conditional densities: we will suppose they follow a Gaussian distribution with parameters $\mu_j, \Sigma_j$, and that all class-conditional densities share the same covariance matrix $\Sigma_j=\Sigma$, so that
\begin{equation}
\pdf(\vLeakVec\mid \sensVar^j) = \frac{1}{(2\pi)^{{\traceLength}/2}\lvert \Sigma\rvert^{1/2}}e^{-\frac{1}{2}(\vLeakVec- \mu_j)^\intercal\Sigma^{-1}(\vLeakVec- \mu_j)} \mbox{ .}
\end{equation}
Under this assumptions Eq.~\eqref{eq:log-ratio} rewrites as: 
\begin{equation}\label{eq:LDA-2classes}
a = \log(\frac{\pdf(\sensVar^1)}{\pdf(\sensVar^2)}) - \frac{1}{2}\mu_1^\intercal\Sigma^{-1}\mu_1 + \frac{1}{2}\mu_2^\intercal\Sigma^{-1}\mu_2 - \vLeakVec^\intercal\Sigma^{-1}(\mu_2-\mu_1) = \www^\intercal \vLeakVec + w_0, 
\end{equation}
where we set 
\begin{align*}
\www =& \Sigma^{-1}(\mu_1-\mu_2)\\
w_0 =  & \log\frac{\pdf(\sensVar^1)}{\pdf(\sensVar^2)} - \frac{1}{2}\mu_1^\intercal\Sigma^{-1}\mu_1 + \frac{1}{2}\mu_2^\intercal\Sigma^{-1}\mu_2 . 
\end{align*}
The quadratic terms in $\vLeakVec$ in the exponent of the Gaussian density have cancelled thanks to the common variance assumption, thus we obtain that the decision boundary for the 2-class problem, given by $a=0$ is a $(\traceLength - 1)$-hyperplane of the input space. \footnote{An analogous result can be obtained in the multi-class problem.} This way of choosing linear boundaries is known under the name of \emph{Linear Discriminant Analysis}. Another way to view the same linear classification model is in terms of dimensionality reduction: intuitively, in the 2-class case\footnote{again extensible to the multi-class case} one can see the term $\www^\intercal \vLeakVec$ of \eqref{eq:LDA-2classes} as a projection of the input $\vLeakVec$ onto a one-dimensional subspace of $\mathbb{R}^\traceLength$ orthogonal to the decision boundary mentioned above, then classify the obtained dimensionality-reduced examples by the means of a threshold (that would correspond to $w_0$, in the optimal case). It can be shown that the dimensionality reduction obtained by the Fisher criterion that we will deploy in Chapter~\ref{ChapterLinear}, to which we will refer to LDA dimensionality reduction by a widely accepted abuse, is equivalent to the dimensionality reduction obtained in this example, making the Gaussian assumption over the class-conditional probabilities, equipped of a common covariance matrix assumption.  \\
Relaxing the assumption of a shared covariance matrix and allowing each class-conditional density $\pdf(\vLeakVec\mid \sensVar^j)$ to have its own covariance matrix $\Sigma_j$, then the earlier cancellations will no longer occur, and the discriminant $a$ turns out to be a quadratic function of $\vLeakVec$. This gives rise to the so-called \emph{Quadratic Discriminant Analysis}, that we already mentioned in Chapter~\ref{ChapterIntroductionSCA} for its analogy with Template Attacks.\\

The two assumptions leading to the log-likelihood ratio \eqref{eq:LDA-2classes}, also leads to the following expression for the posterior probability for $\sensVar^1$, directly implied by \eqref{eq:post_probs_sigmoid}: 
\begin{equation}
\pdf(\sensVar^1\mid \vLeakVec) = \sigma(\www^\intercal \vLeakVec + w_0)\mbox{ .}
\end{equation}
Thus such a posterior probability is given by the sigmoid acting to a linear function of $\vLeakVec$. Similarly, for the multi-class case, the posterior probability of class $\sensVar^j$ is given by the $j$-th entry of the softmax transformation of a linear function of $\vLeakVec$. This kind of \emph{generalized linear model} can be thus used in a probabilistic discriminant approach, where the posterior conditional probabilities are directly modelised from data without passing through the estimations of class-conditional densities and priors. Such a discriminative approach is the one that will be adopted in Chapter~\ref{ChapterCNN} when considering neural networks as models.

%the simplest representations of a linear discriminant function is obtained by taking
%\begin{equation}
%f(\vLeakVec) = \www\vLeakVec + w_0 \mbox{ ,}
%\end{equation}
%where $-w_0$ plays the role of a threshold. Here  $\vLeakVec$ is assigned to $\sensVar^1$ if $f(\vLeakVec)>0$, \ie $\www\vLeakVec>-w_0$, otherwise $\vLeakVec$ is assigned to $\sensVar^2$.



% la decision boundary  è un D-1 iperpiano
% introducendo i vettori di target...
% y(x) = wx + w0...oppure in grosso per piu di due classi: la decision boundary è un iperpiano di dimensione (numClasses - 1)
% per imparare i pesi W un'idea è minimizzare il sum error square sti cazzi, un'altra è passare attraverso la riduzione di dimensione e scegliere la riduzione che amplifica la separabilità. 
%Infatti....fisher dà la nozione di ottimalità col quoziente, che porta alla Fisher Discriminant Analysis (o anche LDA, discussa nel Capitolo 6) ... Nel caso numClasses = 2 le due si equivalgono. 
% E interessante discutere brevemente anche di un semplice modello generativo, basato su hp gaussiana, perche tale ipotesi, insieme ad un'altra, porta a costruire anch'essa un modello lineare, giustificando questa scelta in molti contesti...P(x|C)...log del quoziente, funzione sigmoid, logit...softmax...linear piu sigmoid, dove di nuovo la soluzione LDA è la parte lineare (verifica). Se togliamo l'ipotesi sulle covarianze allora resta la parte quadratica e è la QDA (in pratica la parte di classificazione usata nei template attacks)  


\subsection{Underfitting, Overfitting, Capacity,  and Regularization}\label{sec:overfitting}
\paragraph*{Underfitting and Overfitting} As already said, the main challenge of ML is that the learning algorithms are in general allowed to experience over training data, but the models they output are asked to fit some unseen test data. Observing the training data an ML algorithm sets the model parameters in order to raise the performances over the training set, or equivalently to minimize the training error. Nevertheless, at the end of the learning process, the model performance is evaluated over the test set, evaluating the test error. Thus, two factors determine how well a ML algorithm acts: its ability to reduce the training error, and its ability to reduce the gap between the training and the test error. When the former ability is not satisfactory we assist to the \emph{underfitting} phenomenon: the model is not able to obtain a low training error, or the ML algorithm is not able to determine model parameters that make training error be low. On the other hand, if the latter ability is not satisfactory we assist to the \emph{overfitting} phenomenon: the gap between the training and the test error, called \emph{generalization gap}, is too large. \\

\paragraph*{Capacity}The property of a model that controls its underfitting or overfitting behaviour is the \emph{capacity}. Roughly speaking the capacity of a model quantifies the complexity of the functions it can represent: a model with higher capacity can be parametrized is such a way to represent a higher complex function. For example a linear regression model is able to represent all linear functions. To raise its capacity quadratic, cubic or general polynomial terms might be included, to allow the model represent respectively quadratic, cubic or polynomial functions as well. Another common way to enlarge the capacity of a linear regression model $y = \www^\intercal \vLeakVec$,  consists in choosing some \emph{basis functions} $\varphi_1, \varphi_2,\dots, \varphi_B$ and replace $\vLeakVec$ with the values $\varphi_1(\vLeakVec), \varphi_2(\vLeakVec),\dots, \varphi_B(\vLeakVec)$. The form of the basis functions will determine the capacity of the model. Basis function regression includes the linear and the polynomial case. \\

The polynomial regression provides a striking example to understand the underfitting and overfitting phenomena. Consider a problem in which examples $(x,y)$ lies in $\mathbb{R} \times \mathbb{R}$ and the true underlying function is quadratic, perturbed by a small noise. Let the training set contain 4 data points. Figure ~\ref{fig:poly_reg} shows the results of a linear, quadratic and cubic regression in such a case. We can observe that the linear predictor is underfitting, since its training error is quite high, while the cubic predictor is overfitting: it perfectly fits the 4 training points (it is the Lagrange polynomial interpolating such 4 points) but shows a huge error in predicting new examples. The quadratic regression is obviously in this case the model exhibiting the optimal capacity to solve such a problem. A very rough way to have an intuition about the capacity of a model is counting the number of its parameters: the capacity in general grows with the number of parameters. In our polynomial regression example the cubic model has as many parameters as the number of training points.\\
Some formal ways to quantify the capacity of a model have been provided in ML literature. The most well-known is the \emph{Vapnik-Chervonenkis dimension}: it measures the capacity of a classifier as the cardinality of the largest set of points the model can classify without errors, for any possible assignment of labels. In practice, quantifying the capacity of a model, especially for complex models as those considered in deep learning, is very hard and discouraged. On the other hand, these kinds of quantifications has enabled statistical learning theory to formalize and prove some important intuitions, for example the fact that the generalization gap is upper-bounded by a quantity that grows with the model capacity and that shrinks as the number of training examples increases. In Fig.~\ref{fig:cubic_more_data} we observe how the cubic model used for regression on quadratic distributed data ameliorate its performances and reduces the generalization gap despite its excessive capacity, when trained with more examples. This observation basically justifies on one hand the deep learning attitude consisting in consider very complex models, having confidence in the big size of the typically considered training sets. On the other hand it justifies the interest of \emph{Data Augmentation} (DA) techniques to respond to an eventual lack of data. Some DA techniques wll be proposed in Chapter~\ref{ChapterCNN} for the SCA context.

\begin{figure}
\subfigure[Linear]{\label{fig:linear_regression}
\includegraphics[width=.5\textwidth]{../Figures/linear_regression.pdf}}
\subfigure[Quadratic]{\label{fig:quadratic_regression}
\includegraphics[width=.5\textwidth]{../Figures/quadratic_regression.pdf}}
\subfigure[Cubic]{\label{fig:cubic_regression}
\includegraphics[width=.5\textwidth]{../Figures/cubic_regression.pdf}}
\subfigure[Cubic, more training data]{\label{fig:cubic_more_data}
\includegraphics[width=.5\textwidth]{../Figures/cubic_regression_more.pdf}}
\caption[Example of underfitting and overfitting over a regression problem.]{Example of underfitting and overfitting over a regression problem.Linear \subref{fig:linear_regression}, quadratic \subref{fig:quadratic_regression} and cubic regression for a truly noised quadratic problem. Red circles are the training examples, green points are the test ones. Linear regression underfits data, cubic \subref{fig:cubic_regression}  regression overfits data. \subref{fig:cubic_more_data} Cubic regression for a noised quadratic problem and more training examples. The cubic model trained over more data is better adapted to the truly quadratic data, and overfitting is attenuated.}\label{fig:poly_reg}
\end{figure}

\paragraph*{Regularization}
In a real-case problem, the optimal capacity necessary to learn from given data is unknown. In such a case, trying to fit data with a too low capacity model assures the defeat, thus it is always more interesting to oversize the capacity of the learning model. Choosing an oversized model, we risk to incur in overfitting. The so-called \emph{regularization} techniques respond to such a risk: they consist in general in adding constraints to the learning algorithm in order to guide it in choosing a model among a wide set of eventually fitting models. Going back to the polynomial regression example, one can try to fit data with a cubic polynomial (thus oversizing the model capacity) and induce the optimizer algorithm to choose the smallest-degree polynomial fitting data be chosen via a regularization. This can be obtained adding a penalty that depends on the polynomial degree to the cost function. Applying regularization may make the algorithm be less accurate in learning training data, but more likely to correctly operate on new examples.

\subsection{Hyper-Parameters and Validation}
The \emph{hyper-parameters} of a model are all parameters that are priorly set and that are not learned by the learning algorithm. They define the general form of the model. In the polynomial regression example the model had a single hyper-parameter: the degree of the polynomial. It is evident from the example that such a parameter is somehow forced not to be optimized by the means of the learning algorithm: trying to reduce the training MSE, the algorithm would choose a sufficient high degree to interpolate all training points. This would cause overfitting. In general among all parameters of a model, the hyper-parameters are chosen as those that can not be learned from data because it would cause overfitting, as in the example, or because they are too difficult to optimize. \\
A way to choose a setting for hyper-parameters consists in perform a \emph{validation} phase. To do so the training set is split into two disjoint sets, one still called \emph{training set} and the other one called \emph{validation set}. We can say that as the training set is used to learn the parameters, the validation set is used to somehow learn the hyper-parameters. Indeed during or after the training over the training set, the validation set is used to estimate the test error, which quantifies the generalization ability of the model. In practice the performances of the (partially) trained model are evaluated over the validation set and hyper-parameters are updated accordingly, in order to raise the generalization ability of the model. Once the model has been validated, \ie the hyper-parameters are definitely set, the real test error is evaluated over the test set. Usually the validation error is an underestimation of the test error, since hyper-parameters have been set to reduce it. \\
The validation process just described may strongly depend on the way the training set have been split to create the validation one. In order to avoid to validate a model in a strongly data-dependent way, a slightly different process is encouraged in machine learning community, named the \emph{cross-validation}, which we describe in Appendix~\ref{app:cross-validation}.


\subsection{No Free Lunch Theorem}\label{sec:NFL}
A so-called \emph{No Free Lunch Theorem} has been formulated for optimization and machine learning algorithms around 1997 \cite{wolpert1997no}. It states that any learning algorithm, averaged over all possible data distributions, has the same performances over unseen test data. This means that there cannot exist a universal best machine learning algorithm: any of them performs in the same way, when performances are averaged over all possible tasks. Thus, making research over some kind of data, for example SCA traces, means trying to understand what kinds of machine learning algorithms perform well over such particular kind of data and point out the eventual interesting hyper-parameters of machine learning models that are responsible of the main performance variations. 




%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Overview of Machine Learning in Side-Channel Context}
In 1991 Rivest pointed out for the first time a strong link between the fields of Machine Learning and Cryptanalysis \cite{rivest1991cryptography}. Starting from observing that the goal of cryptanalysis is identifying an unknown encryption function, indexed by a a secret key, and that a classic problem in ML consists as well in learning an unknown function, he drew a strong correspondence between terminology and concepts of the two fields.\\

Machine Learning algorithms started to be investigated in in Side-Channel Attacks context in 2011 \cite{machineLearningSCA}. In this paper the authors formulated for the first time an attack in terms of classification problem and proposed the Support Vector Machine (SVM) \cite{cortes1995support,weston1998multi} as technique to solve it. They also equipped the SVM with a kernel function to allow the it to succeed even in case data would not be linearly separable. Such an approach is similar to the one the we will describe in Chapter~\ref{ChapterKernel}, to obtain Kernel Discriminant Analysis dimensionality reduction technique from the Linear Discriminant Analysis. Further works analysed the use of SVM in SCA context, proposing concrete attack scenarios \cite{intelligentMachineOmicide,effTA_SVM}. 
The technique of Random Forest \cite{lior2014data} drew attention of the SCA community as well. As the SVM, it has been used as a classifier and evaluated in different works \cite{lerman2015machine,lerman2015template,lerman2014power}. As in recent years the privileged tools to tackle classification problem in Machine Learning area are the Neural Networks, whose multi-layer configuration has given name to the so-called \emph{Deep Learning} domain, such tools have as well be analysed in SCA context. Networks in the form of Multi-Layer Perceptrons (MLP) have been proposed as classifiers for side-channel traces in a series of works \cite{martinasek2013optimization,martinasek2013innovative,martinasek2015profiling,martinasek2016profiling}, while Convolutional Neural Network was firstly introduced in \cite{maghrebi2016breaking}. A part of this thesis contributions consists in the application of the convolutional paradigm as way to defeat misalignment countermeasures in side-channel attacks (see Chapter~\ref{ChapterCNN}).

%\subsection{Profiled Attack as a Classification Problem}
%\todo{remark that LDA is first of all a linear method for classification and has been introduced in SCA many years ago as preprocessing for Gaussian TA}
%\subsubsection{Support Vector Machine}
%\subsubsection{Random Forest}
%\subsubsection{Neural Networks}

%\begin{table}[]
%\centering
%\caption{Examples of hyper-parameters}
%\label{tab:hyperparameters}
%\begin{tabular}{c|c}
%\multicolumn{1}{c}{\textbf{Training Hyper-Parameters}} & \multicolumn{1}{c}{\textbf{Architecture Hyper-Parameters}}    \\
%\hline
%training set size                             & number of layers                                     \\
%batch size                                    & nature of each layer{\scriptsize  (\eg FC, ACT,$\dots$)} \\
%number of epochs                              & number of units     \rdelim\}{1}{3mm}[{\scriptsize for FC layers}]                 \\
%optimizer algorithm              &  number of filters               \rdelim\}{4}{3mm}[{\scriptsize for CONV layers}]                          \\
%(initial) learning rate              & kernel size                           \\
%                                              & stride                                               \\
%                                                  & padding                                              \\
%                                              & activation function                  \rdelim\}{1}{3mm}[{\scriptsize for ACT layers}]             \\                   
%                                          
%                                              & pooling function        \rdelim\}{3}{3mm}[{\scriptsize for POOL layers}]                                              \\
%                                              & kernel size \\
%                                              & stride                                              
%\end{tabular}
%\end{table}