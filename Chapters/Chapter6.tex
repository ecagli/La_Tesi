% Chapter Template

\chapter{Linear Dimensionality Reduction} % Main chapter title
% https://stat.columbia.edu/~cunningham/pdf/CunninghamJMLR2015.pdf
\label{ChapterLinear}


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}
Linear dimensionality reduction methods produce a low-dimensional linear mapping of the original high-dimensional data that preserves some feature of interest in the data. An abundance of methods has been developed throughout statistics, machine learning, and applied fields for over a century, and these methods have become indispensable tools for analysing high dimensional, noisy data, such as side-channel traces.  Accordingly, linear dimensionality reduction can be used for visualizing or exploring structure in data, denoising or compressing data, extracting meaningful feature spaces, and more. A very complete survey about this great variety of linear dimensionality reduction technique has been published in 2015 by Cunningham and Zoubin \cite{cunningham2015linear}. They proposed a generalized optimization framework for all linear dimensionality techniques, survey a dozen different techniques and mention some important extension such as kernel mappings. \\

Among the surveyed methods in \cite{cunningham2015linear} we find the two mainly considered in SCA literature: the Principal Components Analysis (PCA) and the Linear Discriminant Analysis (LDA). The PCA has been applied both in an {\em unsupervised} way (\ie non-profiling attacks) \cite{Batina2012,karsmakers2009side}, and in a {\em supervised} way (\ie profiling attacks) \cite{TAprincipal,choudaryefficient,choudary2014efficient,disassembler,Standaert2008}. As already remarked in \cite{disassembler} and not surprisingly, the complete knowledge assumed in the supervised approach hugely raises performances. The main competitor of PCA in the profiling attacks context is the LDA, that thanks to its classification-oriented flavour, is known to be more meaningful and informative \cite{lessIsMore,Standaert2008} than the PCA method  for side channels. Nevertheless, the LDA is often set aside because of its practical constraints; it is subject to the so-called {\em Small Sample Size problem (SSS)}, i.e. it requires a number of observations (traces) which must be higher than the dimension (size) $\traceLength$ of them. In some contexts it might be an excessive requirement, which may become unacceptable in many practical situations where the amount of observations is very limited and the traces size is huge.\\

In 2014 Durvaux et al. proposed the use of another technique for linear dimensionality reduction in SCA context \cite{PP}, the so-called Projection Pursuits (PPs), firstly introduced in 1974 by Friedman and Tukey \cite{friedman1974projection}. This method essentially works by randomly picking parts of the data and randomly setting the projecting coefficient, and
by tracking the improvements (or lack thereof) of the projection when modifying
it with small random perturbations. The main drawback of the PPs  pointed out by the authors of \cite{PP} for the SCA context is their heuristic nature,
since the convergence of the method is not guaranteed and its complexity is
context-dependent. The main advantage is the fact that
PPs can deal with any objective function, which naturally fits to the problem
of higher-order SCA. Thus this technique appears advantageous in higher-order context, where it is used as a PoI selection tool. It is its version for the first-order attacks  which turns out in a method to linearly reduce dimensionality. Nevertheless, in this context it is less interesting than the non-heuristic PCA and LDA. For this reason we will left PPs technique apart in this chapter, and describe their higher-order version in Chapter~\ref{ChapterKernel}.

 In SCA literature, one of the open issues in PCA concerns the choice of the components that must be kept after the dimension reduction: as already remarked by Specht et al.  \cite{specht}, some papers declare that the leading components are those that contain almost all the useful information \cite{TAprincipal,choudary2014efficient}, while others propose to discard the leading components \cite{Batina2012}. In a specific attack context, Specht et al. compares the results obtained by choosing different subsets of consecutive components, starting from some empirically chosen index. They conclude that for their data the optimal result is obtained by selecting a single component, the fourth one, but they give no formal argumentation about this choice. Such a result is obviously very case-specific. Moreover, the possibility of keeping non-consecutive components is not considered. \\

 
In this chapter the classical PCA technique is first of all described (Sec.~\ref{sec:PCA}) . In Sec.~\ref{sec:PCA_SCA}, we recall the previous applications of PCA in SCA context, highlighting the difference between its unsupervised and supervised declination. Then we propose our contribution to the choice of components open issue: our solution is based on the Explained Local Variance (ELV) notion, that we will define and discuss on in the same section. The reasoning behind the ELV selection methodology is essentially based on the observation that, for secure implementations, the leaking information, if existing, is spread over a few time samples of each trace. This observation has already been met by Mavroeidis et al. in \cite{SCAclassProbl}, where the authors  also proposed a components selection method. As we will see in this paper, the main difference between their proposal and ours is that we do not discard the information given by the eigenvalues associated to the PCA components, but we synergistically exploit such information and the observation met.  We will argue about the generality and the soundness of this methodology and show that it can raise the PCA performances, making them close to those of the LDA, even in the supervised context. This makes PCA an interesting alternative to LDA in those cases where the LDA is inapplicable. The LDA technique will be described in Sec.~\ref{sec:LDA}, together with (in Sec.~\ref{sec:LDA_SCA} its previous uses in SCA literature, the description of the SSS problem and some solutions coming from the Pattern and Face Recognition communities \cite{eigenfaces,Chen2000,huang,Yu01adirect}. Through some experiments depicted in Sec.~\ref{sec:experiments} we will conclude about the effectiveness of the PCA-ELV solution. Finally, in Sec.~\ref{sec:misalignment} we will experimentally argue about the non robustness of these techniques to data misalignment.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Principal Component Analysis} \label{sec:PCA}
\begin{figure}
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/PCA_unsupervised_mV2.pdf} 
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/PCAprojection_unsupervised_mV2.pdf} 
\caption{PCA: some 2-dimensional data projected into their 1-dimensional principal subspace.}\label{fig:PCAunsupervised}
\end{figure}
The Principal Component Analysis (PCA) is a technique for data dimensionality reduction. The same PCA algorithm can be deduced from two different points of view, a statistical one and a geometrical one. In the former PCA aims to project orthogonally the data onto a lower-dimensional linear space, the so-called \emph{principal subspace}, such that the variance of the projected data is maximized. In the latter, PCA aims to project data onto a lower-dimensional linear space in such a way that the average projection cost, defined as the mean square distance between the data and their projections, is minimized. In the following it is shown how the PCA algorithm is deduced by the statistical definition. In Appendix~\ref{app:PCA_geometrical} the equivalence between the two approaches is depicted. An example of 2-dimensional data projected over their 1-dimensional principal subspace is depicted in Fig.~\ref{fig:PCAunsupervised}\\

Let $(\vLeakVec)_{i=1..\nbTraces}$ be a set of $\traceLength$-dimensional measurements (or observations, or data), i.e. realizations of a $\traceLength$-dimensional zero-mean random vector $\vaLeakVec{}$, and collect them as columns of an $\traceLength \times \nbTraces$ matrix $\measuresMatrix$, so that the empirical covariance matrix of $\vaLeakVec{}$ can be computed as 
\begin{equation}
\covmat = \frac{1}{\nbTraces}\measuresMatrix\measuresMatrix^\intercal \mbox{ .}
\end{equation}

Let us assume for the moment that we have fixed the dimension $\newTraceLength<\traceLength$ of the principal subspace we are looking for. 

\paragraph{Compute the First Principal Component}
Suppose in a first time that $\newTraceLength = 1$, i.e. that we want to represent our data by a unique variable $Y_1 =  \AAlpha_1 \vaLeakVec{}$, i.e. projecting data over a single $ 1 \times\traceLength$ vector $\AAlpha_1$, in such a way the the variance of the obtained data is maximal. The vector $\AAlpha_1$ that provides such a linear combination is called {\em first principal component}. 
To avoid misunderstanding we will call {\em $j$-th principal component} (PC) the projecting vector $\AAlpha_i$, while we will refer to the variable $Y_j = \AAlpha_j \vaLeakVec{}$ as {\em $i$-th Principal Variable (PV)}. 
Realizations of the PVs are given by the measured data projected over the $j$-th PC, for example we have $\nbTraces $ realizations of $Y_1$:
\begin{equation}
y_1[i] = \AAlpha_1 \vLeakVec \mbox{ for } i=1,\dots , \nbTraces \mbox{ .}
\end{equation}

Let us collect these realizations in a vector $\yyy_1 = \AAlpha_1 \measuresMatrix $; the mean of these realizations will be zero as they are linear combinations of zero-mean variables, and the variance turns to be expressible as
\begin{equation}
\frac{1}{\nbTraces}\yyy_1\yyy_1^\intercal = \frac{1}{\nbTraces}\AAlpha_1\measuresMatrix\measuresMatrix^\intercal\AAlpha_1^\intercal = \AAlpha_1\covmat\AAlpha_1^\intercal \mbox{ .}
\end{equation}
To compute $\AAlpha_1$ we have to look for the vector that maximize such a variance.\\

Obviously, we can attend a value for the variance as high as wished, raising the modulo $\|\AAlpha_1\| = \sqrt{\AAlpha_1^T\AAlpha_1}$. In order to let the maximization problem have a solution, we impose a restriction over it: $\AAlpha_1^T\AAlpha_1 = 1$.\\
Let us handle this constrained optimization problem making use of Lagrange multipliers:
\begin{equation}
\Lambda(\AAlpha_1, \lambda) = \AAlpha_1\covmat\AAlpha_1^\intercal - \lambda(\AAlpha_1\AAlpha_1^\intercal-1)
\end{equation}
and let us compute de partial derivative of $\Lambda$ with respect to $\AAlpha_1$
\begin{equation}
\frac{\partial\Lambda}{\partial\AAlpha_1} = 2\covmat\AAlpha_1^\intercal-2\lambda\AAlpha_1^\intercal \mbox{ .}
\end{equation}
Thus, stationary points of $\Lambda$ verify
\begin{equation}\label{eq:eigProblem}
\covmat\AAlpha_1^\intercal = \lambda\AAlpha_1^\intercal \mbox{ ,}
\end{equation}
which implies that $\AAlpha_1^\intercal$ must be an eigenvector of $\covmat$, with $\lambda$ its correspondent eigenvalue. Multiplying both sides of Eq.~(\ref{eq:eigProblem}) by $\AAlpha_1$ on the left, we remark that
\begin{equation}
\AAlpha_1\covmat\AAlpha_1^\intercal = \lambda\AAlpha_1\AAlpha_1^\intercal = \lambda, 
\end{equation}
which means that the variance of the obtained variable $\yyy_1$ equals $\lambda$. For this reason $\AAlpha_1$ must be the leading eigenvector of $\covmat$.

\paragraph{Compute the Second and Following Principal Components}
The PCs following the first one are defined in an incremental fashion by choosing new directions orthogonal to those already considered and such that the sum of the projected variances over each direction is maximal. Explicitly, if we look for two PCs, \ie $\newTraceLength = 2$,  we look for a $2$-dimensional variable $\YYY = \begin{psmallmatrix} \AAlpha_1 \\ \AAlpha_2 \end{psmallmatrix} \vaLeakVec$ such that the trace of its covariance matrix, \ie the sum of variances $\var(Y_1)+\var(Y_2)$, is maximal. It can be shown that the same result would be obtained maximizing the so-called \emph{generalized variance} of $\YYY$, which is defined as the determinant of its covariance matrix, instead of its trace. \\

Let us write, as in previous case, the Lagrangian of the problem
\begin{equation}
\Lambda = \AAlpha_1 \covmat \AAlpha_1^\intercal + \AAlpha_2\covmat \AAlpha_2^\intercal - \lambda_1(\AAlpha_1\AAlpha_1^\intercal -1) - \lambda_2(\AAlpha_2\AAlpha_2^\intercal -1) \mbox{ .}
\end{equation}

Partial derivatives with respect to $\AAlpha_1$ and $\AAlpha_2$ attend zero under the following conditions:
\begin{align}
\covmat  \AAlpha_1^\intercal &= \lambda_1\AAlpha_1^\intercal\\
\covmat  \AAlpha_2^\intercal &= \lambda_2\AAlpha_2^\intercal \mbox{ ,}
\end{align}

which means that $\AAlpha_1^\intercal$ and $\AAlpha_2^\intercal$ must be eigenvectors of $\covmat$ with correspondent eigenvalues given by $\lambda_1$ and $\lambda_2$. Moreover, as before, $\lambda_1$ and $\lambda_2$ equal the variances of variable components $Y_1$ and $Y_2$, and since we want to maximize the sum of such variables we have to choose $\AAlpha_1$ and $\AAlpha_2$ as the two leading vectors of $\covmat$.\\
Let us remark that the covariance between $Y_1$ and $Y_2$ is given by $\AAlpha_1^T\covmat \AAlpha_2$ which equals zero, since $\AAlpha_1^T\AAlpha_2 = 0$, by orthogonality. In particular the principal variables are uncorrelated, which is a remarkable property of the PCA. \\

In the general case of a $\newTraceLength$-dimensional projection space, it can be shown by induction that the PCs would corresponds to the $\newTraceLength$ leading eigenvectors of the covariance matrix $\covmat$.



%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Application of PCA in SCAs}\label{sec:PCA_SCA}
\subsection{Original vs Class-Oriented PCA}
\begin{figure}
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/dataNoProjection_mV2.pdf} 
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/PCAprojection_mV2.pdf} 
\caption{PCA: some 2-dimensional data projected into their 1-dimensional principal subspace.}\label{fig:PCAunsupervised}
\end{figure}
The classical version of PCA method is unsupervised, in the sense that is does not take into account the information about the value assumed by the target variable during the acquisition of data. On the other hand a profiling attacker is provided not only with a set of data $(\vLeakVec)_{i=1..\nbTraces}$, but he can group such traces depending on the target value: this let him obtain a set of traces $(\vLeakVec)_{i=1..\nbTraces}$ for each value of the target variable $\sensRandVar$. \\
In this context, and for the sake of distinguish the target value assumed by the variable $\sensRandVar$ in new executions, the idea of the \em{Class-Oriented} PCA is to consider {\em equivalent} all traces coming from each group, i.e. obtained by a same characteristic form plus a random noise. To estimate the characteristic form the sample means of the traces of each group is computed, obtaining $\mmmX = \frac{1}{\nbTraces}\sum_{i=1..\nbTraces} \vLeakVec$. These sample means will take the place of data in PCA: we collect them as rows in the data matrix $\measuresMatrix$, shift the empirical mean of each component to zero, compute the covariance matrix $\covmat$ et select the leading $\newTraceLength$ eigenvectors of $\covmat$ as optimal projecting directions.\\
In this way we focus the attention on information that discriminate classes, i.e. target values. 

\subsection{The Choice of the Principal Components}\label{sec:ELV}
The introduction of the PCA method in SCA context (either in its classical or class-oriented version)  has raised some important questions: \textit{how many} principal components and \textit{which ones} are sufficient/necessary to reduce the trace size (and thus the attack processing complexity) without losing important discriminative information?\\

Until now, an answer to the questions above has been given in \cite{choudary2014efficient}, linked to the concept of {\em explained variance} (or {\em explained global variance}, EGV for short) of a PC $\AAlpha_i$:
\begin{equation}\label{eq:EGV}
\mathrm{EGV}(\AAlpha_i) =  \frac{\lambda_i}{\sum_{k=1}^r\lambda_k} \mbox{ ,}
\end{equation}
where $r$ is the rank of the covariance matrix $\covmat$, and $\lambda_j$ is the eigenvalue associated to the $j$-th PC $\AAlpha_j$. $\mathrm{EGV}(\AAlpha_i)$ is the variance of the data projected over the $i$-th PC (which equals $\lambda_i$) divided by the total variance of the original data (given by the trace of the covariance matrix $\covmat$, {\em i.e.} by the sum of all its non-zero eigenvalues). By definition of EGV, the sum of all the EGV values is equal to $1$; that is why this quantity is often multiplied by $100$ and expressed as percentage.
Exploiting the EGV to choose among the PCs consists in fixing a wished {\em cumulative explained variance} $\beta$ and in keeping $\newTraceLength$ different PCs, where $\newTraceLength$ is the minimum integer such that
\begin{equation}
\mbox{EGV}(\AAlpha_1) +\mbox{EGV}(\AAlpha_2) + \dots +\mbox{EGV}(\AAlpha_\newTraceLength) \geq \beta \mbox{ .}
\end{equation}
However, if the adversary has a constraint for the reduced dimension $\newTraceLength$, the EGV notion simply suggests to keep the first $\newTraceLength$ components, taking for granted that the optimal way to chose PCs is in their natural order. This assumption is not always confirmed in SCA context: in some works, researchers have already remarked that the first components sometimes contain more noise than information \cite{Batina2012,specht} and it is worth discarding them. For the sake of providing a first example of this behaviour on publicly accessible traces, we applied a class-oriented PCA on 3000 traces from the DPA contest v4 \cite{DPAcontest}; we focused over a small 1000-dimensional window in which, in complete knowledge about masks and other countermeasures, information about the first Sbox processing leaks (during the first round). In Fig.~\ref{fig:DPAcontest} the first and the sixth PCs are plotted. It may be noticed that the first component indicates that one can attend a high variance by exploiting the regularity of the traces, given by the clock signal, while the sixth one has high coefficients localised in a small time interval, very likely to signalize the instants in which the target sensitive variable leaks.

\begin{figure}
\includegraphics[width=.45\textwidth]{../Figures/CARDIS2015/DPAcontestPC1_new.pdf} 
\includegraphics[width=.45\textwidth]{../Figures/CARDIS2015/DPAcontestPC6_new.pdf} 
\caption{First and sixth PCs in DPA contest v4 trace set (between time samples 198001 and 199000)}\label{fig:DPAcontest}
\end{figure}
To the best of our knowledge, a single method adapted to SCA context has been proposed until now to automatically choose PCs \cite{SCAclassProbl} while dealing with the issue raised in Fig.~\ref{fig:DPAcontest}. It is based on the following assumption:
\begin{assumption}\label{assum:local}
The leaking side-channel information is localised in few points of the acquired trace.
\end{assumption}
In the rest of the paper, we conduct our own analyses under Assumption \ref{assum:local} that we think to be reasonable in SCA contexts where the goal of the security developers is to minimize the number of leaking points.
Under this assumption, the authors of \cite{SCAclassProbl} use for side-channel attack purposes the {\em Inverse Participation Ratio} (IPR), a measure widely exploited in Quantum Mechanics domain (see for example \cite{guhr1998random}). They propose to use such a score to evaluate the eigenvectors {\em localization}. It is defined as follows:
\begin{equation}
\mathrm{IPR}(\AAlpha_i) = \sum_{j=1}^\traceLength \AAlpha_i[j]^4 \mbox{ .}
\end{equation}
The authors of \cite{SCAclassProbl} suggest to collect the PCs in decreasing order with respect to the IPR score.\\

The selection methods provided by the evaluation of the EGV and of the IPR are somehow complementary: the former is based only on the eigenvalues associated to the PCs and does not consider the form of the PCs themselves; the latter completely discards the information given by the eigenvalues of the PCs, considering only the distribution of their coefficients. One of the contributions of the present paper is to propose a new selection method, that builds a bridge between the EGV and the IPR approaches. As we will argue, our method, based on the so-called {\em explained local variance}, does not only lead to the construction of a new selection criterion, but also permits  to modify the PCs, choosing individually the coefficients to keep and those to discard. 

\subsection{The Explained Local Variance Selection Method}
\todo{Riprendere le notazioni e mettere apposto i newcommand} 
%The method we develop in this section is based on a compromise between the variance provided by each PC (more precisely its EGV) and the number of time samples necessary to achieve a consistent part of such a variance. To this purpose we  introduce the concept of {\em Explained Local Variance} (ELV).
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/cumulativeELVallRectangle.pdf} 
%\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/cumulativeELVzoomedRectangle.pdf} 
%\caption{Cumulative ELV trend of principal components. On the right a zoom of the plot on the left. Data acquisition described in Sec.~\ref{sec:experiments}.}\label{fig:ELVcumulative}
%\end{figure}
%
%Let us start by giving some intuition behind our new concept. Thinking to the observations ${\vLeakVec{}}^\intercal$, or to the class-averages ${\mmmX}^\intercal$ in class-oriented PCA case, as realizations of a random variable $\XXX^\intercal$, we have that $\lambda_i$ is an estimator for the variance of the random variable $\XXX^\intercal\cdot\AAlpha_i$. Developing, we obtain
%\begin{align}\label{eq:ELV}
%\lambda_i =& \hat{\mathrm{var}}(\sum_{j=1}^D \XXX^\intercal[j]\AAlpha_i[j]) = \sum_{j=1}^D\sum_{k=1}^D \hat{\mathrm{cov}}(\XXX^\intercal[j]\AAlpha_i[j], \XXX^\intercal[k]\AAlpha_i[k])=\\
%=& \sum_{j=1}^D \AAlpha_i[j]\sum_{k=1}^D\AAlpha_i[k]\hat{\mathrm{cov}}(\XXX^\intercal[j], \XXX^\intercal[k])= \sum_{j=1}^D \AAlpha_i[j] (\covmat_{j}^\intercal \cdot \AAlpha_i)=  \\
%=& \sum_{j=1}^D \AAlpha_i[j] \lambda_i\AAlpha_i[j]= \sum_{j=1}^D  \lambda_i \AAlpha_i[j]^2 \label{eq:toJustify}
%\end{align}
%where $\covmat_{j}^\intercal$ denotes the $j$-th row of $\covmat$ and \eqref{eq:toJustify} is justified by the fact that $\AAlpha_i$ is an eigenvector of $\covmat$, with $\lambda_i$ its corresponding eigenvalue. The result of this computation is quite obvious, since $\parallel \AAlpha_i\parallel=1$, but it evidences the contribution of each time sample in the information held by the PC. This makes us introduce the following definition:
\begin{definition}


The {\em Explained Local Variance} of a PC $\AAlpha_i$ in a sample $j$, is defined by
\begin{equation}
\mathrm{ELV}(\AAlpha_i,j) = \frac{\lambda_i \AAlpha_i[j]^2}{\sum_{k=1}^r\lambda_k} = \mathrm{EGV}(\AAlpha_i) \AAlpha_i[j]^2  \mbox{ .}
\end{equation}
\end{definition}
\begin{figure}
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC1.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC2.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC3.pdf} \\
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC4.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC5.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC6.pdf} 
\caption{The first six PCs. Acquisition campaign on an 8-bit AVR Atmega328P (see Sec.~\ref{sec:experiments}).}\label{fig:6components}
\end{figure}
Let $\mathcal{J}=\{j^i_1, j^i_2, \dots, j^i_{\traceLength}\}\subset\{1,2,\dots,\traceLength\}$ be a set of indexes sorted such that $\mathrm{ELV}(\AAlpha_i,j^i_1)\geq \mathrm{ELV}(\AAlpha_i,j^i_2)\geq \dots \geq \mathrm{ELV}(\AAlpha_i,j^i_\traceLength)$.
It may be observed that the sum over all the $\mathrm{ELV}(\AAlpha_i,j)$, for $j\in[1,\dots,\traceLength],$   equals $\mathrm{EGV}(\AAlpha_i)$. If we operate such a sum in a cumulative way following the order provided by the sorted set $\mathcal{J}$, we obtain a complete description of the trend followed by the component $\AAlpha_i$ to achieve its EGV. As we can see in Fig.~\ref{fig:ELVcumulative}, where such cumulative ELVs are represented, the first 3 components are much slower in achieving their final EGV, while the $4^\text{th}$, the $5^\text{th}$ and the $6^\text{th}$ achieve a large part of their final EGVs very quickly ({\em i.e.} by adding the ELV contributions of much less time samples). For instance, for $i=4$, the sum of the $\mathrm{ELV}(\AAlpha_4, j^4_k)$, with $k\in[1,\dots,30]$, almost equals $\mathrm{EGV}(\AAlpha_4)$, whereas the same sum for $i=1$ only achieves about the 15\% of $\mathrm{EGV}(\AAlpha_1)$. Actually, the EGV of the $4^\text{th}$, the $5^\text{th}$ and the $6^\text{th}$ component only essentially depends on a very few time samples. This observation, combined with Assumption \ref{assum:local}, suggests that they are more suitable for SCA than the three first ones. To validate this statement, it suffices to look at the form of such components (Fig.~\ref{fig:6components}): the leading ones are very influenced by the clock, while the latest ones are well localised over the leaking points.\\

Operating a selection of components {\em via} ELV, in analogy with the EGV, requires to fix the reduced space dimension $\newTraceLength$, or a threshold $\beta$ for the cumulative ELV. In the first case, the maximal ELVs of each PC are compared, and the $\newTraceLength$ components achieving the highest values of such ELVs are chosen. In the second case, all pairs (PC, time sample) are sorted in decreasing order with respect to their ELV, and summed until the threshold $\beta$ is achieved. Then only PCs contributing in this sum are selected. \\

We remark that the ELV is a score associated not only to the whole components, but to each of their coefficients. This interesting property can be exploited to further remove, within a selected PC, the non-significant points, {\em i.e.} those with a low ELV. In practice this is done by setting these points to zero. That is a natural way to exploit the ELV score in order to operate a kind of {\em denoising} for the reduced data, making them only depend  on the significant time samples. In Sec.~\ref{sec:experiments} (scenario 4) we test the performances of an attack varying the number of time samples involved in the computation of the reduced data, and showing that such a denoising processing might impact significantly. 



%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Linear Discriminant Analysis}\label{sec:LDA}

% da delle vecchie note
\subsubsection{Linear Discriminant Analysis}
%The Linear Discriminant Analysis (LDA) method is first of all a linear method for classification: given a set of training labelled elements $(\vLeakVec[i])_{i=1..\nbProfilingTraces}$, where labels $\sensVar \in \sensVarSet$ and $\vLeakVec[i]\in \mathbb{R}^\traceLength$, its aim is to propose a set of {\em linear discriminant functions} $\delta_\sensVar(\vLeakVec[i])$ that provides a good classification for new data $\vLeakVec[]$ when the classification rule is
%\begin{equation}
%\sensVar = \mathrm{argmax}_{\sensVar\in\sensVarSet} \delta_\sensVar(\vLeakVec[i]) \mbox{ .}
%\end{equation}
%
%Asking for a set of linear discriminant functions is equivalent to asking for a set of linear subspaces of $\mathbb{R}^\traceLength$ chosen as boundaries between classes. \\
%An optimal classification requires the knowledge of the posterior probabilities 
%\begin{equation}
%\prob(\sensRandVar \mid \vaLeakVec) \mbox{ . }
%\end{equation}
%
%Suppose $f_\sensVar(\sss[]{})$ is the class-conditional density of $\sss[]{}$ in the class $\sensVar$ and let $\pi_\sensVar$ be the prior probability of the class $\sensVar$. Then from Bayes and total probability theorems it follows
%\begin{equation}
%\prob(\sensRandVar = \sensVar \mid \SSS[] = \sss[]{}) = \frac{f_\sensVar(\sss[]{}) \pi_\sensVar}{\sum_{\sensVar\in\sensVarSet}f_\sensVar(\sss[]{})\pi_\sensVar} \mbox{ .} 
%\end{equation}
%
%In this context LDA arises in the special situation when we model each class density as a multivariate Gaussian and we assume that the classes have a common covariance matrix $\Sigma$:
%\begin{equation}
%f_\sensVar(\sss[]{}) = \frac{1}{(2\pi)^{\traceLength}/2\lvert \Sigma\rvert^{1/2}}e^{-\frac{1}{2}(\sss[]{}- \mu_\sensVar)^T\Sigma^{-1}(\sss[]{}- \mu_\sensVar)} \mbox{ ;}
%\end{equation}
%indeed in this special case to compare two classes $\sensVar_1$ and $\sensVar_2$ we can look at the log-ratio and obtain
%
%\begin{equation}
%\log{\frac{\prob(\sensRandVar = \sensVar_1 \mid \SSS[] = \sss[]{})}{\prob(\sensRandVar = \sensVar_2 \mid \SSS[] = \sss[]{})}}= \log{\frac{\pi_{\sensVar_1}}{\pi_{\sensVar_2}}} - \frac{1}{2}{(\mu_{\sensVar_1}+\mu_{\sensVar_2})^{T}} {\Sigma^{-1}}(\mu_{\sensVar_1}-\mu_{\sensVar_2}) + {{\sss[]{}}^{T}} {\Sigma^{-1}}(\mu_{\sensVar_1}-\mu_{\sensVar_2}) \mbox{ ,}
%\end{equation}
%
%which implies that the decision boundary between class $\sensVar_1$ and $\sensVar_2$, i.e. the set where 
%$\prob(\sensRandVar = \sensVar_1 \mid \SSS[] = \sss[]{})=\prob(\sensRandVar = \sensVar_2 \mid \SSS[] = \sss[]{})$, is a hyperplane in $\mathbb{R}^\traceLength$.\\
%Obviously, in practice the parameters $\pi_\sensVar, \mu_\sensVar $ and $\Sigma$ are not known and have to be estimated by the training data.\\
%
%Although the LDA method arises as a discriminant linear classifier, part of its popularity is due to the fact that it can be view as an informative low-dimensional projections of data, just as PCA, with the difference that LDA projections keep informativeness in the sense of discriminating features.In practice LDA boundaries divide the $\traceLength$-dimensional space into subsets such that elements belonging to the subset related to a class $\sensVar$ are closest to the centroid of training data belonging to $\sensVar$. Let $\mmmX\in\mathbb{R}^\traceLength$ be the centroid of class $\sensVar$, i.e. the estimation for $\mu_\sensVar$ given by $\mmmX = \frac{1}{\numTraces}\sum_{i=1}^{\numTraces}\sss{i}$. If $\traceLength \geq \lvert \sensVarSet \rvert$ all these centroids actually lie in an affine subspace of $\mathbb{R}^\traceLength$ of dimension $\leq \lvert \sensVarSet \rvert -1$. Moreover, in locating the closest centroid, we can ignore distances orthogonal to this subspace, since they will contribute equally to each class. Thus there is a natural dimension reduction in LDA, namely, that we need only consider the data in a subspace of dimension at most $\lvert \sensVarSet \rvert -1$. So it can be used as a practical tool for viewing in case of $\lvert \sensVarSet \rvert = 3$: a two-dimensional plot is sufficient to view classes (via color-coding) separation. In order to obtain an equivalent viewing tool in case $\lvert \sensVarSet \rvert > 3$ one can wonder if it is possible to define a 2-dimensional subspace optimal for LDA in some sense. Moreover one can be interested in a 3-dimensional optimal plot, or in general in finding an optimal subspace for each dimension $L < \lvert \sensVarSet \rvert-1$. \\
%
% Fisher defined the optimality in this context, asking for a subspace in which class centroids were spread out as much as possible in terms of variance. Actually under this optimality condition, the LDA returns a decomposition that is identical to another that is obtainable by another approach, that does not refer to Gaussian distribution at all. This approach, proposed by Fisher, amounts to maximising the {\em Rayleigh quotient}:
% \begin{equation}\label{eq:LDA}
% \mathrm{argmax}_{\AAlpha} \frac{\AAlpha^T \SB \AAlpha}{\AAlpha^T \SW \AAlpha} \mbox{ .}
% \end{equation}
%
%Here $\SB$ is called {\em Between-Class Scatter Matrix} or {\em Between-Class Covariance} and $\SW$ is called 
%{\em Within-Class Scatter Matrix} or {\em Within-Class Covariance}:
%
%\begin{equation}
%\SB = \sum_{\sensVar\in\sensVarSet}(\mmmX-\mmmXtot)^T(\mmmX-\mmmXtot)
%\end{equation}
%\begin{equation}
%\SW = \sum_{\sensVar\in\sensVarSet}\sum_{i=1}^{\numTraces}(\sss{i}-\mmmX)^T(\sss{i}-\mmmX) \mbox{ ,}
%\end{equation}
%where 
%
%\begin{equation}
%\mmmXtot = \frac{1}{\lvert \sensVarSet \rvert}\sum_{\sensVar\in\sensVarSet}\mmmX \mbox{ .}
%\end{equation}
%
%\begin{remark}
%Sometimes in the 2-classes case, i.e. $\sensVarSet = \{\sensVar_1, \sensVar_2\}$, the Between-Class Scatter is defined as $\SB = (\mmmX[\sensVar_1]-\mmmX[\sensVar_2])^T(\mmmX[\sensVar_1]-\mmmX[\sensVar_2])$.
%\end{remark}
%
%
%One can remark that the function in (\ref{eq:LDA}) is invariant with respect to rescaling of the vectors $\AAlpha \rightarrow c\AAlpha$.  So we can choose $\AAlpha$ such that the denominator is $\AAlpha^T \SW \AAlpha=1$. Indeed, suppose $\AAlpha^T \SW \AAlpha = d$, we can substitute $\AAlpha$ by $\frac{\AAlpha}{\sqrt{d}}$ to obtain the unitary denominator. For this reason the problem (\ref{eq:LDA}) is equivalent to the following constrained optimization problem
%
%\begin{equation}
%\mathrm{argmax}_{\AAlpha}{\AAlpha^T \SB \AAlpha}  \mbox{ such that } \AAlpha^T \SW \AAlpha =1 \mbox{ ,}
%\end{equation}
%or equivalently
%\begin{equation}
%\mathrm{argmin}_{\AAlpha}{-\frac{1}{2}\AAlpha^T \SB \AAlpha}  \mbox{ such that } \AAlpha^T \SW \AAlpha =1 \mbox{ ,}
%\end{equation}
%
%which corresponds to the Lagrangian
%
%\begin{equation}
%\Lambda = -\frac{1}{2}\AAlpha^T \SB \AAlpha + \frac{1}{2}\lambda( \AAlpha^T \SW \AAlpha -1) \mbox{ ,}
%\end{equation}
%
%where the halves are added for convenience. Solutions to this problem are given by vectors satisfying
%
%\begin{equation}\label{eq:genEVprob}
%\SB\AAlpha = \lambda \SW \AAlpha \quad \Rightarrow \quad \SW^{-1}\SB\AAlpha = \lambda\AAlpha \mbox{ .}
%\end{equation}
%
%The solution $\AAlpha_1$ is given by the largest eigenvector of $\SW^{-1} \SB$. This is known as a generalized eigenvalue problem, and not an eigenvalue problem because the matrix $\SW^{-1}\SB$ is not guaranteed to be symmetric. However, since $\SB$ is symmetric positive definite, it can be written as $\SB^{\frac{1}{2}}\SB^{\frac{1}{2}}$ , where $\SB^{\frac{1}{2}}$ is simply constructed from the eigenvalue decomposition of $\SB$: $\SB = U^TDU \rightarrow \SB^{\frac{1}{2}} = U^TD^{\frac{1}{2}}U$. Defining $\AAlpha' = \SB^{\frac{1}{2}} \AAlpha$ we can rewrite the first equation of (\ref{eq:genEVprob}) as
%\begin{align}
%&\SB^{\frac{1}{2}}\SB^{\frac{1}{2}}\AAlpha = \lambda \SW\AAlpha\\
%&\SB^{\frac{1}{2}}\AAlpha' = \lambda \SW\SB^{-\frac{1}{2}}\AAlpha'\\
%&\SB^{\frac{1}{2}}\SW^{-1}\SB^{\frac{1}{2}}\AAlpha' = \lambda \AAlpha' \mbox{ .}
%\end{align}
%
%Now we have a regular eigenvalue problem for the symmetric positive definite matrix $\SB^{\frac{1}{2}}\SW^{-1}\SB^{\frac{1}{2}}$. We can find solutions $\lambda_i$ and $\AAlpha'_i$ that would correspond to solutions $\AAlpha_i = \SB^{-\frac{1}{2}}\AAlpha'_i$.
%
% 
%The first eigenvector $\AAlpha_1$ is the solution to Eq.~({\ref{eq:LDA}}). The second eigenvector $\AAlpha_2$, is the solution  in the subspace orthogonal to $\langle\AAlpha_1\rangle$, and the following eigenvectors, in decreasing order with respect to the eigenvalues, are solutions in the subspace orthogonal to those generated by the previous eigenvectors. These eigenvectors are called \emph{linear discriminant coordinates} and represent the optimal decomposition under the Fisher criterion.


%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Application of LDA in SCAs}\label{sec:LDA_SCA}
\subsection{The Small Sample Size problem}



%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------


\section{Experimental Results}\label{sec:experiments}

In this section we compare the different extractors provided by the PCA and the LDA in association with the different techniques  of components selection. Defining an universal criterion to compare the different extractors would not make sense since the latter one should encompass a lot of parameters, sometimes opposite, that vary according to the context (amount of noise, specificity of the information leakage, nature of the side channel, etc.). For this reason we choose to split our comparisons into four scenarios. Each scenario has a single varying parameter that, depending on the attacker context, may wish to be minimized. Hereafter the definition of the four scenario. In the following only results of the two first is reported, the interested reader might refer to Appendix\ref{Appendix_scenario3_4_cardis2015} for results of in the two other scenarios.  
\begin{itemize}
\item[Scenario 1] varying parameter: number of attack traces $\nbAttackTraces$, 
\item[Scenario 2] varying parameter: number of profiling traces $\nbProfilingTraces$, 
\item[Scenario 3] varying parameter: number of projecting components selected $\newTraceLength$,
\item[Scenario 4] varying parameter: number of original time samples implied into the trace preprocessing computation $\numPoI$ .
\end{itemize}
 
For scenarios in which $\nbProfilingTraces$ is fixed, the value of $\nbProfilingTraces$ is chosen high enough to avoid the SSS problem, and the extensions of LDA presented in Sec.~\ref{sec:SSS} are not evaluated.
 This choice of $\nbProfilingTraces$ will imply that the LDA is always performed in a favourable situation, which makes expect the LDA to be particularly efficient for these experiments. Consequentely, for the scenarios in which $\nbProfilingTraces$ is high, our goal is to study whether the PCA can be made almost as efficient as the LDA thanks to the component selection methods discussed in Sec.~\ref{sec:ELV}. 



\todo{This part will maybe be useless: somewhere I will have described all trace sets}
\subsubsection{The testing adversary.}  
%Our testing adversary attacks an 8-bit AVR microprocessor Atmega328P and acquires power-consumption traces via the ChipWhisperer platform \cite{o2014chipwhisperer}.\footnote{This choice has been done to allow for reproducibility of the experiments.} The target device stores a secret 128-bit key and performs the first steps of an AES: the loading of 16 bytes of the plaintext, the AddRoundKey step and the AES Sbox. It has been programmed twice: two different keys are stored in the device memory during the acquisition of the profiling and of the attack traces, to simulate the situation of two identical devices storing a different secret. The size $\traceLength$ of the traces equals $3996$. The sensitive target  variable is the output of the first Sbox processing, but, since the key is fixed also during the profiling phase, and both Xor and Sbox operations are bijective, we expect to detect three interesting regions (as those high-lighted by PCs 4, 5 and 6, in Fig.~\ref{fig:6components}): the reading of the first byte of the plaintext, the first AddRoundKey and the first Sbox. We consider an {\em identity classification} leaking function (i.e. we make minimal assumption on the leakage function), which implies that the 256 possible values of the Sbox output yields to 256 classes. For each class we assume that the adversary acquires the same number $N_p$ of traces, \textit{i.e.} $\numTraces[]' = N_p\times 256$. After the application of the extractor $\extract$, the trace size is reduced to $\newTraceLength$. Then the attacker performs a Bayesian Template Attack \cite{Chari2003}, using $\newTraceLength$-variate Gaussian templates. This choice comes from the information-theoretic optimality of such an attack which, exploiting the maximum likelihood parameters estimation, yields to an unbiased comparison between the extractors.


\begin{figure}[t]
\subfigure[]{\label{fig:1.1}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015//Criterion1.pdf}}
\subfigure[]{\label{fig:1.2}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015//Criterion1Good.pdf}}
\caption[Guessing Entropy as function of the number of attack traces]{Guessing Entropy as function of the number of attack traces for different extraction methods. All Guessing Entropies are estimated as the average rank of the right key over 100 independent experiments.}\label{fig:scenario1}
\end{figure}
\subsubsection{Scenario 1.}
\todo{cos'e $N_z$}
To analyse the dependence between the extraction methods presented in Sections~\ref{sec:PCA} and \ref{sec:LDA} and the number of attack traces $\nbAttackTraces$ needed to achieve a given GE, we fixed the other parameters as follows: $N_z=50$ ($\nbProfilingTraces=50\times 256$), $\newTraceLength = 3$ and $\numPoI = 3996$ (all points are allowed to participate in the building of the PCs and of the LDCs). The experimental results, depicted in Fig.~\ref{fig:scenario1}\subref{fig:1.1}-\subref{fig:1.2}, show that the PCA standard method has very bad performances in SCA, while the LDA outperforms the others. Concerning the class-oriented PCA, we observe that its performance is close to that of LDA when combined with the selection methods ELV (which performs best) or IPR.  



\subsubsection{Scenario 2.}
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS.pdf}
%\includegraphics[width=0.5\textwidth]{figures/Criterion2notSSS.pdf} 
%\caption{Guessing Entropy as function of the number of profiling traces per class, for different extraction methods: on the left the LDA is substituted by its extensions to handle the SSS problem.}\label{fig:2}
%\end{figure}
Now we test the behaviour of the extraction methods as function of the number $N_z$ of available profiling traces per class. The number of components $\newTraceLength$ is still fixed to 3, $\numPoI=3996$ again and the number of attack traces is $\nbAttackTraces=100$. This scenario has to be divided into two parts: if $N_z\leq 15$, then $\nbProfilingTraces<\traceLength$ and the SSS problem occurs. Thus, in this case we test the four extensions of LDA presented in Sec.~\ref{sec:SSS}, associated to either the standard selection, to which we abusively refer as EGV,%\footnote{It consists in keeping the $\newTraceLength$ first LDCs, except for the Direct LDA, which asks to keep the last LDCs.}
\footnote{It consists in keeping the $\newTraceLength$ first LDCs (the $C$ last for the Direct LDA)}
or to the IPR selection.  We compare them to the class-oriented PCA associated to EGV, IPR or ELV. The ELV selection is not performed for the techniques extending LDA, since for some of them the projecting LDCs are not associated to some eigenvalues in a meaningful way. On the contrary, if $N_z\geq 16$ there is no need to approximate the LDA technique, so the classical one is performed. Results for this scenario are shown in Fig.~\ref{fig:scenario2}. It may be noticed that the combinations class-oriented PCA + ELV/IPR select exactly the same components, for our data, see Fig.~\ref{fig:pcaclass} and do not suffer from the lack of profiling traces. They are slightly outperformed by the $\SW$ Null Space method associated with the EGV, see Fig.\ref{fig:swnullspace}. The Direct LDA (Fig.~\ref{fig:direct}) method also provides a good alternative, while the other tested methods do not show a stable behaviour. The results in absence of the SSS problem (Fig.\ref{fig:notSSS}) confirm that the standard PCA is not adapted to SCA, even when provided with more profiling traces. It also shows that among class-oriented PCA and LDA, the class-oriented PCA converges faster.



\begin{figure}
\subfigure[]{\label{fig:fisherface}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_Fisherface.pdf}}
\subfigure[]{\label{fig:direct}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_DirectLDA.pdf}}
\subfigure[]{\label{fig:stspannedspace}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_STSpannedSpace.pdf}}
\subfigure[]{\label{fig:swnullspace}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_SWnullSpace.pdf}}
\subfigure[]{\label{fig:pcaclass}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_PCAclass.pdf}}
\subfigure[]{\label{fig:notSSS}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2notSSS.pdf}}
\caption[Guessing Entropy as function of the number of profiling traces.]{Guessing Entropy as function of the number of profiling traces. Figures \subref{fig:fisherface}-\subref{fig:swnullspace}: methods extending the LDA in presence of SSS problem; Figure \subref{fig:pcaclass}: class-oriented PCA in presence of the SSS problem; Figure \subref{fig:notSSS}: number of profiling traces high enough to avoid the SSS problem.}\label{fig:scenario2}
\end{figure}

%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------
\section{Misaligning Effects}\label{sec:misalignment}
\todo{give parameters: 6 4}
\todo{citare Choudary, Template Attacks over different devices}
In this section we experimentally show how the approach based on linear dimensionality reduction described in this chapter is affected by traces misalignment. To this aim, we simply take the same data and parameters exploited for Scenario 1 in Sec.~\ref{sec:experiments}, and artificially misalign them through the technique proposed in Appendix~\ref{appendix:artificial_jitter} with parameters \todo{parameters here}. Then we tried to pre-process attack them through the 9 methodology tested in Scenario 1. It may be noticed in Fig.~\ref{fig:PCA_LDA_misalignment} that none of the 9 techniques is still efficient, included the optimal LDA+EGV that lead to null guessing entropy the synchronized traces using less 7 attack traces. In this case it cannot lead to successful attack in less than 3000 traces.
\begin{figure}
\includegraphics[width=\textwidth]{../Figures/desynchro_results_PCA_LDA.pdf} 
\caption{Degradation of linear-reduction-based template attacks in presence of misalignment.}\label{fig:PCA_LDA_misalignment}
\end{figure}