% Chapter Template

\chapter{Linear Dimensionality Reduction} 
\label{ChapterLinear}
\setlength{\epigraphwidth}{0.7\textwidth}
\epigraph{\textit{\textquotedbl One side will make you grow taller, and the
other side will make you grow shorter.\textquotedbl \\
\textquotedbl One side of what? The other side of what?\textquotedbl \ thought Alice to herself.\\
\textquotedbl Of the mushroom,\textquotedbl \ said the Caterpillar, just as if she had asked it aloud; and
in another moment it was out of sight.}}{--- \textup{Lewis Carroll --- \textquotedbl Alice's Adventures in Wonderland\textquotedbl}}


In this chapter, we explore solutions for dimensionality reduction of side-channel traces exploiting linear combinations of time samples. The results presented in this chapter have been published in the proceedings of CARDIS 2015 \cite{Cagli2016}.

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}\label{sec:intro_chapter_linear}
Linear dimensionality reduction methods produce a low-dimensional linear mapping of the original high-dimensional data that preserves some original feature of interest. An abundance of methods has been developed throughout statistics, machine learning, and applied fields for over a century, and these methods have become indispensable tools for analysing high dimensional, noisy data, such as side-channel traces.  Accordingly, linear dimensionality reduction can be used for visualizing or exploring structures in data, denoising or compressing data, extracting meaningful feature spaces, and more. A very complete survey about this great variety of linear dimensionality reduction technique has been published in 2015 by Cunningham and Ghahramani \cite{cunningham2015linear}. They proposed a generalized optimization framework for all linear dimensionality techniques, survey a dozen different techniques and mention some important extensions such as kernel mappings. \\

Among the surveyed methods in \cite{cunningham2015linear} we find the two ones that are mainly considered in SCA literature: the Principal Component Analysis (PCA) and the Linear Discriminant Analysis (LDA). The PCA has been applied both in an {\em unsupervised} way (\ie non-profiling attacks) \cite{Batina2012,karsmakers2009side}, and in a {\em supervised} way (\ie profiling attacks) \cite{TAprincipal,choudaryefficient,choudary2014efficient,disassembler,Standaert2008}. As already remarked in \cite{disassembler}, and not surprisingly, the complete knowledge assumed in the supervised approach hugely raises performances. The main competitor of PCA in the profiling attacks context is the LDA, that thanks to its classification-oriented flavour (see Sec.~\ref{example:LDA}), is known to be more meaningful and informative \cite{lessIsMore,Standaert2008} than the PCA method  for side channels. Nevertheless, the LDA is often set aside because of its practical constraints; it is subject to the so-called {\em Small Sample Size problem (SSS)}, i.e. it requires a number of observations (traces) which must be higher than the dimension (size) $\traceLength$ of them. In some contexts it might be an excessive requirement, which may become unacceptable in many practical situations where the amount of observations is very limited and the traces size is huge.\\

In 2014 Durvaux et al. proposed the use of another technique for linear dimensionality reduction in SCA context \cite{PP}, the so-called Projection Pursuits (PPs), firstly introduced in 1974 by Friedman and Tukey \cite{friedman1974projection}. This method essentially works by randomly picking time samples, randomly setting the projecting coefficients, and tracking the improvement (or the worsening) of the projection when modifying
them with small random perturbations. The main drawback of the PPs  pointed out by the authors of \cite{PP} for the SCA context is their heuristic nature,
since the convergence of the method is not guaranteed and its complexity is
context-dependent. The main advantage is the fact that
PPs can deal with any objective function, which may be adjusted to fit the problem
of higher-order SCA. Thus this technique appears advantageous in higher-order context, where it is used as a PoI selection tool. Its version for the first-order attacks, which produces a linear dimensionality reduction, is less interesting than the non-heuristic PCA and LDA. For this reason we will left PPs technique apart in this chapter, and describe their higher-order version in Chapter~\ref{ChapterKernel}.\\

 In SCA literature, one of the open issues for PCA application concerns the choice of the principal components that must be kept after the dimension reduction: as already remarked by Specht et al.  \cite{specht}, some papers declare that the leading components are those that contain almost all the useful information \cite{TAprincipal,choudary2014efficient}, while others propose to discard the leading components \cite{Batina2012}. In a specific attack context, Specht et al. compares the results obtained by choosing different subsets of consecutive components, starting from some empirically chosen index. They conclude that for their data the optimal result is obtained by selecting a single component, the fourth one, but they give no formal argumentation about this choice. Such a result is obviously very case-specific. Moreover, the possibility of keeping non-consecutive components is not considered in their analysis. \\

 
In Sec.~\ref{sec:PCA} the classical PCA technique is described, then the previous applications of PCA in SCA context are recalled, highlighting the difference between its unsupervised and supervised declination. Finally our contribution to  \textquotedbl the choice of components open issue is described\textquotedbl \  is described:  it is based on the Explained Local Variance (ELV) notion, that we will define and argument in the same section. The reasoning behind the ELV selection methodology is essentially based on the observation that, for secure implementations, the leaking information, if existing, is spread over a few time samples of each trace. This observation has already been met by Mavroeidis et al. in \cite{SCAclassProbl}, where the authors  also proposed a components selection method. As we will see in Sec.~\ref{sec:ELV}, the main difference between their proposal and ours is that in in \cite{SCAclassProbl} the information given by the eigenvalues associated to the PCA components is completely discarded, while the ELV methodology takes advantage of such information as well.  We will argue about the generality and the soundness of this methodology and we will show that it can raise the PCA performances, making them close to those of the LDA, even in the supervised context. This makes PCA an interesting alternative to LDA in those cases where the LDA is inapplicable due to the SSS problem. The ELV selection tool has been tested in a successive experimental work \cite{choudary2018efficient}. Unfortunately, the authors of this work could not observe an improvement (nor a worsening) using our new selector, because in their specific case its selection of components were equivalent to the classical one, that will be referred to as EGV in the following.\\

 The LDA technique will be described in Sec.~\ref{sec:LDA}, together with the description of the SSS problem and some solutions coming from the Pattern and Face Recognition communities \cite{eigenfaces,Chen2000,huang,Yu01adirect}. Through some experiments depicted in Sec.~\ref{sec:experiments} we will conclude about the effectiveness of the PCA-ELV solution. Finally, in Sec.~\ref{sec:misalignment} we will experimentally argue about the weakness of all these techniques when data are misaligned.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Principal Component Analysis} \label{sec:PCA}
\subsection{Principles and algorithm description}
\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/PCAprojection_unsupervised.pdf} 
\caption{PCA: some 2-dimensional data (blue crosses) projected into their 1-dimensional principal subspace (represented by the green line).}\label{fig:PCAunsupervised}
\end{figure}
The Principal Component Analysis (PCA) is a technique for data dimensionality reduction. The PCA algorithm can be deduced from two different points of view, a statistical one and a geometrical one. In the former one, PCA aims to project orthogonally the data onto a lower-dimensional linear space, the so-called \emph{principal subspace}, such that the variance of the projected data is maximized. In the latter one, PCA aims to project data onto a lower-dimensional linear space in such a way that the average projection cost, defined as the mean square distance between the data and their projections, is minimized. In the following it is shown how the PCA algorithm is deduced by the statistical definition. The reader interested by the equivalence between the two approaches can refer to \cite[Ch.\ 12]{christopher2006pattern}. An example of 2-dimensional data projected over their 1-dimensional principal subspace is depicted in Fig.~\ref{fig:PCAunsupervised}.\\

Let $(\vLeakVec)_{i=1..\nbTraces}$ be a set of $\traceLength$-dimensional measurements (or observations, or data), i.e. realizations of a $\traceLength$-dimensional zero-mean random vector $\vaLeakVec{}$, and collect them as columns of an $\traceLength \times \nbTraces$ matrix $\measuresMatrix$, so that the empirical covariance matrix of $\vaLeakVec{}$ can be computed as 
\begin{equation}\label{eq:covmat}
\covmat = \frac{1}{\nbTraces}\measuresMatrix\measuresMatrix^\intercal \mbox{ .}
\end{equation}

Let us first assume that we have priorly fixed the dimension $\newTraceLength<\traceLength$ of the principal subspace we are looking for. 

\paragraph*{Compute the First Principal Component}
Suppose in a first time that $\newTraceLength = 1$, i.e. that we want to represent our data by a unique variable $Y_1 =  \AAlpha_1^\intercal \vaLeakVec{}$, i.e. projecting data over a single $\traceLength\times 1$ vector $\AAlpha_1$, in such a way the variance of the obtained data is maximal. The vector $\AAlpha_1$ that provides such a linear combination is called {\em first principal component}. 
To avoid misunderstanding we will call {\em $j$-th principal component} (PC) the projecting vector $\AAlpha_j$, while we will refer to the variable $Y_j = \AAlpha_j^\intercal \vaLeakVec$ as the {\em $j$-th Principal Variable (PV)}. 
Realizations of the PVs are given by the measured data projected over the $j$-th PC, for example we can collect, in a vector $\yyy_1^\intercal = \AAlpha_1^\intercal \measuresMatrix $, $\nbTraces $ realizations of $Y_1$:
\begin{equation}
y_1[i] = \AAlpha_1^\intercal \vLeakVec_i \mbox{ for } i=1,\dots , \nbTraces \mbox{ .}
\end{equation}

The mean of these realizations will be zero as they are linear combinations of zero-mean variables, and the variance turns to be estimable as
\begin{equation}\label{eq:PCAobjective}
\frac{1}{\nbTraces}\yyy_1\yyy_1^\intercal = \frac{1}{\nbTraces}\AAlpha_1^\intercal\measuresMatrix\measuresMatrix^\intercal\AAlpha_1 = \AAlpha_1^\intercal\covmat\AAlpha_1 \mbox{ .}
\end{equation}
To compute $\AAlpha_1$ we look for the vector that maximises the variance estimate in \eqref{eq:PCAobjective}.\\

The maximisation problem by itself is not well posed, because the variance value is not limited until a restriction is not imposed to the modulo $\|\AAlpha_1| = \sqrt{\AAlpha_1^\intercal\AAlpha_1}$. In order to let the maximization problem have a solution, a restriction is thus imposed: $\AAlpha_1^\intercal\AAlpha_1 = 1$. 
This constrained  optimization problem is handled by making use of Lagrange multipliers:
\begin{equation}
\Lambda(\AAlpha_1, \lambda) = \AAlpha_1^\intercal\covmat\AAlpha_1 - \lambda(\AAlpha_1^\intercal\AAlpha_1-1) \mbox{ ,}
\end{equation}
and by computing the partial derivative of $\Lambda$ with respect to $\AAlpha_1^\intercal$:
\begin{equation}
\frac{\partial\Lambda}{\partial\AAlpha_1^\intercal} = 2\covmat\AAlpha_1-2\lambda\AAlpha_1 \mbox{ .}
\end{equation}
Thus, stationary points of $\Lambda$ verify:
\begin{equation}\label{eq:eigProblemPCA}
\covmat\AAlpha_1 = \lambda\AAlpha_1 \mbox{ ,}
\end{equation}
which implies that $\AAlpha_1$ must be an eigenvector of $\covmat$, with $\lambda$ its correspondent eigenvalue. Multiplying both sides of Eq.~(\ref{eq:eigProblemPCA}) by $\AAlpha_1^\intercal$ on the left, we remark that
\begin{equation}
\AAlpha_1^\intercal\covmat\AAlpha_1 = \lambda\AAlpha_1^\intercal\AAlpha_1 = \lambda, 
\end{equation}
which means that the variance of the obtained variable $\yyy_1$ equals $\lambda$. For this reason $\AAlpha_1$ must be the leading eigenvector of $\covmat$, the one corresponding to the maximal eigenvalue.

\paragraph*{Compute the Second and Following Principal Components}
The PCs others than the first are defined in an incremental fashion by choosing new directions orthogonal to those already considered and such that the sum of the projected variances over each direction is maximal. Explicitly, if we look for two PCs, \ie $\newTraceLength = 2$,  we look for a $2$-dimensional variable $\YYY = \begin{psmallmatrix} \AAlpha_1^\intercal \\ \AAlpha_2^\intercal \end{psmallmatrix} \vaLeakVec$ such that the trace of its covariance matrix, \ie the sum of variances $\var(Y_1)+\var(Y_2)$, is maximal. It can be shown that the same result would be obtained by maximising the so-called \emph{generalized variance} of $\YYY$, which is defined as the determinant of its covariance matrix, instead of its trace. \\

Consider, as in previous case, the Lagrangian of the problem
\begin{equation}\label{eq:lagrangian_2comps}
\Lambda = \AAlpha_1^\intercal \covmat \AAlpha_1 + \AAlpha_2^\intercal\covmat \AAlpha_2 - \lambda_1(\AAlpha_1^\intercal\AAlpha_1 -1) - \lambda_2(\AAlpha_2^\intercal\AAlpha_2 -1) \mbox{ .}
\end{equation}

The partial derivatives of \eqref{eq:lagrangian_2comps} with respect to $\AAlpha_1^\intercal$ and $\AAlpha_2^\intercal$ are null under the following conditions:
\begin{align}
\covmat  \AAlpha_1 &= \lambda_1\AAlpha_1 \\
\covmat  \AAlpha_2 &= \lambda_2\AAlpha_2 \mbox{ .}
\end{align}

It means that $\AAlpha_1$ and $\AAlpha_2$ must be eigenvectors of $\covmat$ with corresponding eigenvalues given by $\lambda_1$ and $\lambda_2$. Moreover, as before, $\lambda_1$ and $\lambda_2$ respectively equal the estimated variances of the variable components $Y_1$ and $Y_2$, and since the goal is maximizing the sum of these variables we choose $\AAlpha_1$ and $\AAlpha_2$ as the two leading vectors of $\covmat$. Let us remark that the estimated covariance between $Y_1$ and $Y_2$ is given by $\AAlpha_1^\intercal\covmat \AAlpha_2$ which equals zero, since $\AAlpha_1^\intercal\AAlpha_2 = 0$ by orthogonality. In particular the principal variables are uncorrelated, which is a remarkable property of the PCA. \\

In the general case of a $\newTraceLength$-dimensional projection space, it can be shown by induction that the PCs would correspond to the $\newTraceLength$ leading eigenvectors of the covariance matrix $\covmat$.



%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\subsection{Original vs Class-Oriented PCA}
\begin{figure}[t]
\subfigure[]{\label{fig:PCAunsupervised_lab}
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/PCAprojection.pdf}}
\subfigure[]{\label{fig:PCAsupervised_lab}
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/PCA_class_projection.pdf}}
\caption{PCA: some 2-dimensional labelled data (blue crosses and red circles) projected into their 1-dimensional principal subspaces (represented by the green line). \subref{fig:PCAunsupervised_lab} classical unsupervised PCA, \subref{fig:PCAsupervised_lab} class-oriented PCA. In \subref{fig:PCAsupervised_lab} black stars represents the 2 classes centroids (sample means).}\label{fig:2class-toys}
\end{figure}
The classical version of the PCA method is unsupervised.
% in the sense that is does not take into account the information about the value assumed by the target variable during the acquisition of data. 
On the other hand a profiling attacker is not only provided with a set of traces $(\vLeakVec_i)_{i=1..\nbTraces}$, but he also has the knowledge of the target values handled during each acquisition. We denote by $(\vLeakVec_i, \sensVar_i)_{i=1..\nbProfilingTraces}$ the labelled set of traces . In Fig.~\ref{fig:2class-toys} the same data of Fig.~\ref{fig:PCAunsupervised} have been grouped into 2 classes. Even if before projection the two groups are clearly separable, after projecting data over the first principal component given by the classical PCA algorithm, the separability is lost (Fig.\ref{fig:PCAunsupervised_lab}. In the supervised context, and for the sake of distinguishing the target value assumed by the target $\sensRandVar$ in new executions, the idea of the {\em Class-Oriented} PCA is to consider as {\em equivalent} all the traces belonging to the same class. Modelling traces of a same class as traces showing the same characteristic form plus a random noise, the denoised characteristic form can be estimated by the sample means of the traces in the class. Let us recall from \eqref{eq:mmmXclass} that the empirical mean of traces belonging to the same class $\sensVarGenValue$ is given by
\begin{equation*}
\mmmXclass = \frac{1}{\nbTracesPerClass}\sum_{i\colon \sensVar_i=\sensVarGenValue} \vLeakVec_i \mbox{ ,}
\end{equation*}
where  $\nbTracesPerClass$ is the number of traces belonging to class $\sensVarGenValue$. The class-oriented version of the PCA  consists in applying the PCA dimensionality reduction to the set $(\mmmXclass)_{\sensVarGenValue \in \sensVarSet}$, instead of applying it directly to the traces $\vLeakVec_i$. This implies that the empirical covariance matrix will be computed using only the $\nbClasses$ average traces. Equivalently, in case of \textit{balanced} acquisitions ($\nbTracesPerClass$ constant for each class $\sensVarGenValue$), it amounts to replace the empirical covariance matrix $\covmat$ of data in \eqref{eq:covmat}  by the so-called {\em between-class} or  {\em inter-class scatter matrix}, given by:
\begin{equation}\label{eq:SB}
\SB = \sum_{\sensVarGenValue\in\sensVarSet}\nbTracesPerClass(\mmmXclass-\mmmX)(\mmmXclass-\mmmX)^\intercal \mbox{ .}
\end{equation}
Remark that $\SB$ coincides, up to a multiplicative factor, to the covariance matrix obtained using the class-averaged traces. In this way we focus the attention on information that discriminate the characteristic forms of classes, \ie target values. Figure~\ref{fig:PCAsupervised_lab} shows how the 2-class toy data are projected over the first class-oriented principal component: in the figure, black stars represent the class sample means. Projected data are slightly better separated than in Fig.~\ref{fig:PCAunsupervised_lab}.



\subsection{Computational Consideration}\label{sec:trick}
Performing PCA (and LDA, as explained later) always implies to compute the eigenvector of some symmetric matrix $\covmat$, obtained by the multiplication of a constant with a matrix and the transposed same matrix, \eg $\covmat = \frac{1}{\nbTraces}\measuresMatrix\measuresMatrix^\intercal$ . Let $\measuresMatrix$ have dimension $\traceLength\times\nbTraces$, and suppose $\nbTraces\ll \traceLength$. This condition is almost always satisfied when performing class-oriented PCA, because in such a case $\nbTraces$ corresponds to the number of classes $\nbClasses$, and $\traceLength$ is the traces' size. Anyway, for high-dimensional data, \ie $\traceLength$ high, it can be satisfied even when performing classical PCA. Thus, in such a common case, the $\traceLength\times\traceLength$ matrix $\covmat$ is far from being a full-rank matrix, since $\mathrm{rank}(\covmat)\leq \nbTraces\ll \traceLength$. For this reason, we expect to find at most $\nbTraces$ eigenvectors; often columns of $\measuresMatrix$ are linearly dependent, because for example they are forced to have zero mean, so actually the rank of $\covmat$ is strictly less than $\nbTraces$ and we expect to obtain at most $\nbTraces-1$ eigenvectors.

A practical problem in case of high-dimensional data, is represented by the computation and the storage of the $\traceLength\times\traceLength$ matrix $\covmat$. This problem can be bypassed by exploiting the following lemma coming from linear algebra, as proposed by Archambeau \etal \cite{TAprincipal}:
\begin{lemma}
For any $\traceLength\times\nbTraces$ matrix $\measuresMatrix$, the function $\vLeakVec\mapsto \measuresMatrix\vLeakVec$ is a one-to-one mapping that maps eigenvectors of $\measuresMatrix^\intercal\measuresMatrix$ ($\nbTraces\times\nbTraces$) onto those of $\measuresMatrix\measuresMatrix^\intercal$ ($\traceLength\times\traceLength$).
\end{lemma}
This lemma allows to compute and store the smaller $\nbTraces\times\nbTraces$ matrix $\tilde{\covmat} = \frac{1}{\nbTraces}\measuresMatrix^\intercal\measuresMatrix$, to compute its ($\nbTraces\times 1$)-sized eigenvectors $\BBeta_i$ and the relative eigenvalues $\lambda_i$, and then to convert them into eigenvectors of $\covmat$, given by $\AAlpha_i =\measuresMatrix \BBeta_i$. Observing that by definition $\tilde{\covmat}\BBeta_i =\frac{1}{\nbTraces}\measuresMatrix^\intercal\measuresMatrix\BBeta_i =  \lambda_i\BBeta_i$ the lemma is easy to verify: 
\begin{equation}
\covmat\AAlpha_i = \frac{1}{\nbTraces}\measuresMatrix\measuresMatrix^\intercal \measuresMatrix \BBeta_i = \lambda_i\measuresMatrix\BBeta_i = \lambda_i\AAlpha_i \mbox{ .}
\end{equation}
However, it is not guaranteed that eigenvectors $\AAlpha_i$ obtained in this way have norm equal to 1. This is why a normalization step usually follows.



\subsection{The Choice of the Principal Components}\label{sec:ELV}
The introduction of the PCA method in SCA context (either in its classical or class-oriented version)  has raised some non-trivial questions: \textit{how many} principal components and \textit{which ones} are sufficient/necessary to reduce the trace size (and thus the attack processing complexity) without losing important discriminative information?\\

Until 2015, the sole attempt to give an answer to the questions above was made in \cite{choudary2014efficient}, linked to the concept of {\em explained variance} (or {\em explained global variance}, EGV for short) of a PC $\AAlpha_i$:
\begin{equation}\label{eq:EGV}
\mathrm{EGV}(\AAlpha_i) =  \frac{\lambda_i}{\sum_{k=1}^r\lambda_k} \mbox{ ,}
\end{equation}
where $r$ is the rank of the covariance matrix $\covmat$, and $\lambda_j$ is the eigenvalue associated to the $j$-th PC $\AAlpha_j^\intercal$. $\mathrm{EGV}(\AAlpha_i)$ is the variance of the data projected over the $i$-th PC (which equals $\lambda_i$) divided by the total variance of the original data (given by the trace of the covariance matrix $\covmat$, \ie by the sum of all its non-zero eigenvalues). By definition of EGV, the sum of all the EGV values is equal to $1$; for this reason this quantity is often multiplied by $100$ and expressed as percentage.
Exploiting the EGV to choose among the PCs consists in fixing a wished {\em cumulative explained variance} $\beta$ and in keeping $\newTraceLength$ different PCs, where $\newTraceLength$ is the minimum integer such that
\begin{equation}
\mbox{EGV}(\AAlpha_1) +\mbox{EGV}(\AAlpha_2) + \dots +\mbox{EGV}(\AAlpha_\newTraceLength) \geq \beta \mbox{ .}
\end{equation}
However, if the attacker has a constraint for the reduced dimension $\newTraceLength$, the EGV notion simply suggests to keep the first $\newTraceLength$ components, taking for granted that the optimal way to chose PCs is in their natural order. This assumption is not always confirmed in SCA context: in some works, researchers have already remarked that the first components sometimes contain more noise than information \cite{Batina2012,specht} and it is worth discarding them. For the sake of providing a first example of this behaviour on publicly accessible traces, we applied a class-oriented PCA on 3000 traces from the DPA contest v4 \cite{DPAcontest}; we focused over a small 1000-dimensional window in which, in complete knowledge about masks and other countermeasures, information about the first Sbox processing leaks (during the first round). In Fig.~\ref{fig:DPAcontest} the first and the sixth PCs are plotted. It may be noticed that the first component indicates that one can attend a high variance by exploiting the regularity of the traces, given by the clock signal, while the sixth one has high coefficients localised in a small time interval, very likely to signalize the instants in which the target sensitive variable leaks.

\begin{figure}
\includegraphics[width=.45\textwidth]{../Figures/CARDIS2015/DPAcontestPC1_new.pdf} 
\includegraphics[width=.45\textwidth]{../Figures/CARDIS2015/DPAcontestPC6_new.pdf} 
\caption[First and sixth PCs in DPA contest v4 trace set.]{First and sixth PCs in DPA contest v4 trace set (between time samples 198001 and 199000)}\label{fig:DPAcontest}
\end{figure}
A single method adapted to SCA context has been proposed until 2015 to automatically choose PCs \cite{SCAclassProbl} while dealing with the issue raised in Fig.~\ref{fig:DPAcontest}. It was based on the following assumption:
\begin{assumption}\label{assum:local}
The leaking side-channel information is localised in few points of the acquired trace.
\end{assumption}
This assumption is reasonable in SCA contexts where the goal of the security developers is to minimize the number of leaking points.
Under this assumption, the authors of \cite{SCAclassProbl} use for side-channel attack purposes the {\em Inverse Participation Ratio} (IPR), a measure widely exploited in Quantum Mechanics domain (see for example \cite{guhr1998random}). They propose to use such a score to evaluate the eigenvectors {\em localization}. It is defined as follows:
\begin{equation}
\mathrm{IPR}(\AAlpha_i) = \sum_{j=1}^\traceLength \AAlpha_i[j]^4 \mbox{ .}
\end{equation}
The authors of \cite{SCAclassProbl} suggest to collect the PCs in decreasing order with respect to the IPR score.\\

The selection methods provided by the evaluation of the EGV and of the IPR are somehow complementary: the former one is based only on the eigenvalues associated to the PCs and does not consider the form of the PCs themselves; the latter completely discards the information given by the eigenvalues of the PCs, considering only the distribution of their coefficients. In the next section we describe a new method, part of the contributions published in \cite{Cagli2016}, that builds a bridge between the EGV and the IPR approaches. As we will argue, our method, based on the so-called {\em explained local variance}, does not only lead to the construction of a new selection criterion, but also permits  to modify the PCs, choosing individually the coefficients to keep and those to discard. 

\subsubsection{Explained Local Variance Selection Method}\label{sec:ELVmethod}
The method we develop in this section is based on a compromise between the variance provided by each PC (more precisely its EGV) and the number of time samples necessary to achieve a consistent part of such a variance. To this purpose we  introduce the concept of {\em Explained Local Variance} (ELV).
\begin{figure}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/cumulativeELVallRectangle.pdf} 
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/cumulativeELVzoomedRectangle.pdf} 
\caption[Cumulative ELV trend of principal components.]{Cumulative ELV trend of principal components. On the right a zoom of the plot on the left. Data acquisition described in Sec.~\ref{sec:experiments}.}\label{fig:ELVcumulative}
\end{figure}
%
Let us start by giving some intuition behind our new concept. Thinking to the observations ${\vLeakVec}$, or to the class-averages ${\mmmX}$ in class-oriented PCA case, as realizations of a random variable $\vaLeakVec$, we have that $\lambda_i$ is an estimator for the variance of the random variable $\vaLeakVec^\intercal\AAlpha_i$. Developing, we obtain
\begin{align}\label{eq:ELV}
\lambda_i =& \hat{\var}(\sum_{j=1}^\traceLength \vaLeakVec^\intercal[j]\AAlpha_i[j]) = \sum_{j=1}^\traceLength\sum_{k=1}^\traceLength \hat{\cov}(\vaLeakVec^\intercal[j]\AAlpha_i[j], \vaLeakVec^\intercal[k]\AAlpha_i[k])=\\
=& \sum_{j=1}^\traceLength \AAlpha_i[j]\sum_{k=1}^\traceLength\AAlpha_i[k]\hat{\cov}(\vaLeakVec^\intercal[j], \vaLeakVec^\intercal[k])= \sum_{j=1}^\traceLength \AAlpha_i[j] (\covmat_{j}^\intercal \AAlpha_i)=  \\
=& \sum_{j=1}^\traceLength \AAlpha_i[j] \lambda_i\AAlpha_i[j]= \sum_{j=1}^\traceLength  \lambda_i \AAlpha_i[j]^2 \label{eq:toJustify}
\end{align}
where $\covmat_{j}^\intercal$ denotes the $j$-th row of $\covmat$ and \eqref{eq:toJustify} is justified by the fact that $\AAlpha_i$ is an eigenvector of $\covmat$, with $\lambda_i$ its corresponding eigenvalue. The result of this computation is quite obvious, since $\parallel \AAlpha_i\parallel=1$, but it evidences the contribution of each time sample in the information held by the PC. This makes us introduce the following definition:
\begin{definition}


The {\em Explained Local Variance} of a PC $\AAlpha_i$ in a sample $j$, is defined by
\begin{equation}
\mathrm{ELV}(\AAlpha_i,j) = \frac{\lambda_i \AAlpha_i[j]^2}{\sum_{k=1}^r\lambda_k} = \mathrm{EGV}(\AAlpha_i) \AAlpha_i[j]^2  \mbox{ .}
\end{equation}
\end{definition}
\begin{figure}
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC1.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC2.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC3.pdf} \\
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC4.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC5.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC6.pdf} 
\caption[The first six PCs. Acquisition campaign on an 8-bit AVR Atmega328P.]{The first six PCs. Acquisition campaign on an 8-bit AVR Atmega328P (see Sec.~\ref{sec:experiments}).}\label{fig:6components}
\end{figure}
Let $\mathcal{J}=\{j^i_1, j^i_2, \dots, j^i_{\traceLength}\}\subset\{1,2,\dots,\traceLength\}$ be a set of indexes sorted such that $\mathrm{ELV}(\AAlpha_i,j^i_1)\geq \mathrm{ELV}(\AAlpha_i,j^i_2)\geq \dots \geq \mathrm{ELV}(\AAlpha_i,j^i_\traceLength)$.
It may be observed that the sum over all the $\mathrm{ELV}(\AAlpha_i,j)$, for $j\in[1,\dots,\traceLength],$   equals $\mathrm{EGV}(\AAlpha_i)$. If we operate such a sum in a cumulative way following the order provided by the sorted set $\mathcal{J}$, we obtain a complete description of the trend followed by the component $\AAlpha_i$ to achieve its EGV. As we can see in Fig.~\ref{fig:ELVcumulative}, where such cumulative ELVs are represented, the first 3 components are much slower in achieving their final EGV, while the $4^\text{th}$, the $5^\text{th}$ and the $6^\text{th}$ achieve a large part of their final EGVs very quickly ({\em i.e.} by adding the ELV contributions of much less time samples). For instance, for $i=4$, the sum of the $\mathrm{ELV}(\AAlpha_4, j^4_k)$, with $k\in[1,\dots,30]$, almost equals $\mathrm{EGV}(\AAlpha_4)$, whereas the same sum for $i=1$ only achieves about the 15\% of $\mathrm{EGV}(\AAlpha_1)$. Actually, the EGV of the $4^\text{th}$, the $5^\text{th}$ and the $6^\text{th}$ component only essentially depends on a very few time samples. This observation, combined with Assumption \ref{assum:local}, suggests that they are more suitable for SCA than the three first ones. To validate this statement, it suffices to look at the form of such components (Fig.~\ref{fig:6components}): the leading ones are strongly influenced by the clock, while the latest ones are well localised over the leaking points.\\

Operating a selection of components {\em via} ELV, in analogy with the EGV, requires to fix the reduced space dimension $\newTraceLength$, or a threshold $\beta$ for the cumulative ELV. In the first case, the maximal ELVs of each PC are compared, and the $\newTraceLength$ components achieving the highest values of such ELVs are chosen. In the second case, all pairs (PC, time sample) are sorted in decreasing order with respect to their ELV, and summed until the threshold $\beta$ is achieved. Then only PCs contributing in this sum are selected. \\

We remark that the ELV is a score associated not only to the whole components, but to each of their coefficients. This interesting property can be exploited to further remove, within a selected PC, the non-significant points, {\em i.e.} those with a low ELV. In practice this is done by setting these points to zero. That is a natural way to exploit the ELV score in order to operate a kind of {\em denoising} for the reduced data, making them only depend  on the significant time samples. In Sec.~\ref{sec:experiments} (scenario 4) we test the performances of an attack varying the number of time samples involved in the computation of the reduced data, and showing that such a denoising processing might impact significantly. 



%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Linear Discriminant Analysis}\label{sec:LDA}

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{../Figures/PCA_LDA_geometric/LDAprojection.pdf} 
\caption{LDA: some 2-dimensional labelled data (blue crosses and red circles) projected onto their 1-dimensional discriminant component (represented by the green line).}\label{fig:LDAprojection}
\end{figure}

\subsection{Fisher's Linear Discriminant and Terminology Remark}
Fisher's Linear Discriminant \cite{Fukunaga} is another statistical tool for dimensionality reduction, which is commonly used as a preliminary step before classification. Indeed it seeks for linear combinations of data that characterize or separate two or more classes, not only spreading class centroids as much as possible, like the class-oriented PCA does, but also minimizing the so-called {\em intra-class variance}, i.e. the variance shown by data belonging to the same class. The terms Fisher's Linear Discriminant and Linear Discriminant Analysis (LDA) are often used interchangeably, and in particular in SCA literature the Fisher's Linear Discriminant is almost always referred to as LDA, \eg \cite{lessIsMore,Standaert2008}. As we anticipated in Chapter~\ref{ChapterIntroML} - Example~\ref{example:LDA}, this widely-accepted abuse is due to the fact that under the assumptions leading to the LDA classification tools (\ie Gaussian class-conditional densities, sharing a common covariance matrix), the solution provided by the Fisher's Linear Discriminant (that does not require such assumptions) is the same as the solution provided by the LDA. From now on we will use the more common terminology LDA to refer to Fisher's Linear Discriminant. \\

\subsection{Description} Applying LDA consists in maximizing the so-called {\em Rayleigh quotient}:
 \begin{equation}\label{eq:LDA}
 \AAlpha_1=\mathrm{argmax}_{\AAlpha} \frac{\AAlpha^\intercal \SB \AAlpha}{\AAlpha^\intercal \SW \AAlpha} \mbox{ ,}
 \end{equation}
where $\SB$ is the {\em between-class scatter matrix} already defined in \eqref{eq:SB} and $\SW$ is called 
{\em within-class} (or intra-class) {\em scatter matrix}:

\begin{equation}
\SW = \sum_{\sensVarGenValue\in\sensVarSet}\sum_{i=1}^{\nbProfilingTraces}(\vLeakVec_i-\mmmXclass)(\vLeakVec_i-\mmmXclass)^\intercal \mbox{.}
\end{equation}


\begin{remark}
Let $\covmat$ be the the global covariance matrix of data, also called {\em total scatter matrix}, defined in \eqref{eq:covmat}; we have the following relationship between $\SW,\SB$ and $\covmat$:
\begin{equation}
\covmat = \frac{1}{\nbProfilingTraces}(\SW + \SB) \mbox{ .}
\end{equation}
\end{remark}

It can be shown that the vector $\AAlpha_1$ which maximizes \eqref{eq:LDA} must satisfy $\SB\AAlpha_1 = \lambda \SW \AAlpha_1$, for a constant $\lambda$, \textit{i.e.} has to be an eigenvector of $\SW^{-1} \SB$. Moreover, for any eigenvector $\AAlpha$ of $\SW^{-1}\SB$, with associated eigenvalue $\lambda$, the Rayleigh quotient equals such a $\lambda$:
\begin{equation}\label{eq:lambdas}
\frac{\AAlpha^\intercal \SB \AAlpha}{\AAlpha^\intercal \SW \AAlpha} = \lambda \mbox{ .}
\end{equation}
Then, among all eigenvectors of $\SW^{-1} \SB$, $\AAlpha_1$ must be the leading one. \\

The computation of the eigenvectors of $\SW^{-1} \SB$ is known under the name of {\em generalized eigenvector problem}. The difficulty here comes from the fact that $\SW^{-1} \SB$ is not guaranteed to be symmetric. Due to this non-symmetry,  $\AAlpha_1$ and the others eigenvectors do not form an orthonormal basis for $\mathbb{R}^\traceLength$, but they are anyway useful for classifications scopes. Let us refer to them as {\em Linear Discriminant Components} (LDCs for short); as for PCs we consider them sorted in decreasing order with respect to their associated eigenvalue, which gives a score for their informativeness, see \eqref{eq:lambdas}. Analogously to the PCA, the LDA provides a natural dimensionality reduction: one can project the data over the $\newTraceLength$ first LDCs. In Fig.~\ref{fig:LDAprojection} the 2-class toy data used as example above, projected over their leading discriminant component, are depicted. The two classes are kept well separated in the 1-dimensional subspace. As for PCA, this choice might not be optimal when applying this reduction to side-channel traces. For the sake of comparison, we test in Sec.~\ref{sec:experiments} all the selection methods proposed for the PCA (EGV, IPR and ELV) in association to the LDA as well.\\

In the following subsection we will present a well-known problem that affects the LDA in many practical contexts, and describe four methods that circumvent such a problem, with the intention to test them over side-channel data.


\subsection{The Small Sample Size Problem}\label{sec:SSS}
In the special case in which the matrix $\SB$ is invertible, the generalized eigenvalue problem is convertible in a regular one, as in \cite{Standaert2008}. On the contrary, when $\SB$ is singular, the simultaneous diagonalization is suggested to solve such a problem \cite{Fukunaga}. In this case one can take advantage by the singularity of $\SB$ to apply the computational trick described in Sec.~\ref{sec:trick}, since at most $r = \mathrm{rank}(\SB)$ eigenvectors can be found.\\

If the singularity of $\SB$ does not affect the LDA dimensionality reduction, we cannot say the same about the singularity of $\SW$:  SCA and Pattern Recognition literatures point out the same drawback of the LDA, known as the {\em Small Sample Size problem} (SSS for short). It occurs when the total number of acquisitions $\nbProfilingTraces$ is less than or equal to the size $\traceLength$ of them.
%\footnote{It can happen for example when attacking an RSA implementation, where the acquisitions are often huge (of the order of 1,000,000 points) and the number of measurements may be small when the SNR is good, implying that a good GE can be achieved with a small $N$.} 
The direct consequence of this problem is the singularity of $\SW$ and the non-applicability of the LDA. \\

If the LDA has been introduced relatively lately in the SCA literature, the Pattern Recognition community looks for a solution to the SSS problem at least since the early nineties. We browsed some of the proposed solutions and chose some of them to introduce, in order to test them over side channel traces.

\subsubsection{Fisherface Method}
The most popular among the solutions to SSS is the so-called {\em Fisherface} method\footnote{The name is due to the fact that it was proposed and tested for face recognition scopes.} \cite{eigenfaces}. It simply relies on the combination between PCA and LDA: a standard PCA dimensionality reduction is performed to data, making them pass from dimension $\traceLength$ to dimension $\nbProfilingTraces-\nbClasses$, which is the general maximal rank for $\SW$. In this reduced space, $\SW$ is very likely to be invertible and the LDA therefore applies.

\subsubsection{$\SW$ Null Space Method}
It has been introduced by Chen et al. in \cite{Chen2000} and exploits an important result of Liu et al. \cite{liu1992generalized} who showed that Fisher's criterion \eqref{eq:LDA} is equivalent to:
 \begin{equation}
 \AAlpha_1=\mathrm{argmax}_{\AAlpha} \frac{\AAlpha^\intercal \SB \AAlpha}{\AAlpha^\intercal \SW \AAlpha + \AAlpha^\intercal \SB \AAlpha} \mbox{ .}
 \end{equation}
The authors of \cite{Chen2000} point out that such a formula is upper-bounded by 1, and that it achieves its maximal value, \textit{i.e.} 1, if and only if  $\AAlpha$ is in the null space of $\SW$. Thus they propose to first project data onto the null space of $\SW$ and then to perform a PCA, \textit{i.e.} to select as LDCs the first $\nbClasses - 1$ eigenvectors of the between-class scatter matrix of data into this new space. More precisely, let $Q = [\vvv_1, \dots, \vvv_{\traceLength - \mathrm{rank}(\SW)}]$ be the matrix of vectors that span the null space of $\SW$. \cite{Chen2000} proposes to transform the data $\vLeakVec$ into $\vLeakVec^\prime = QQ^\intercal\vLeakVec$. Such a transformation maintains the original dimension $\traceLength$ of the data, but let the new within-class matrix $\SW' = QQ^\intercal\SW QQ^\intercal$ be the null $\traceLength \times \traceLength$ matrix. Afterwards, the method looks for the eigenvectors of the new between-class matrix $\SB' = QQ^\intercal\SB QQ^\intercal$. Let $U$ be the matrix containing its first $\nbClasses - 1$ eigenvectors: the LDCs obtained via the $\SW$ null space method are the columns of $QQ^\intercal U$.

\subsubsection{Direct LDA}
As the previous, this method, introduced in \cite{Yu01adirect}, privileges the low-ranked eigenvectors of $\SW$, but proposes to firstly project the data onto the rank space of $\SB$, arguing the fact that vectors of the null space of $\SB$ do not provide any between-class separation of data. Let $D_B=V^\intercal\SB V$ be the diagonalization of $\SB$, and let $V^\star$ be the matrix of the eigenvectors of $\SB$ that are not in its null space, \textit{i.e.} whose eigenvalues are different from zero. Let also $D_B^\star$ denotes the matrix ${V^\star}^\intercal\SB V^\star$; transforming the data $\vLeakVec$ into ${D_B^\star}^{1/2}{V^\star}^\intercal\vLeakVec$ makes the between-class variance to be equal to   the $(\nbClasses - 1) \times (\nbClasses- 1)$ identity matrix. After this transformation the within-class variance assumes the form $\SW' = {D_B^\star}^{1/2}{V^\star}^\intercal\SW ' V^\star {D_B^\star}^{1/2}$. After storing the $\newTraceLength$ lowest-rank eigenvectors in a matrix $U^\star$, the LDCs obtained via the Direct LDA method are the columns of $V^\star{D_B^\star}^{1/2}{U^\star}^\intercal$. 


\subsubsection{$\ST$ Spanned Space Method}
The last variant of LDA that we consider has been proposed in \cite{huang} and is actually a variant of the Direct LDA: instead of removing the null space of $\SB$ as first step, this method removes the null space of $\ST = \SB + \SW$. Then, denoting $\SW'$ the within-class matrix in the reduced space, the reduced data are projected onto its null space, i.e. are multiplied by the matrix storing by columns the eigenvectors of $\SW'$ associated to the null eigenvector, thus reduced again. A final optional step consists in verifying whether  the between-class matrix presents a non-trivial null-space after the last projection and, in this case, in effectuating a further projection removing it.

\begin{remark}
Let us remark that, from a computational complexity point of view (see \cite{huang} for a deeper discussion), the least time-consuming procedure among the four proposed is the Direct LDA, followed by the Fisherface and the $\ST$ Spanned Space Method, that have a similar complexity. The $\SW$ Null Space Method is in general much more expensive, because the task of removing the $\SW$ null space requires the actual computation of the $(\traceLength\times \traceLength)$-dimensional matrix $\SW$, {\em i.e.} the computational trick  described in Sec.~\ref{sec:trick} is not applicable. On the contrary, the other three methods take advantage of such a procedure, reducing drastically their complexity.
\end{remark}



%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------


\section{Experimental Results}\label{sec:experiments}

In this section we compare the different extractors (\ie functions applying a data dimensionality reduction, see Sec.~\ref{sec:extractors}) provided by the PCA and the LDA in association with the different techniques  of components selection. Defining an universal criterion to compare the different extractors would not make sense since the latter one should encompass a lot of parameters, sometimes opposite, that vary according to the context (amount of noise, specificity of the information leakage, nature of the side channel, etc.). For this reason we choose to split our comparisons into four scenarios. Each scenario has a single varying parameter that, depending on the attacker context, may wish to be minimized. Hereafter the definition of the four scenarios: 
\begin{itemize}
\item[][Scenario 1] varying parameter: number of attack traces $\nbAttackTraces$, 
\item[][Scenario 2] varying parameter: number of profiling traces $\nbProfilingTraces$, 
\item[][Scenario 3] varying parameter: number of projecting components selected $\newTraceLength$,
\item[][Scenario 4] varying parameter: number of original time samples implied into the trace preprocessing computation $\numPoI$ .
\end{itemize}
 
For scenarios in which $\nbProfilingTraces$ is fixed, the value of $\nbProfilingTraces$ is chosen high enough to avoid the SSS problem, and the extensions of LDA presented in Sec.~\ref{sec:SSS} are not evaluated.
 This choice of $\nbProfilingTraces$ will imply that the LDA is always performed in a favourable situation, which makes expect the LDA to be particularly efficient for these experiments. Consequentely, for the scenarios in which $\nbProfilingTraces$ is high, our goal is to study whether the PCA can be made almost as efficient as the LDA thanks to the component selection methods discussed in Sec.~\ref{sec:ELV}. 



\subsection{The testing adversary.}  
Our testing adversary attacks an 8-bit AVR microprocessor Atmega328P and acquires power-consumption traces via the ChipWhisperer platform \cite{o2014chipwhisperer}.\footnote{This choice has been done to allow for reproducibility of the experiments.} The target device stores a secret 128-bit key and performs the first steps of an AES: the loading of 16 bytes of the plaintext, the AddRoundKey step and the AES Sbox. It has been programmed twice: two different keys are stored in the device memory during the acquisition of the profiling and of the attack traces, to simulate the situation of two identical devices storing a different secret. The size $\traceLength$ of the traces equals $3996$. The sensitive target  variable is the output of the first Sbox processing, but, since the key is fixed also during the profiling phase, and both Xor and Sbox operations are bijective, we expect to detect three interesting regions (as those high-lighted by PCs 4, 5 and 6, in Fig.~\ref{fig:6components}): the reading of the first byte of the plaintext, the first AddRoundKey and the first Sbox. We consider an {\em identity classification} leaking function (i.e. we make minimal assumption on the leakage function, see Sec.~\ref{sec:sensVar}), which implies that the 256 possible values of the Sbox output yields to 256 classes. For each class we assume that the adversary acquires the same number $\nbTraces$ of traces, \textit{i.e.} $\nbProfilingTraces =\nbTraces\times 256$. After the application of the extractor $\extract$, the trace size is reduced to $\newTraceLength$. Then the attacker performs a Template Attack (see Sec.~\ref{sec:TA}), using $\newTraceLength$-variate Gaussian templates.


\begin{figure}[t]
\subfigure[]{\label{fig:1.1}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015//Criterion1.pdf}}
\subfigure[]{\label{fig:1.2}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015//Criterion1Good.pdf}}
\caption[Guessing Entropy as function of the number of attack traces]{Guessing Entropy as function of the number of attack traces for different extraction methods. All Guessing Entropies are estimated as the average rank of the right key over 100 independent experiments.}\label{fig:scenario1}
\end{figure}
\subsection{Scenario 1.}
To analyse the dependence between the extraction methods presented in Sections~\ref{sec:PCA} and \ref{sec:LDA} and the number of attack traces $\nbAttackTraces$ needed to achieve a given GE (Guessing Entropy, see Sec.~\ref{sec:metrics}), we fixed the other parameters as follows: $\nbTraces=50$ ($\nbProfilingTraces=50\times 256$), $\newTraceLength = 3$ and $\numPoI = 3996$ (all points are allowed to participate in the building of the PCs and of the LDCs). The experimental results, depicted in Fig.~\ref{fig:scenario1}\subref{fig:1.1}-\subref{fig:1.2}, show that the PCA standard method has very bad performances in SCA, while the LDA outperforms the others. Concerning the class-oriented PCA, we observe that its performance is close to that of LDA when combined with the selection methods ELV (which performs best) or IPR, while it is similar to the one of classic PCA when associated with the EGV selection.



\subsection{Scenario 2.}

Now we test the behaviour of the extraction methods as function of the number $N_z$ of available profiling traces per class. The number of components $\newTraceLength$ is still fixed to 3, $\numPoI=3996$ again and the number of attack traces is $\nbAttackTraces=100$. This scenario has to be divided into two parts: if $N_z\leq 15$, then $\nbProfilingTraces<\traceLength$ and the SSS problem occurs. Thus, in this case we test the four extensions of LDA presented in Sec.~\ref{sec:SSS}, associated to either the standard selection, to which we abusively refer as EGV,\footnote{It consists in keeping the $\newTraceLength$ first LDCs (the $C$ last for the Direct LDA)}
or to the IPR selection.  We compare them to the class-oriented PCA associated to EGV, IPR or ELV. The ELV selection is not performed for the techniques extending LDA, since for some of them the projecting LDCs are not associated to some eigenvalues in a meaningful way. On the contrary, if $\nbTraces \geq 16$ there is no need to approximate the LDA technique, so the classical one is performed. Results for this scenario are shown in Fig.~\ref{fig:scenario2}. It may be noticed that the combinations class-oriented PCA + ELV/IPR select exactly the same components, for our data, see Fig.~\ref{fig:pcaclass} and do not suffer from the lack of profiling traces. They are slightly outperformed by the $\SW$ Null Space method associated with the EGV, see Fig.\ref{fig:swnullspace}. The Direct LDA (Fig.~\ref{fig:direct}) method also provides a good alternative, while the other tested methods do not show a stable behaviour. The results in absence of the SSS problem (Fig.\ref{fig:notSSS}) confirm that the standard PCA is not adapted to SCA, even when provided with more profiling traces. It also shows that among class-oriented PCA and LDA, the class-oriented PCA converges faster.



\begin{figure}
\subfigure[]{\label{fig:fisherface}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_Fisherface.pdf}}
\subfigure[]{\label{fig:direct}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_DirectLDA.pdf}}
\subfigure[]{\label{fig:stspannedspace}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_STSpannedSpace.pdf}}
\subfigure[]{\label{fig:swnullspace}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_SWnullSpace.pdf}}
\subfigure[]{\label{fig:pcaclass}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_PCAclass.pdf}}
\subfigure[]{\label{fig:notSSS}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2notSSS.pdf}}
\caption[Guessing Entropy as function of the number of profiling traces.]{Guessing Entropy as function of the number of profiling traces. Figures \subref{fig:fisherface}-\subref{fig:swnullspace}: methods extending the LDA in presence of SSS problem; Figure \subref{fig:pcaclass}: class-oriented PCA in presence of the SSS problem; Figure \subref{fig:notSSS}: number of profiling traces high enough to avoid the SSS problem.}\label{fig:scenario2}
\end{figure}


\subsection{Scenario 3.}
Let  $\newTraceLength$ be now variable and let the other parameters be fixed as follows: $\nbAttackTraces = 100, N_z=200, \numPoI = 3996$. Looking at Fig.~\ref{fig:3}, we might observe that the standard PCA might actually well perform in SCA context if provided with a larger number of kept components; on the contrary, a little number of components suffices to the LDA. Finally, keeping more of the necessary components does not worsen the efficiency of the attack, which allows the attacker to choose $\newTraceLength$ as the maximum value supported by his computational means. 

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{../Figures/CARDIS2015/Criterion3.pdf}
\caption{Guessing Entropy as function of the trace size after reduction.}\label{fig:3}
\end{figure}

\begin{remark}
In our experiments the ELV selection method only slightly outperforms the IPR. Nevertheless, since it relies on more sound and more general observations, {\em i.e.} the maximization of explained variance concentrated into few points, it is likely to be more robust and less case-specific. For example, in Fig.~\ref{fig:notSSS} it can be remarked that while the class-oriented PCA + ELV line keeps constant on the value 1 of guessing entropy, the class-oriented PCA + IPR is sometimes higher than 1.
\end{remark}

\subsection{Scenario 4.}

\begin{figure}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion4.pdf}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion4cutted.pdf} 
\caption{Guessing Entropy as function of the number of time samples contributing to the extractor computation.}\label{fig:4}
\end{figure}

This is the single scenario in which we allow the ELV selection method to not only select the components to keep but also to modify them, keeping only some coefficients within each component, setting the other ones to zero. We select pairs \textit{(component, time sample)} in decreasing order of the ELV values, allowing the presence of only $\newTraceLength = 3$ components and $\numPoI$ different times samples, \ie each component must have only 3 entries different from 0.
Looking at Fig.~\ref{fig:4} we might observe that the LDA allows to achieve the maximal guessing entropy with only 1 PoI in each of the 3 selected components. 
Actually, adding PoIs worsen its performances, which is coherent with the assumption that the vulnerable information leaks in only a few points. Such points are excellently detected by the LDA. Adding contribution from other points raises the noise, which is then compensated by the contributions of further noisy points, in a very delicate balance. Such a behaviour is clearly visible in standard PCA case: the first 10 points considered raise the level of noise, that is then balanced by the last 1000 points.


\subsection{Overview of this Study and Conclusions}


This study focused on two well-known techniques to construct extractors for side-channel traces, the PCA and the LDA. The LDA method is more adequate than the PCA one, thanks to its class-distinguishing asset, but more expensive and not always available in concrete situations. We deduced from a general consideration about side-channel traces, \ie the fact that for secured implementations the vulnerable leakages are concentrated into few points, a new methodology  for selecting components, called ELV. We showed that the class-oriented PCA, equipped with the ELV, achieves performances close to those of the LDA, becoming a cheaper and valid alternative to the LDA. Being our core consideration very general in side-channel context, we believe that our results are not case-specific.  \\
A second part of the proposed study analysed experimentally some alternatives to the LDA in presence of SSS problem proposed in Pattern Recognition literature. Such experiments showed that the Direct LDA and the $\SW$ Null Space Method are promising techniques, exhibiting performances close to the ones given by the ELV-equipped class-oriented PCA. A synthetic overview of the performed comparisons in scenarios 1,2 and 3 is depicted in Table~\ref{table:results}.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&&\multicolumn{4}{|>{\columncolor[gray]{0.7}}c|}{Parameter to minimize}\\
\hline
\multicolumn{1}{|>{\columncolor[gray]{0.7}}c|}{Method}&\multicolumn{1}{|>{\columncolor[gray]{0.7}}c|}{Selection}& $\nbAttackTraces$ &  $\nbProfilingTraces$ (SSS) &  $\nbProfilingTraces'$ ($\neg$SSS) &  $\newTraceLength$ \\
\hline
PCA standard & EGV & {\bf -} &  &{\bf -} &{\bf -} \\
\hline
PCA standard &\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{ELV} & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} \\
\hline
PCA standard & IPR &{\bf -} & &{\bf -} &{\bf +} \\
\hline
PCA class & EGV & {\bf -} &{\bf -} &{\bf -} &{\bf -} \\
\hline
PCA class & \multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{ELV} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$}&\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} \\
\hline 
PCA class & IPR & {\bf {\bf +}} &$\bigstar$ &{\bf +} &{\bf -} \\
\hline 
LDA & EGV &$\bigstar$ & & {\bf +} & $\bigstar$\\
\hline 
LDA & \multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{ELV} & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} &  & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$}\\
\hline 
LDA & IPR & {\bf +} & &{\bf +} & $\bigstar$ \\

\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{$\SW$ Null Space}  & EGV & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$ } & & \\
\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{$\SW$ Null Space}  & IPR & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} & & \\
\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{Direct LDA} & EGV & & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$}& & \\
\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{Direct LDA} & IPR & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}}& & \\
\hline
\multicolumn{2}{|>{\columncolor[gray]{0.8}}c|}{Fisherface} & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} & & \\
\hline 
\multicolumn{2}{|>{\columncolor[gray]{0.8}}c|}{$\ST$ Spanned Space}  & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} & & \\
\hline
\end{tabular}
\caption[Linear Methods. Overview of the extractors' performances.]{Overview of the extractors' performances in tested situations. Depending on the adversary purpose, given by the parameter to minimize, a $\bigstar$ denotes the best method, a ${\bf +}$ denotes a method with performances close to those of the best one and a ${\bf -}$ is for methods that show lower performances. Techniques introduced in \cite{Cagli2016} are highlighted by a grey background.}\label{table:results}
\end{table}

%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------
\section{Misaligning Effects}\label{sec:misalignment}
The fact that trace misalignment leads to a drastic drop of the \emph{dimensionality reduction/ template attack} routine is well-known. When we are in presence of a misalignment, caused by the implementation of a countermeasure or  by the lack of a good trigger signal for the acquisition, the application of some previous re-synchronization techniques is recommended (see for instance \cite{choudary2014template}, where the same PCA and LDA techniques are exploited as template attack preprocessing, after a prior resnchronization). In this section we experimentally show how the approach based on linear dimensionality reduction described in this chapter is affected by traces misalignment. To this aim, we simply take the same data and parameters exploited for Scenario 1 in Sec.~\ref{sec:experiments}, and artificially misalign them through the technique proposed in Appendix~\ref{appendix:artificial_jitter} with parameters \texttt{sigma}$= 6$, \texttt{B}$= 4$. Then we tried to attack them through the 9 methodologies tested in Scenario 1. It may be noticed in Fig.~\ref{fig:PCA_LDA_misalignment} that none of the 9 techniques is still efficient, included the optimal LDA+EGV that lead to minimal guessing entropy with the synchronized traces using less than 7 attack traces. In this case it cannot lead to successful attack in less than 3000 traces.
\begin{figure}
\includegraphics[width=\textwidth]{../Figures/desynchro_results_PCA_LDA.pdf} 
\caption{Degradation of linear-reduction-based template attacks in presence of misalignment.}\label{fig:PCA_LDA_misalignment}
\end{figure}