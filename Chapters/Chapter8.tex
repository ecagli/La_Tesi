% Chapter Template

\chapter{Convolutional Neural Networks against Jitter-Based Countermeasures} % Main chapter title

\label{ChapterCNN}

In this chapter we explore a new strategy to perform profiling SCAs, endorsing the Deep Learning (DL) paradigm. To this aim we revisit the publication \cite{cagli2017defeating} appeared at CHES 2017.\\
\\

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Moving from Kernel Machines to Neural Networks}

Often Machine Learning approach declines in multiple pre-processing phases such as data alignment, feature selections or dimensionality reduction, followed by a final model optimisation. This is the case for the SCAs routines that we considered in previous chapters: a dimensionality reduction pre-processing $\extract$ is previously learned, followed by a Gaussian template parameters estimation.  Deep Learning is a
branch of Machine Learning whose characteristic is to avoid any preliminary pre-processing step from the model construction work-flow. For example, in Deep
Learning the data dimensionality reduction is not necessarily explicitly performed by a distinct learned  function $\extract$. Thus, DL models are in charge
of directly and implicitly extracting interesting features and of estimating the
opportune model to solve the task. The model is searched in a family of models that are composed by a cascade of parametrisable layers that can be optimised in a single global learning process. Such models are called \emph{Artificial Neural Network}, or simply \emph{Neural Networks} (NNs). As anticipated in Chapters~\ref{ChapterIntroductionSCA} and \ref{ChapterIntroML}, we are interested in the NNs' solutions for the classification task, as we remarked a strong analogy between profiling SCAs and classical ML classification task. Often for this task NNs are exploited to create discriminative models, \ie models that approximate the conditional probability of a label given the observed trace. This is in opposition to the Template Attack we exploited in previous paragraph, that in based over the construction of generative models, \ie the approximation of the \emph{templates} which coincide with the conditional probabilities of the traces given a label, as described in Sec.~\ref{sec:TA}.\\

By construction, NNs are the ML answer to the drawback of work-flows we analysed in previous chapters and discussed as \emph{two-phased approach drawback} in last section of Chapter~\ref{ChapterKernel}. Actually, NNs are answers to other drawbacks pointed out in the same section: in particular they are not memory-based. It implies that, after the training phase whose computational complexity is influenced by the size of the training set, they do not need to access the training set any more. By consequence, the obtained model is in general faster in processing new data, with respect to those obtained \via kernel machines: the form of the obtained model (\ie the so-called \emph{architecture}) is independent from the size of the training set. This property, together with other computational optimizations they allow that will be discussed later, make NNs more easily scalable for huge training sets. Finally, we pointed out as drawback of techniques analysed in previous chapters their weakness to trace misalignment. In DL literature, a family of models called \emph{Convolutional Neural Networks} (CNNs) has been developed to treat difficulties usually met in image processing as misalignments, scaling, rotations, \dots We claim in this chapter, and verify through various experiments, that such CNNs provide an attack strategy that is robust to misalignment countermeasure. Attacks proposed in this chapter are performed against non-masked implementation. Nevertheless, since NNs are in general non-linear models, they naturally well-fit also the higher-order attack context, as discussed in \cite{maghrebi2016breaking} and \cite{DLwhitepaper}. The CNNs are a generalisation of the simplest NN architecture, named \emph{Multi-Layer Perceptron}, that we describe hereafter. 
% The answers to the 3 drawbacks of previous chapter (misalignment, memory complexity and actual number of params, two phases approach)
%"In practice, however, it is often worth investing substantial computational resources during the training phase in order to obtain a compact model that is fast at processing new data" (Bishop intro chap 5)

% RICICLATO DALL'EX CAPITOLO 1
%Kernel techniques like the KDA  are as well inherited from Machine Learning domain and consist in strategies that allow to build interesting extensions of many algorithms. One of their characteristics, that turns to be a drawback to apply them in SCA context is that are memory-based: the entire set of profiling traces, \ie those acquired by observing the open samples, has to be stored and accessed in the attack phase. In this sense they are highly memory-consuming, and quite slow to apply: they do not scale well in presence of huge profiling trace sets as those that are often necessary to perform profiling SCAs. In contrast to them, models provided by Neural Networks (NNs) are Machine Learning solutions that are known to be easily scalable to huge datasets and not memory-based. We decided to explore such an approach and pointed out that it not only provided solutions to tackle such a computational performance drawback.
\subsection{Multi-Layer Perceptrons}
% descrizione (dal paper) 
% universal approximation theorem (da pagg 194 e seguenti del deeplearningbook)

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Misalignment of Side-Channel Traces}

\subsection{The Necessity and the Risks of Applying Realignment Techniques}
\subsection{Analogy with Image Recognition Issues}

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Convolutional Layers to Impose Shift-Invariance}

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Data Augmentation for Misaligned Side-Channel Traces}
%\todo{cita o qui o all'inizio il paper di CARDIS 2017 sulla trace augmentation}
%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Experiments against Software Countermeasures}


%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

\section{Experiments against Artificial Hardware Countermeasures}\label{sec:hardware}%label citato in appendice artifical jitter

%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------

\section{Experiments against Real-Case Hardware Countermeasures}