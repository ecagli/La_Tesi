% Chapter Template

\chapter{Convolutional Neural Networks against Jitter-Based Countermeasures} % Main chapter title

\label{ChapterCNN}

In this chapter we explore a new strategy to perform profiling SCAs, endorsing the Deep Learning (DL) paradigm. To this aim we revisit the publication \cite{cagli2017defeating} appeared at CHES 2017.\\

Often Machine Learning approach declines in multiple pre-processing phases such as data alignment, feature selections or dimensionality reduction, followed by a final model optimisation. This is the case for the SCAs routines that we considered in previous chapters: a dimensionality reduction pre-processing $\extract$ is previously learned an applied, followed by a Gaussian template parameters estimation that are used to construct the model which solves the side-channel classification task.  Deep Learning is a branch of Machine Learning whose characteristic is to avoid any preliminary pre-processing step from the model construction work-flow. For example, in Deep Learning the data dimensionality reduction is not necessarily explicitly performed by a distinct learned  function $\extract$. Thus, DL models are in charge of directly and implicitly extracting interesting features and of estimating the opportune model to solve the task. The model is searched in a family of models that are composed by a cascade of parametrisable layers, which may be optimised in a single global learning process. Such models are called \emph{Artificial Neural Network}, or simply \emph{Neural Networks} (NNs). \\

By construction, NNs are the ML answer to the drawback of work-flows we analysed in previous chapters and discussed as \emph{two-phased approach drawback} in last section of Chapter~\ref{ChapterKernel}. Actually, NNs are answers to other drawbacks pointed out in the same section. \\
In particular they are not memory-based. It implies that, after the training phase whose computational complexity is influenced by the size of the training set, they do not need to access the training set any more. By consequence, the obtained model is in general faster in processing new data, with respect to those obtained \via kernel machines: the form of the obtained model (\ie the so-called \emph{architecture}) is independent from the size of the training set. This property, together with other computational optimizations they allow that will be discussed later, make NNs more easily scalable for huge training sets.\\
Finally, we pointed out as drawback of techniques analysed in previous chapters their weakness to trace misalignment. In DL literature, a family of models called \emph{Convolutional Neural Networks} (CNNs) has been developed to treat difficulties usually met in image processing as misalignments, scaling, rotations, \dots We claim in this chapter, and verify through various experiments, that such CNNs provide an attack strategy that is robust to misalignment countermeasure. Attacks proposed in this chapter are performed against non-masked implementation. Nevertheless, since NNs are in general non-linear models, they naturally well-fit also the higher-order attack context, as discussed in \cite{maghrebi2016breaking} and \cite{DLwhitepaper}.\\
The CNNs are a generalisation of the simplest NN architecture, named \emph{Multi-Layer Perceptron}, which is described hereafter. 

% The answers to the 3 drawbacks of previous chapter (misalignment, memory complexity and actual number of params, two phases approach)
%"In practice, however, it is often worth investing substantial computational resources during the training phase in order to obtain a compact model that is fast at processing new data" (Bishop intro chap 5)

% RICICLATO DALL'EX CAPITOLO 1
%Kernel techniques like the KDA  are as well inherited from Machine Learning domain and consist in strategies that allow to build interesting extensions of many algorithms. One of their characteristics, that turns to be a drawback to apply them in SCA context is that are memory-based: the entire set of profiling traces, \ie those acquired by observing the open samples, has to be stored and accessed in the attack phase. In this sense they are highly memory-consuming, and quite slow to apply: they do not scale well in presence of huge profiling trace sets as those that are often necessary to perform profiling SCAs. In contrast to them, models provided by Neural Networks (NNs) are Machine Learning solutions that are known to be easily scalable to huge datasets and not memory-based. We decided to explore such an approach and pointed out that it not only provided solutions to tackle such a computational performance drawback.


\subsection{Neural Networks and Multi-Layer Perceptrons}
As anticipated in Chapters~\ref{ChapterIntroductionSCA} and \ref{ChapterIntroML}, we are interested in the NNs' solutions for the classification task, as we remarked a strong analogy between profiling SCAs and classical ML classification task. We recall from Chapters~\ref{ChapterIntroML} that for the classification task, the learning algorithm is asked to construct a function $\MLmodel\colon \mathbb{R}^\traceLength \rightarrow \{0,1\}^{\numClasses}$, when elements of $\sensVarSet$, \ie the set of classes, are expressed \via the \emph{one-hot encoding}. The output of such a function is said to be \emph{categorical}, \ie $\sensVarSet$ is a discrete finite set. A variant of the classification task consists in finding a function $\MLmodel\colon \mathbb{R}^\traceLength \rightarrow [0,1]^{\numClasses}$ defining a probability distribution over classes. Often for this task NNs are exploited to create discriminative models, \ie models that directly approximate the latter function $\MLmodel$ that describes the posterior conditional probability of a label given the observed trace. The proper classification is then done by choosing the label that maximize such a posterior probability. This is in opposition to the Template Attack we exploited in previous chapters, that is based over the construction of generative models, \ie the approximation of the \emph{templates}, which coincide with the conditional probabilities of the traces given a label, as described in Sec.~\ref{sec:TA}.\\

 aim at constructing a function $\MLmodel\colon \mathbb{R}^D\rightarrow \mathbb{R}^{|\sensVarSet|}$ that takes data $\xxx\in \mathbb{R}^D$ and outputs vectors $\yyy\in \mathbb{R}$ of  scores. The classification of $\xxx$ is done afterwards by choosing the label $z^j$ such that $j = \mathrm{argmax}\ \yyy[j]$. In general $\MLmodel$ is obtained by combining several simpler functions, called \emph{layers}. An NN has an \emph{input layer} (the identity over the input datum $\xxx$), an output layer (the last function, whose output is the scores vector $\yyy$) and all other layers are called \emph{hidden} layers. The nature (the number and the dimension) of the layers is called the \emph{architecture} of the NN. All the parameters that define an architecture, together with some other parameters that govern the training phase, have to be carefully set by the attacker, and are called \emph{hyper-parameters}. The so-called \emph{neurons}, that give the name to the NNs, are the computational units of the network and essentially process a scalar product between the coordinates of its input and a vector of  \emph{trainable weights} (or simply \emph{weights}) that have to be \emph{trained}. Each layer processes some neurons and the outputs of the neuron evaluations will form new input vectors for the subsequent layer. The training phase consists in an automatic tuning of the weights and it is done \emph{via} an iterative approach which locally applies the Stochastic Gradient Descent algorithm \cite{Goodfellow-et-al-2016} to minimize a  \emph{loss function} quantifying the classification error of the function $F$ over the training set. We will not give further details about this classical optimization approach, and the interested reader may refer to \cite{Goodfellow-et-al-2016}.\\


% descrizione (dal paper) 
% universal approximation theorem (da pagg 194 e seguenti del deeplearningbook)

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Misalignment of Side-Channel Traces}

\subsection{The Necessity and the Risks of Applying Realignment Techniques}
\subsection{Analogy with Image Recognition Issues}

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Convolutional Layers to Impose Shift-Invariance}

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Data Augmentation for Misaligned Side-Channel Traces}
%\todo{cita o qui o all'inizio il paper di CARDIS 2017 sulla trace augmentation}
%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Experiments against Software Countermeasures}


%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

\section{Experiments against Artificial Hardware Countermeasures}\label{sec:hardware}%label citato in appendice artifical jitter

%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------

\section{Experiments against Real-Case Hardware Countermeasures}