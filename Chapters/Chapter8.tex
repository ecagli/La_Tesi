% Chapter Template

\chapter{Convolutional Neural Networks against Jitter-Based Countermeasures} % Main chapter title

\label{ChapterCNN}

In this chapter we explore a new strategy to perform profiling SCAs, endorsing the Deep Learning (DL) paradigm. To this aim we revisit the publication \cite{cagli2017defeating} appeared at CHES 2017.\\

Often Machine Learning approach declines in multiple pre-processing phases such as data alignment, feature selections or dimensionality reduction, followed by a final model optimisation. This is the case for the SCAs routines that we considered in previous chapters: a dimensionality reduction pre-processing $\extract$ is previously learned an applied, followed by a Gaussian template parameters estimation that are used to construct the model which solves the side-channel classification task.  Deep Learning is a branch of Machine Learning whose characteristic is to avoid any preliminary pre-processing step from the model construction work-flow. For example, in Deep Learning the data dimensionality reduction is not necessarily explicitly performed by a distinct learned  function $\extract$. Thus, DL models are in charge of directly and implicitly extracting interesting features and of estimating the opportune model to solve the task. The model is searched in a family of models that are composed by a cascade of parametrisable layers, which may be optimised in a single global learning process. Such models are called \emph{Artificial Neural Network}, or simply \emph{Neural Networks} (NNs). \\

By construction, NNs are the ML answer to the drawback of work-flows we analysed in previous chapters and discussed as \emph{two-phased approach drawback} in last section of Chapter~\ref{ChapterKernel}. Actually, NNs are answers to other drawbacks pointed out in the same section. \\
In particular they are not memory-based. It implies that, after the training phase whose computational complexity is influenced by the size of the training set, they do not need to access the training set any more. By consequence, the obtained model is in general faster in processing new data, with respect to those obtained \via kernel machines: the form of the obtained model (\ie the so-called \emph{architecture}) is independent from the size of the training set. This property, together with other computational optimizations they allow that will be discussed later, make NNs more easily scalable for huge training sets.\\
Finally, we pointed out as drawback of techniques analysed in previous chapters their weakness to trace misalignment. In DL literature, a family of models called \emph{Convolutional Neural Networks} (CNNs) has been developed to treat difficulties usually met in image processing as misalignments, scaling, rotations, \dots We claim in this chapter, and verify through various experiments, that such CNNs provide an attack strategy that is robust to misalignment countermeasure. Attacks proposed in this chapter are performed against non-masked implementation. Nevertheless, since NNs are in general non-linear models, they naturally well-fit also the higher-order attack context, as discussed in \cite{maghrebi2016breaking} and \cite{DLwhitepaper}.\\
The CNNs are a generalisation of the simplest NN architecture, named \emph{Multi-Layer Perceptron}, which is described hereafter. 

% The answers to the 3 drawbacks of previous chapter (misalignment, memory complexity and actual number of params, two phases approach)
%"In practice, however, it is often worth investing substantial computational resources during the training phase in order to obtain a compact model that is fast at processing new data" (Bishop intro chap 5)

% RICICLATO DALL'EX CAPITOLO 1
%Kernel techniques like the KDA  are as well inherited from Machine Learning domain and consist in strategies that allow to build interesting extensions of many algorithms. One of their characteristics, that turns to be a drawback to apply them in SCA context is that are memory-based: the entire set of profiling traces, \ie those acquired by observing the open samples, has to be stored and accessed in the attack phase. In this sense they are highly memory-consuming, and quite slow to apply: they do not scale well in presence of huge profiling trace sets as those that are often necessary to perform profiling SCAs. In contrast to them, models provided by Neural Networks (NNs) are Machine Learning solutions that are known to be easily scalable to huge datasets and not memory-based. We decided to explore such an approach and pointed out that it not only provided solutions to tackle such a computational performance drawback.


\subsection{Neural Networks and Multi-Layer Perceptrons}
As anticipated in Chapters~\ref{ChapterIntroductionSCA} and \ref{ChapterIntroML}, we are interested in the NNs' solutions for the classification task, as we remarked a strong analogy between profiling SCAs and classical ML classification task. We recall from Chapters~\ref{ChapterIntroML} that for the classification task, the learning algorithm is asked to construct a function $\MLmodel\colon \mathbb{R}^\traceLength \rightarrow \{0,1\}^{\numClasses}$, where elements of $\sensVarSet$, \ie the set of classes, are here expressed \via the \emph{one-hot encoding}. The output of such a function is said to be \emph{categorical}, \ie $\sensVarSet$ is a discrete finite set. A variant of the classification task consists in finding a function $\MLmodel\colon \mathbb{R}^\traceLength \rightarrow [0,1]^{\numClasses}$ defining a probability distribution over classes. Often for this task, NNs are exploited to create discriminative models, \ie models that directly approximate the latter function $\MLmodel$ that describes the posterior conditional probability of a label given the observed trace. This is in opposition to the Template Attack we exploited in previous chapters, that is based over the construction of generative models, \ie the approximation of the \emph{templates}, which coincide with the conditional probabilities of the traces given a label, as described in Sec.~\ref{sec:TA}. \\

Using NNs the function $\MLmodel$ is obtained by combining several simpler functions, called \emph{layers}. An NN has an \emph{input layer} (the identity over the input datum $\vLeakVec$), an output layer (the last function) and all other layers are called \emph{hidden} layers.  The output of $MLmodel$ is a $\numClasses$-sized vector $\vNNOutput$ of scores for the $\numClasses$ labels. Such a vector might or not represent the approximation of a probability distribution. The nature of the NN's  layers, their number and their dimension in particular, is called the \emph{architecture} of the NN. All the parameters that define an architecture, together with some other parameters that govern the training phase, are its \emph{hyper-parameters}. The so-called \emph{neurons}, that give the name to the NNs, are the computational units of the network and essentially process a scalar product between the coordinates of its input and a vector of  \emph{trainable weights} (or simply \emph{weights}) that have to be \emph{trained}. Each layer processes some neurons and the outputs of the neuron evaluations will form new input vectors for the subsequent layer. The training phase consists in an automatic tuning of the weights and it is done \emph{via} an iterative approach which locally applies the Stochastic Gradient Descent algorithm \cite{Goodfellow-et-al-2016} to minimize a  \emph{loss function} quantifying the classification error of the function $\MLmodel$ over the training set. We will not give further details about this classical optimization approach, and the interested reader may refer to \cite{Goodfellow-et-al-2016}.\\

The {\em Multi-Layer Perceptrons} (MLPs), also called \emph{Feed-forward Neural Networks} are a family of NN's architectures, associated with a function $\MLmodel$  that is composed of multiple linear functions and some non-linear functions, called {\em activations}. The name \emph{feedforward} is due to the fact that for this kind of models information ﬂows from  the input to the output, through the intermediate computations used to deﬁne $\MLmodel$, without any feedback connection in which outputs of the model are fed back into itself. This is in opposition to the so-called \emph{Recurrent Neural Network} structures.\\

We can express a typical classification-oriented MLP by the following equation:
\begin{equation}\label{eq:MLP}
\MLmodel(\vLeakVec) = \softmax\circ\lambda_n\circ\sigma_{n-1}\circ\lambda_{n-1}\circ\dots\circ \lambda_1(\vLeakVec)=\yyy \mbox{ ,}
\end{equation}
where:
\begin{itemize}
\item The $\lambda_i$ functions are typically the so-called \emph{Fully-Connected} (FC) layers and are expressible as affine functions: denoting $\vLeakVec\in\mathbb{R}^D$ the input of an FC, its output is given by $\mathbb{A}\vLeakVec + \vec{b}$, being $\mathbb{A}\in\mathbb{R}^{D\times C}$ a matrix of weights and $\vec{b}\in\mathbb{R}^C$ a vector of biases. These weights and biases are the trainable weights of the FC layer. They are called \emph{Fully-Connected} because each $i$-th input coordinate is \emph{connected} to each $j$-th output via the $\mathbb{A}[i,j]$ weight. FC layers can be seen as a special case of the linear layers in general feedforward networks, in which not all the connections are present. The absence of some $(i,j)$-th connections can be formalized as a constraint for the matrix $\mathbb{A}$ consisting in forcing to $0$ its $(i,j)$-th coordinates.

\item  The $\sigma_i$ are the so-called \emph{activation functions} (ACT): an activation function is a non-linear real function that is applied independently to each coordinate of its input. We denote them by $\sigma$ since in general they are functions similar to the \emph{logistic sigmoid} introduced in \ref{example:LDA} and denoted by $\sigma$ as well: indeed historically sigmoidal functions where recommended, \ie real-valued, bounded, monotonic, and differentiable functions with a non-negative first derivative. Nevertheless, the recommended function in modern neural network literature is the so-called \emph{Rectified Linear Unit} (ReLU), introduced by \cite{nair2010rectified}} and defined as $\mathrm{ReLU}(\vLeakVec)[i] = max(0,\vLeakVec[i])$. Even it this function is not sigmoidal, not being bounded, nor differentiable, near transformation the fact of being a non-linear transformation but still piecewise linear, allows to preserve many of the properties that make linear models easy to optimize with gradient-
function.

\item $\softmax$ is the  \emph{softmax}\footnote{To prevent underflow, the log-softmax is usually preferred if several classification outputs must be combined.} function (SOFT), already introduced in \ref{example:LDA}: $\softmax(\vLeakVec)[i] = \frac{e^{\vLeakVec[i]}}{\sum_{j}e^{\vLeakVec[j]}}$.
\end{itemize}


 Examples of ACT layers are the {\em sigmoid} $f(\vLeakVec)[i] = (1+e^{-\vLeakVec[i]})^{-1}$ or the {\em rectified linear unit} (ReLU) $f(\vLeakVec)[i] = max(0,\vLeakVec[i])$. In general they do not depend on trainable weights.\\
 
The role of the \emph{softmax} is to renormalise the output scores in such a way that they define a probability distribution $\yyy \approx \mathrm{Pr}[\ZZZ | \XXX = \xxx]$.

In this way, the computed output does not only provide the most likely label to solve the classification problem, but also the likelihood of the remaining $|\sensVarSet|-1$ other labels. In the profiling SCA context, this form of output allows us to enter it in \eqref{eq:bayes} (setting the preprocessing function $\extract$ equal to the identity) to rank key candidates; actually \eqref{eq:MLP} may be viewed as an approximation of the pdf in \eqref{eq:prob_distr}.\footnote{Remarkably, this places SCAs based on MLP as a particular case of the classical profiling attack that exploits the maximum likelihood as distinguisher.}  We can thus rewrite \eqref{eq:bayes} as:

\begin{equation}\label{eq:NN_SCA}
d_k = \prod_{i=1}^{N} F(\xxx_i)[f(p_i,k)].
\end{equation}

We refer to \cite{lin2016does} for an (excellent) explication over the relationship between the softmax function and the Bayes theorem.



% descrizione (dal paper) 
% universal approximation theorem (da pagg 194 e seguenti del deeplearningbook)

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Misalignment of Side-Channel Traces}

\subsection{The Necessity and the Risks of Applying Realignment Techniques}
\subsection{Analogy with Image Recognition Issues}

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Convolutional Layers to Impose Shift-Invariance}

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Data Augmentation for Misaligned Side-Channel Traces}
%\todo{cita o qui o all'inizio il paper di CARDIS 2017 sulla trace augmentation}
%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Experiments against Software Countermeasures}


%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

\section{Experiments against Artificial Hardware Countermeasures}\label{sec:hardware}%label citato in appendice artifical jitter

%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------

\section{Experiments against Real-Case Hardware Countermeasures}