% Chapter Template

\chapter{Linear Dimensionality Reduction} % Main chapter title
% https://stat.columbia.edu/~cunningham/pdf/CunninghamJMLR2015.pdf
\label{ChapterLinear}


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}
\subsection{Principal Component Analysis}
\subsection{Linear Discriminant Analysis}
\subsection{Projection Pursuits}
%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Principal Component Analysis}
\subsection{Statistical Point of View}
\subsection{Geometrical Point of View}

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Application of PCA in SCAs}
\subsection{Original vs Class-Oriented PCA}
\begin{remark}
Stacked Auto-Encoders...
\end{remark}
\subsection{The Choice of the Principal Components}\label{sec:ELV}
The introduction of the PCA method in SCA context (either in its classical or class-oriented version)  has raised some important questions: \textit{how many} principal components and \textit{which ones} are sufficient/necessary to reduce the trace size (and thus the attack processing complexity) without losing important discriminative information?\\

Until now, an answer to the questions above has been given in \cite{choudary2014efficient}, linked to the concept of {\em explained variance} (or {\em explained global variance}, EGV for short) of a PC $\AAlpha_i$:
\begin{equation}\label{eq:EGV}
\mathrm{EGV}(\AAlpha_i) =  \frac{\lambda_i}{\sum_{k=1}^r\lambda_k} \mbox{ ,}
\end{equation}
where $r$ is the rank of the covariance matrix $\covmat$, and $\lambda_j$ is the eigenvalue associated to the $j$-th PC $\AAlpha_j$. $\mathrm{EGV}(\AAlpha_i)$ is the variance of the data projected over the $i$-th PC (which equals $\lambda_i$) divided by the total variance of the original data (given by the trace of the covariance matrix $\covmat$, {\em i.e.} by the sum of all its non-zero eigenvalues). By definition of EGV, the sum of all the EGV values is equal to $1$; that is why this quantity is often multiplied by $100$ and expressed as percentage.
Exploiting the EGV to choose among the PCs consists in fixing a wished {\em cumulative explained variance} $\beta$ and in keeping $\newTraceLength$ different PCs, where $\newTraceLength$ is the minimum integer such that
\begin{equation}
\mbox{EGV}(\AAlpha_1) +\mbox{EGV}(\AAlpha_2) + \dots +\mbox{EGV}(\AAlpha_\newTraceLength) \geq \beta \mbox{ .}
\end{equation}
However, if the adversary has a constraint for the reduced dimension $\newTraceLength$, the EGV notion simply suggests to keep the first $\newTraceLength$ components, taking for granted that the optimal way to chose PCs is in their natural order. This assumption is not always confirmed in SCA context: in some works, researchers have already remarked that the first components sometimes contain more noise than information \cite{Batina2012,specht} and it is worth discarding them. For the sake of providing a first example of this behaviour on publicly accessible traces, we applied a class-oriented PCA on 3000 traces from the DPA contest v4 \cite{DPAcontest}; we focused over a small 1000-dimensional window in which, in complete knowledge about masks and other countermeasures, information about the first Sbox processing leaks (during the first round). In Fig.~\ref{fig:DPAcontest} the first and the sixth PCs are plotted. It may be noticed that the first component indicates that one can attend a high variance by exploiting the regularity of the traces, given by the clock signal, while the sixth one has high coefficients localised in a small time interval, very likely to signalize the instants in which the target sensitive variable leaks.

\begin{figure}
\includegraphics[width=.45\textwidth]{../Figures/CARDIS2015/DPAcontestPC1_new.pdf} 
\includegraphics[width=.45\textwidth]{../Figures/CARDIS2015/DPAcontestPC6_new.pdf} 
\caption{First and sixth PCs in DPA contest v4 trace set (between time samples 198001 and 199000)}\label{fig:DPAcontest}
\end{figure}
To the best of our knowledge, a single method adapted to SCA context has been proposed until now to automatically choose PCs \cite{SCAclassProbl} while dealing with the issue raised in Fig.~\ref{fig:DPAcontest}. It is based on the following assumption:
\begin{assumption}\label{assum:local}
The leaking side-channel information is localised in few points of the acquired trace.
\end{assumption}
In the rest of the paper, we conduct our own analyses under Assumption \ref{assum:local} that we think to be reasonable in SCA contexts where the goal of the security developers is to minimize the number of leaking points.
Under this assumption, the authors of \cite{SCAclassProbl} use for side-channel attack purposes the {\em Inverse Participation Ratio} (IPR), a measure widely exploited in Quantum Mechanics domain (see for example \cite{guhr1998random}). They propose to use such a score to evaluate the eigenvectors {\em localization}. It is defined as follows:
\begin{equation}
\mathrm{IPR}(\AAlpha_i) = \sum_{j=1}^\traceLength \AAlpha_i[j]^4 \mbox{ .}
\end{equation}
The authors of \cite{SCAclassProbl} suggest to collect the PCs in decreasing order with respect to the IPR score.\\

The selection methods provided by the evaluation of the EGV and of the IPR are somehow complementary: the former is based only on the eigenvalues associated to the PCs and does not consider the form of the PCs themselves; the latter completely discards the information given by the eigenvalues of the PCs, considering only the distribution of their coefficients. One of the contributions of the present paper is to propose a new selection method, that builds a bridge between the EGV and the IPR approaches. As we will argue, our method, based on the so-called {\em explained local variance}, does not only lead to the construction of a new selection criterion, but also permits  to modify the PCs, choosing individually the coefficients to keep and those to discard. 

\subsection{The Explained Local Variance Selection Method}
\todo{Riprendere le notazioni e mettere apposto i newcommand} 
%The method we develop in this section is based on a compromise between the variance provided by each PC (more precisely its EGV) and the number of time samples necessary to achieve a consistent part of such a variance. To this purpose we  introduce the concept of {\em Explained Local Variance} (ELV).
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/cumulativeELVallRectangle.pdf} 
%\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/cumulativeELVzoomedRectangle.pdf} 
%\caption{Cumulative ELV trend of principal components. On the right a zoom of the plot on the left. Data acquisition described in Sec.~\ref{sec:experiments}.}\label{fig:ELVcumulative}
%\end{figure}
%
%Let us start by giving some intuition behind our new concept. Thinking to the observations ${\sss[]{}}^\intercal$, or to the class-averages ${\mmmX}^\intercal$ in class-oriented PCA case, as realizations of a random variable $\XXX^\intercal$, we have that $\lambda_i$ is an estimator for the variance of the random variable $\XXX^\intercal\cdot\AAlpha_i$. Developing, we obtain
%\begin{align}\label{eq:ELV}
%\lambda_i =& \hat{\mathrm{var}}(\sum_{j=1}^D \XXX^\intercal[j]\AAlpha_i[j]) = \sum_{j=1}^D\sum_{k=1}^D \hat{\mathrm{cov}}(\XXX^\intercal[j]\AAlpha_i[j], \XXX^\intercal[k]\AAlpha_i[k])=\\
%=& \sum_{j=1}^D \AAlpha_i[j]\sum_{k=1}^D\AAlpha_i[k]\hat{\mathrm{cov}}(\XXX^\intercal[j], \XXX^\intercal[k])= \sum_{j=1}^D \AAlpha_i[j] (\covmat_{j}^\intercal \cdot \AAlpha_i)=  \\
%=& \sum_{j=1}^D \AAlpha_i[j] \lambda_i\AAlpha_i[j]= \sum_{j=1}^D  \lambda_i \AAlpha_i[j]^2 \label{eq:toJustify}
%\end{align}
%where $\covmat_{j}^\intercal$ denotes the $j$-th row of $\covmat$ and \eqref{eq:toJustify} is justified by the fact that $\AAlpha_i$ is an eigenvector of $\covmat$, with $\lambda_i$ its corresponding eigenvalue. The result of this computation is quite obvious, since $\parallel \AAlpha_i\parallel=1$, but it evidences the contribution of each time sample in the information held by the PC. This makes us introduce the following definition:
\begin{definition}


The {\em Explained Local Variance} of a PC $\AAlpha_i$ in a sample $j$, is defined by
\begin{equation}
\mathrm{ELV}(\AAlpha_i,j) = \frac{\lambda_i \AAlpha_i[j]^2}{\sum_{k=1}^r\lambda_k} = \mathrm{EGV}(\AAlpha_i) \AAlpha_i[j]^2  \mbox{ .}
\end{equation}
\end{definition}
\begin{figure}
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC1.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC2.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC3.pdf} \\
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC4.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC5.pdf} 
\includegraphics[width=0.31\textwidth]{../Figures/CARDIS2015/PC6.pdf} 
\caption{The first six PCs. Acquisition campaign on an 8-bit AVR Atmega328P (see Sec.~\ref{sec:experiments}).}\label{fig:6components}
\end{figure}
Let $\mathcal{J}=\{j^i_1, j^i_2, \dots, j^i_{\traceLength}\}\subset\{1,2,\dots,\traceLength\}$ be a set of indexes sorted such that $\mathrm{ELV}(\AAlpha_i,j^i_1)\geq \mathrm{ELV}(\AAlpha_i,j^i_2)\geq \dots \geq \mathrm{ELV}(\AAlpha_i,j^i_\traceLength)$.
It may be observed that the sum over all the $\mathrm{ELV}(\AAlpha_i,j)$, for $j\in[1,\dots,\traceLength],$   equals $\mathrm{EGV}(\AAlpha_i)$. If we operate such a sum in a cumulative way following the order provided by the sorted set $\mathcal{J}$, we obtain a complete description of the trend followed by the component $\AAlpha_i$ to achieve its EGV. As we can see in Fig.~\ref{fig:ELVcumulative}, where such cumulative ELVs are represented, the first 3 components are much slower in achieving their final EGV, while the $4^\text{th}$, the $5^\text{th}$ and the $6^\text{th}$ achieve a large part of their final EGVs very quickly ({\em i.e.} by adding the ELV contributions of much less time samples). For instance, for $i=4$, the sum of the $\mathrm{ELV}(\AAlpha_4, j^4_k)$, with $k\in[1,\dots,30]$, almost equals $\mathrm{EGV}(\AAlpha_4)$, whereas the same sum for $i=1$ only achieves about the 15\% of $\mathrm{EGV}(\AAlpha_1)$. Actually, the EGV of the $4^\text{th}$, the $5^\text{th}$ and the $6^\text{th}$ component only essentially depends on a very few time samples. This observation, combined with Assumption \ref{assum:local}, suggests that they are more suitable for SCA than the three first ones. To validate this statement, it suffices to look at the form of such components (Fig.~\ref{fig:6components}): the leading ones are very influenced by the clock, while the latest ones are well localised over the leaking points.\\

Operating a selection of components {\em via} ELV, in analogy with the EGV, requires to fix the reduced space dimension $\newTraceLength$, or a threshold $\beta$ for the cumulative ELV. In the first case, the maximal ELVs of each PC are compared, and the $\newTraceLength$ components achieving the highest values of such ELVs are chosen. In the second case, all pairs (PC, time sample) are sorted in decreasing order with respect to their ELV, and summed until the threshold $\beta$ is achieved. Then only PCs contributing in this sum are selected. \\

We remark that the ELV is a score associated not only to the whole components, but to each of their coefficients. This interesting property can be exploited to further remove, within a selected PC, the non-significant points, {\em i.e.} those with a low ELV. In practice this is done by setting these points to zero. That is a natural way to exploit the ELV score in order to operate a kind of {\em denoising} for the reduced data, making them only depend  on the significant time samples. In Sec.~\ref{sec:experiments} (scenario 4) we test the performances of an attack varying the number of time samples involved in the computation of the reduced data, and showing that such a denoising processing might impact significantly. 



%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Linear Discriminant Analysis}
\subsection{Statistical Point of View}
\subsection{Geometrical Point of View}

%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Application of LDA in SCAs}
\subsection{The Small Sample Size problem}



%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------


\section{Experimental Results}\label{sec:experiments}

In this section we compare the different extractors provided by the PCA and the LDA in association with the different techniques  of components selection. Defining an universal criterion to compare the different extractors would not make sense since the latter one should encompass a lot of parameters, sometimes opposite, that vary according to the context (amount of noise, specificity of the information leakage, nature of the side channel, etc.). For this reason we choose to split our comparisons into four scenarios. Each scenario has a single varying parameter that, depending on the attacker context, may wish to be minimized. Hereafter the definition of the four scenario. In the following only results of the two first is reported, the interested reader might refer to Appendix\ref{Appendix_scenario3_4_cardis2015} for results of in the two other scenarios.  
\begin{itemize}
\item[Scenario 1] varying parameter: number of attack traces $\nbAttackTraces$, 
\item[Scenario 2] varying parameter: number of profiling traces $\nbProfilingTraces$, 
\item[Scenario 3] varying parameter: number of projecting components selected $\newTraceLength$,
\item[Scenario 4] varying parameter: number of original time samples implied into the trace preprocessing computation $\numPoI$ .
\end{itemize}
 
For scenarios in which $\\nbProfilingTraces$ is fixed, the value of $\nbProfilingTraces$ is chosen high enough to avoid the SSS problem, and the extensions of LDA presented in Sec.~\ref{sec:SSS} are not evaluated.
 This choice of $\nbProfilingTraces$ will imply that the LDA is always performed in a favourable situation, which makes expect the LDA to be particularly efficient for these experiments. Consequentely, for the scenarios in which $\nbProfilingTraces$ is high, our goal is to study whether the PCA can be made almost as efficient as the LDA thanks to the component selection methods discussed in Sec.~\ref{sec:ELV}. 



\todo{This part will maybe be useless: somewhere I will have described all trace sets}
\subsubsection{The testing adversary.}  
%Our testing adversary attacks an 8-bit AVR microprocessor Atmega328P and acquires power-consumption traces via the ChipWhisperer platform \cite{o2014chipwhisperer}.\footnote{This choice has been done to allow for reproducibility of the experiments.} The target device stores a secret 128-bit key and performs the first steps of an AES: the loading of 16 bytes of the plaintext, the AddRoundKey step and the AES Sbox. It has been programmed twice: two different keys are stored in the device memory during the acquisition of the profiling and of the attack traces, to simulate the situation of two identical devices storing a different secret. The size $\traceLength$ of the traces equals $3996$. The sensitive target  variable is the output of the first Sbox processing, but, since the key is fixed also during the profiling phase, and both Xor and Sbox operations are bijective, we expect to detect three interesting regions (as those high-lighted by PCs 4, 5 and 6, in Fig.~\ref{fig:6components}): the reading of the first byte of the plaintext, the first AddRoundKey and the first Sbox. We consider an {\em identity classification} leaking function (i.e. we make minimal assumption on the leakage function), which implies that the 256 possible values of the Sbox output yields to 256 classes. For each class we assume that the adversary acquires the same number $N_p$ of traces, \textit{i.e.} $\numTraces[]' = N_p\times 256$. After the application of the extractor $\extract$, the trace size is reduced to $\newTraceLength$. Then the attacker performs a Bayesian Template Attack \cite{Chari2003}, using $\newTraceLength$-variate Gaussian templates. This choice comes from the information-theoretic optimality of such an attack which, exploiting the maximum likelihood parameters estimation, yields to an unbiased comparison between the extractors.


\begin{figure}[t]
\subfigure[]{\label{fig:1.1}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015//Criterion1.pdf}}
\subfigure[]{\label{fig:1.2}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015//Criterion1Good.pdf}}
\caption[Guessing Entropy as function of the number of attack traces]{Guessing Entropy as function of the number of attack traces for different extraction methods. All Guessing Entropies are estimated as the average rank of the right key over 100 independent experiments.}\label{fig:scenario1}
\end{figure}
\subsubsection{Scenario 1.}
\todo{cos'e $N_z$}
To analyse the dependence between the extraction methods presented in Sections~\ref{sec:PCA} and \ref{sec:LDA} and the number of attack traces $\nbAttackTraces$ needed to achieve a given GE, we fixed the other parameters as follows: $N_z=50$ ($\nbProfilingTraces=50\times 256$), $\newTraceLength = 3$ and $\numPoI = 3996$ (all points are allowed to participate in the building of the PCs and of the LDCs). The experimental results, depicted in Fig.~\ref{fig:scenario1}\subref{fig:1.1}-\subref{fig:1.2}, show that the PCA standard method has very bad performances in SCA, while the LDA outperforms the others. Concerning the class-oriented PCA, we observe that its performance is close to that of LDA when combined with the selection methods ELV (which performs best) or IPR.  



\subsubsection{Scenario 2.}
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS.pdf}
%\includegraphics[width=0.5\textwidth]{figures/Criterion2notSSS.pdf} 
%\caption{Guessing Entropy as function of the number of profiling traces per class, for different extraction methods: on the left the LDA is substituted by its extensions to handle the SSS problem.}\label{fig:2}
%\end{figure}
Now we test the behaviour of the extraction methods as function of the number $N_z$ of available profiling traces per class. The number of components $\newTraceLength$ is still fixed to 3, $\numPoI=3996$ again and the number of attack traces is $\nbAttackTraces=100$. This scenario has to be divided into two parts: if $N_z\leq 15$, then $\nbProfilingTraces<\traceLength$ and the SSS problem occurs. Thus, in this case we test the four extensions of LDA presented in Sec.~\ref{sec:SSS}, associated to either the standard selection, to which we abusively refer as EGV,%\footnote{It consists in keeping the $\newTraceLength$ first LDCs, except for the Direct LDA, which asks to keep the last LDCs.}
\footnote{It consists in keeping the $\newTraceLength$ first LDCs (the $C$ last for the Direct LDA)}
or to the IPR selection.  We compare them to the class-oriented PCA associated to EGV, IPR or ELV. The ELV selection is not performed for the techniques extending LDA, since for some of them the projecting LDCs are not associated to some eigenvalues in a meaningful way. On the contrary, if $N_z\geq 16$ there is no need to approximate the LDA technique, so the classical one is performed. Results for this scenario are shown in Fig.~\ref{fig:scenario2}. It may be noticed that the combinations class-oriented PCA + ELV/IPR select exactly the same components, for our data, see Fig.~\ref{fig:pcaclass} and do not suffer from the lack of profiling traces. They are slightly outperformed by the $\SW$ Null Space method associated with the EGV, see Fig.\ref{fig:swnullspace}. The Direct LDA (Fig.~\ref{fig:direct}) method also provides a good alternative, while the other tested methods do not show a stable behaviour. The results in absence of the SSS problem (Fig.\ref{fig:notSSS}) confirm that the standard PCA is not adapted to SCA, even when provided with more profiling traces. It also shows that among class-oriented PCA and LDA, the class-oriented PCA converges faster.



\begin{figure}
\subfigure[]{\label{fig:fisherface}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_Fisherface.pdf}}
\subfigure[]{\label{fig:direct}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_DirectLDA.pdf}}
\subfigure[]{\label{fig:stspannedspace}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_STSpannedSpace.pdf}}
\subfigure[]{\label{fig:swnullspace}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_SWnullSpace.pdf}}
\subfigure[]{\label{fig:pcaclass}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2SSS_PCAclass.pdf}}
\subfigure[]{\label{fig:notSSS}
\includegraphics[width=0.5\textwidth]{../Figures/CARDIS2015/Criterion2notSSS.pdf}}
\caption[Guessing Entropy as function of the number of profiling traces.]{Guessing Entropy as function of the number of profiling traces. Figures \subref{fig:fisherface}-\subref{fig:swnullspace}: methods extending the LDA in presence of SSS problem; Figure \subref{fig:pcaclass}: class-oriented PCA in presence of the SSS problem; Figure \subref{fig:notSSS}: number of profiling traces high enough to avoid the SSS problem.}\label{fig:scenario2}
\end{figure}

%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------
\section{Misaligning Effects}
\todo{give parameters: 6 4}
In this section we experimentally show how the approach based on linear dimensionality reduction described in this chapter is affected by traces misalignment. To this aim, we simply take the same data and parameters exploited for Scenario 1 in Sec.~\ref{sec:experiments}, and artificially misalign them through the technique proposed in Appendix~\ref{appendix:artificial_jitter} with parameters \todo{parameters here}. Then we tried to pre-process attack them through the 9 methodology tested in Scenario 1. It may be noticed in Fig.~\ref{fig:PCA_LDA_misalignment} that none of the 9 techniques is still efficient, included the optimal LDA+EGV that lead to null guessing entropy the synchronized traces using less 7 attack traces. In this case it cannot lead to successful attack in less than 3000 traces.
\begin{figure}
\includegraphics[width=\textwidth]{../Figures/desynchro_results_PCA_LDA.pdf} 
\caption{Degradation of linear-reduction-based template attacks in presence of misalignment.}\label{fig:PCA_LDA_misalignment}
\end{figure}