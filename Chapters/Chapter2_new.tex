\chapter{Introduction to Side-Channel Attacks} % Main chapter title

\label{ChapterIntroductionSCA}

\section{Notations and Probability and Statistics Recalls}\label{sec:notations}
\paragraph*{Basic Notations.}
In this thesis we use calligraphic letters as $\mathcal{X}$ to denote
sets, the corresponding upper-case letter $X$ to denote random variables (random
vectors $\vec{X}$ if with an arrow) over $\mathcal{X}$, and the corresponding
lower-case letter $x$ (resp. $\vec{x}$ for vectors) to denote realizations of
$X$ (resp. $\vec{X}$). The cardinality of a set $\mathcal{X}$ is denoted by $\lvert\mathcal{X}\rvert$. Matrices will be denoted with bold capital letters. When the vectors' orientation minds, they are understood as column vectors. The
$i$-th entry of a vector $\vec{x}$ is denoted by $\vec{x}[i]$, while the transposed of a vector $\vec{x}$ is denoted as $\vec{x}^\intercal$. We will use the transposed mark to refer to row vectors $\vec{x}^\intercal$. In general the $i$th observation of a random vector $X$ will be denoted by $x_i$. Observations will sometimes be grouped into datasets $\setData=\{x_1,\dots, x_N\}$. Throughout this thesis, a finite set $\sensVarSet = \{\sensVarValue{1}, \dots, \sensVarValue{\nbClasses}\}$ will be often considered: it will always denote the possible values for a \emph{sensitive variable} $\sensRandVar$ (see later). Its elements are sometimes encoded via a so-called \emph{one-hot-encoding}: to each element $\sensVarValue{j}$ a $\numClasses$-dimensional vector  $\sensVarOneHot{j}$ is associated,
with all entries equal to $0$ and the $j$-th entry equal to $1$: $\sensVarValue{j}
\rightarrow \sensVarOneHot{j} = (0,\ldots , 0,\underbrace{1}_{j},0,\dots,0)$. We will denote by $\sensVarGenValue$ a generic element of $\sensVarSet$, when useless to specify its index $\sensVarValue{i}$ for the context.

\paragraph*{Probability Notations.}
The probability of a random variable $X$ taking value in a subset $\mathcal{U}\in\mathcal{X}$ is denoted by $\prob(X\in\mathcal{U})$, or simply by $\prob(\mathcal{U})$ if not ambiguous. When $\mathcal{U}$ is reduced to a singleton $\mathcal{U}=\{x\}$ the same probability is denoted by $\prob(X=x)$ or simply by $\prob(x)$ if not ambiguous. If $X$ is a discrete variable $\pdf_X$ denotes its probability mass function (pmf for short), such that $\prob(X\in\mathcal{U})=\sum_{x\in \mathcal{U}}\pdf_X(x)$. The same symbol $\pdf_X$ is used to denote the probability density function (pdf for short) if $X$ is a continuous variable, such that $\prob(X\in \mathcal{U}) = \int_{\mathcal{U}}\pdf_X(x)dx$, for $[a,b]\subset\mathcal{X}$. 
The symbol $\esper[f(X)]$, or equivalently $\esper_X[f(X)]$, denotes the expected value of a function $f$ of the random value $X$, under the distribution of $X$. In the same way, symbols $\var(X)$ and $\var_X(X)$ denote the variance of $X$.

When two random variables $X$ and $Y$ are considered, their joint probability is denoted by $\prob(X=x,Y=x)$, or simply by $\prob(x,y)$ if not ambiguous, and their joint probability density (or mass) function is denoted by $\pdf_{X,Y}(x,y)$. The conditional probability of $X$ assuming the value $x$ given an outcome $y$ for $Y$ is denoted by $\prob (\given{X=x}{Y=y})$, or simply by $\prob(\given{x}{y})$ if not ambiguous. The conditional probability density (or mass) function of $X$ given an outcome $y$ for $Y$ is denoted by $\pdf_{\given{X} {Y=y}}(x)$. Finally, the covariance of the two variables is denoted by $\cov(X,Y)$.\\


\paragraph*{Bayes' Theorem.}
We recall some basic probability rules. For every $x\in\mathcal{X}$ and for every $y\in\mathcal{Y}$ we have what follows:
\begin{itemize}
\item \emph{Symmetry of joint probabilities: } $\pdf_{X,Y}(x,y)=\pdf_{Y,X}(y,x)$;
\item \emph{Marginal probabilities from joint ones: } $\pdf_X(x) = \sum_{Y=y}\pdf_{X,Y}(x,y)$ (where the sum has to be intended as an integral if $Y$ is a continuous variable);
\item \emph{Product rule: } $\pdf_{X,Y}(x,y) = \pdf_{\given{Y}{X=x}}(y)\pdf_{X}(x)$;
\end{itemize} 
These rules are sufficient to demonstrate, in the case of discrete random variables $X,Y$, a key stone of probability theory, the Bayes' theorem: 
\begin{equation}
\pdf_{\given{X}{Y=y}}(x) = \frac{\pdf_{\given{Y}{X=x}(y)}\pdf_{X}(x)}{\pdf_{Y}(y)} \mbox{ ;}
\end{equation}
the marginal probability function $\pdf_X$ is referred to as \emph{prior} probability of $X$, and describes the distribution of $X$ without taking into account the variable $Y$. The conditional probability $\pdf_{\given{X}{Y=y}}$ is referred to as \emph{posterior} probability of $X$, and gives the distribution of $X$ once the outcome $y$ of $Y$ is taken into account. Notions of measure's theory are needed to show that Bayes' theorem is valid and keeps unchanged in case of continuous random variables and in cases in which one of the two involved variables is discrete and the other one is continuous. The interested reader might refer to \cite{feller2008introduction}. 

\paragraph*{The Gaussian distribution.}
The Gaussian or normal distribution is a widely used model for the distribution of continuous variables. We use the symbol $X\sim\mathcal{N}(\mu,\sigma^2)$ to denote a random variable $X$ that follows a Gaussian distribution with parameters $\mu\in\mathbb{R}$ and $\sigma^2\in\mathbb{R}^{+}$. For a $D$-dimensional random vector $\vec{X}$ we use the symbol $X\sim\mathcal{N}(\mumumu,\Sigma)$ to denote a vector that follows a multivariate Gaussian distribution with parameter $\mumumu\in\mathbb{R}^D$ and $\Sigma\in\mathbb{R}^{D\times D}$ positive-definite. The density of the Gaussian distribution is completely determined by the value of its two parameters. It is given by the following expressions, in unidimensional and multidimensional case: 
\begin{equation}
\pdf_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}\mbox{ ,}
\end{equation}
\begin{equation}
\pdf_{\vec{X}}(\vec{x}) = \frac{1}{\sqrt{2\pi\det(\Sigma)}}\exp{-\frac{1}{2}\left(\vec{x}-\mumumu\right)^\intercal \Sigma^ {-1}(\vec{x}-\mumumu)} \mbox{ .}
\end{equation}
The expected value of a Gaussian distribution coincides with the parameter $\mu$ for the univariate case and with $\mumumu$ for the multivariate one. The parameter $\sigma^2$ coincides with the variance of the univariate distribution, while $\Sigma$ coincides with the covariance matrix of the multivariate one.

\paragraph*{Basics of Statistics.}
The word \emph{statistics}  refers to a branch of mathematics that aims to analyse, describe or interpret observed data. Differently, the word \emph{statistic} refers to any measure obtained applying a function to some observed data. Let $\setData = \{x_1,\dots,x_N\}$ be a dataset of observations of a random variable $X$. We might distinguish two sub-branches in statistics: the descriptive statistics, and the inferential statistics. In descriptive statistics data are described by means of more or less complex statistics (in the sense of measure), the most common of them being the \emph{arithmetic mean}: 
\begin{equation}
\overline{x} = \frac{1}{N}\sum_{i=1}^N x_i \mbox{ .}
\end{equation}

In inferential statistics data are considered as sample observation of random variables and the data analysis aims at modelling the distribution of such variables. Dealing with random variables, inferential statistics exploit the probability theory framework and theorems. Statistics of data (in the sense of measures) play an important role in inferential statistics as well, usually aiming to estimate some random variable parameters. In this case they are called \emph{estimators} and will be denoted by a hat: for example, $\esperEst[X]$  denotes an estimator for the expected value of $X$ and $\varEst(X)$ denotes an estimator for the variance of $X$. The most classical estimator for the expected value is the arithmetic mean $\overline{x}$. It has several valuable properties, for example it is \emph{unbiased}, in the sense that, considering it as a random variable, its expected value coincides with the true value of $\esper[X]$. Moreover, it is the \emph{maximum-likelihood} estimator under the Gaussian distribution: for data that are independent among each other and drawn from a Gaussian distribution,  the arithmetic mean of the observed data $\setData$ is the value for the parameter $\mu$ that maximises the probability of observing the data $\setData$. A common unbiased estimator for the variance is the following so-called sample variance: 
\begin{equation}
\varEst = \frac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x})^2 \mbox{ ;}
\end{equation}
when the observed random variable  follows a Gaussian distribution, such an estimator differs from the maximum-likelihood one, in which the factor $\frac{1}{N-1}$ is substituted by $\frac{1}{N}$. In the same way, acting with Gaussian random vectors, the maximum-likelihood estimator of the covariance matrix is biased, and differs from the common unbiased one for a multiplicative factor. \\

Various approaches exist to make statistical inference. The two main ones are the frequentist approach and the Bayesian one. The frequentist inference is an approach that draws conclusions exclusively from sample data. It makes use of methodologies like the statistical hypothesis testing and the confidence interval, and in this thesis it is sometimes referred to as classical statistics. In the frequentist approach, parameters that define the distribution of the analysed random variable are priorly considered as fixed and unknown, and are estimated or tested on the sole basis of the observation of the sample data $\setData$. A second approach is the Bayesian inference, for which parameters that describe the analysed random variable are admitted to be probabilistic: in Bayesian inference, before the observation of sample data, the parameters have a prior distribution that reflects the knowledge and belief of the data-scientist about them. The observation of data leads to an update procedure, based on the Bayes' theorem, that allows such probability distribution of parameters to become more and more appropriate, each time exploiting the new available information. For both approaches, the maximum-likelihood is an optimal statistical principle and is widely exploited to choose parameters, in the frequentist approach, or to update parameters' probability distributions in the Bayesian one. Up-to-now the Bayesian inference has never explicitly influenced the Side-Channel Attack literature, nor we will use such a framework in this thesis. We leave this track opened for future works, briefly discussing its suitability for Side-Channel Attacks domain in Chapter~\ref{ChapterConclusions}.

\section{Side-Channel Attacks}
\subsection{Introduction}
\subsubsection{Generalities: Grey-Box, Divide-and-Conquer}
Side-Channel Attacks (SCA) belong to the cryptanalysis domain, since they aim to breach cryptographic security systems. Usually their goal is to retrieve a secret variable of a cryptographic algorithm, typically a secret key. They distinguish from classic mathematical cryptanalysis techniques by the fact that they are based on information gained from the physical implementation of a cryptosystem, rather than theoretical weaknesses in the algorithms. 
The possibility to physically observe the electronic device that performs the cryptographic computations, allows Side-Channel Attacks to go beyond the cryptographic complexity that assures resistance against classical cryptanalysis strategies.  Indeed, no matter the size of the secret variables manipulated by the algorithm and the algebraic complexity of the encrypting/decrypting operations, an physical implementation of any algorithm always handles variables of a bounded size, that depends on the hardware architecture of the cryptographic device. For example, in an 8-bit architecture an AES with 128-bit-sized key will be necessarily implemented as multiple partial computations over 8-bit blocks of data. In classical cryptanalysis, the typical attacker model faces to a black-box that performs the cryptographic algorithm: an attacker may make queries to the black-box, asking for ciphertexts of given plaintexts or viceversa. The black-box acts as a function that output the asked value, but does not provide any information about partial computations. On the contrary, a side-channel attacker is said to face to a \emph{grey-box} model: he has a way to obtain noised information about partial computations. This allows him to perform a \emph{divide-and-conquer} strategy: if his goal is to retrieve the full 128-bit AES key, he will smartly divide his problem into the recovery of small parts of such keys at time, called \emph{key chunks},\footnote{or \emph{subkeys} when they coincide to a byte of key for the AES algorithm} making the complexity of the attack significantly drop. 

\subsubsection{An Overview}
Since the seminal paper by Paul Kocher in 1996 \cite{kocher1996timing}, side-channel analysis domain has developed fast, together with its flourish literature. Without being exhaustive, the last literature includes: proposals for new kind of exploitable signals, proposals for useful statistical tools, new attacks strategies and routines, analysis of side-channel vulnerabilities of well-specified cryptographic algorithms, side-channel countermeasures, formal proofs of countermeasures' security claims, discussions about tools and metrics to compare side-channel attacks and strategies, reports of real-case successful attacks, and a few attempts to unify the side-channel literature under some comprehensive frameworks. The contributions we present in this thesis may be resumed as proposals for statistical tools for some specific attack contexts. The aim of the following part of this section is not to provide a comprehensive state-of-the-art of side-channel domain, but to provide the reader with the necessary concepts to understand such contributions, and get a view of the contexts in which they can provide improvements to the state-of-the-art. To this aim we propose a brief overview of the main properties that define and characterise a side-channel attack among others, dwelling on aspects that are the mainly concern our contributions, \eg the concept of \emph{profiing attack}. 

\subsection{Descriptors of a Side-Channel Attacks}
The main properties we identified to describe a side-channel attacks are the following:
\begin{itemize}
\item the physical nature of the exploited signals,
\item the sensitive variables,
\item the strategy family,
\item the form,
\item and the attacker knowledge.
\end{itemize} 

\subsubsection{Physical Nature of the Exploited Signals}\label{sec:physical_signals}
As already introduced in Sec.~\ref{sec:SCAintro}, a SCA may exploit signals obtained by the observation of different kind of \emph{side channels}. Mainly exploited physical quantities are the power consumption, the electromagnetic emanation, the elapsing time and the acoustic emanation. 

\subsection{Sensitive Variables}\label{sec:sensitive_variables}
Physical signals are acquired \via appropriate instrumentation, and collected into vectors called \emph{traces} (or \emph{acquisitions}). They will be denoted by $\vLeakVec_i$ and considered as observations of a random real vector $\vaLeakVec$, where each coordinate corresponds to a time sample of the acquired signal.  They are then interpreted as noisy observations of the intermediate variables handled by the device during the execution. An attacker is in particular interested to the so-called \emph{sensitive variables}:  they are quantities handled during the processing that depend somehow on a secret of the implementation, and not only on public variables, as a plaintext or an algorithm constant. Side-channel analysis acts clearing traces from noise, in such a way to determine with the highest possible precision the association between a trace (or a set of traces) and the value taken by the target \emph{sensitive variable} $\sensRandVar$ during its (their) acquisition. For an attack, a single or several sensitive variables may be targeted, and its/their algebraic relation with the secret key serves to complete the attack.  Actually, \emph{sensitive variables} would be more appropriately called \emph{sensitive targets}, since they might not be variable. Some typical examples of sensitive variables include: 


\begin{itemize}
\item $\sensRandVar = \keyRandVar$ with $\keyRandVar$ a secret subkey - this is the most direct choice for a sensitive target, nevertheless it is often not variable, since in some cases a device always manipulates the same key for a given embedded primitive. When the target is not variable we are performing a \emph{simple attack} (see Sec.~\ref{sec:strategies});
\item a cryptographic variable that depends on a sufficiently small subkey and a part of a known input variable $\publicParRandVar$: $\sensRandVar = \sensFunction(\keyRandVar,\publicParRandVar)$ - this is the most classical choice to perform a so-called \emph{differential} or \emph{advanced} SCA (see \ref{strategy_advanced});
\item any function of a cryptographic variable (ex: $\HW(\sensFunction(\keyRandVar,\publicParRandVar))$), where $\HW(\cdot)$ represents the Hamming weight operation, \ie the operation that counts the number of 1's in the binary string representing the entry. Sometimes, as for example we will see in Chapter~\ref{ChapterKernel} (see Sec.~\ref{sec:multiclass}) it can be interesting not to target a variable but a non-injective function of a variable as its Hamming weight; when the identity function is applied we are in the previous case;
\item an operation (ex: $\sensRandVar \in \{\mathrm{square}, \mathrm{multiply}\}$)
\item a register (ex: $\sensRandVar$ is the register used to store results of intermediate operations in a Montgomery ladder implementation of RSA)
\end{itemize}
In this thesis we will try as much as possible to abstract from the form of the sensitive variable, thinking of any entity $\sensRandVar$ that assumes values in a finite set $\sensVarSet$ and whose value permits an attacker to make inference on a secret of the implemented algorithm.

\subsubsection{The Strategy Family}\label{sec:strategies}
The wide range of attack strategies, together with their still-evolving taxonomy, makes the task of group attack strategies very hard. We propose here a simplified grouping into three strategy families, the \emph{Simple Attacks}, the \emph{Collision Attacks} and the \emph{Advanced Attacks}, but we highlight the fact that such a grouping is not sharp and impermeable in literature. Hereafter a brief description of \emph{Simple} and \emph{Collision} strategies is provided, while a bit larger introduction to \emph{Advanced Attacks} is provided in Sec.~\ref{sec:strategy_advanced}. 
\paragraph{Simple Attacks}
% TODO: timing attack example (add with figure)
% mangard2002simple ceerca di capire se sono simple attack anche per te
%  clavier2014simple
Simple Attacks are characterised by the fact that they do not require to observe the variation of the side-channel signals under the variation of the algorithm entries. The relevant information is obtained directly from the observation of trace pattern, without statistical analysis, and the sensitive variable coincides in general with the secret key (or a chunk of it). Typical targets for SPA attacks are cryptographic devices in which the execution path depends on the key. For example, considering software-implementation, branching to different instructions may occur when a secret key chunk has a specific value, in general dealing with operations as comparisons, multiplications or exponentiations. Since simple attacks do not ask for making entries vary, they are often referred to as \emph{one-trace attacks}, since the observation of a single trace is often sufficient to perform them. In literature the terms \emph{simple attacks} and \emph{one-trace attacks} are often considered equivalent. Anyway, we aim to include in the \emph{simple attacks} family those attacks for which many observations are acquired with fixed entry parameters and by consequence in which the observed leakage always corresponds to a fixed value of $\sensRandVar$. The attacker may exploit the several acquisitions in mainly two ways: he computes their average before performing the attack, aiming to reduce the noise influence, or he performs the attack of each acquisition (expecting each gives the same outcome) and then applies a function to the several outcomes (\eg majority vote) to guess the right label. We observe that this approach with several observations allows the attacker to reduce the noise impact, while observing many times the same variable. The relation between the variable and the key chunk being fixed (and being the identity, in general), he will not exploit algebraic relations to ameliorate his inference over the latter. \\
In next chapter, Section~\ref{sec:TPE}, we will describe some classic example of machine learning task. Here we point out the fact a simple attacks exactly correspond to resolving a \emph{classification task} in side-channel context. 

% 
 In 2002 Mangard \etal  \cite{mangard2002simple} proposed for example a simple power analysis (SPA) strategy, improved by Clavier \etal in 2014 \cite{clavier2014simple}, to retrieve the whole AES key observing leakages from a single execution of the AES key expansion. When Algebraic SCAs (see below) appeared in literature in 2009, their aim was to be a strategy to perform simple attacks as well.  


\paragraph{Collision Attacks}
Collision attacks were introduced by Schramm \etal in 2003 \cite{schramm2003new} as a side-channel generalisation of classic cryptanalysis collision attacks, typically used to break cryptographic hash functions. They deduct information about the secret key of a block cipher from the presence or the absence of an internal collision during the encryption (or decryption). A collision has to be intended as the fact that, while processing different inputs, an internal computation acts over the same operand, or outputs the same value. To perform a collision attack, the side-channel attacker is thus not required to interpret side-channel signals to perfectly understand which operation is executed and over which operands. The assumption is weaker: the attacker is supposed to be able to state if two signals (or portions of signals) correspond or not to the same operation. In the seminal work \cite{schramm2003new}, as in several further developments as \cite{schramm2004collision,bogdanov2007improved,bogdanov2008multiple}, sets of several acquisitions under well-chosen entries are exploited to establish, through statistical tools, \eg correlation estimators, whether a collision is present. In the same year 2003, Fouque and Valette \cite{fouque2003doubling} proposed a collision attack in a context declared by authors more favourable than block ciphers, \ie operations like modular exponentiation.\footnote{or scalar multiplication in the elliptic curve setting} In this context, authors proposed an attack strategy based on the observation and comparison of only two acquisitions. In analogy with simple attacks, often labelled as \textquotedbl one-trace\textquotedbl , correlation attacks are thus somehow categorised as \textquotedbl two-traces\textquotedbl attacks, even if this connotation is not always pertinent. Moreover, in the same way we observed that simple attacks perfectly declines the machine learning task of classification, we observe that collision attacks are in a strong analogy with another classical machine learning task, \ie the \emph{verification task}, that will be as well introduced in Sec.~\ref{sec:TPE}.


\subsubsection{The Form}
%horizontal / vertical...quando un advanced è fatto orizzontale, invece che variare l'entrata si varia osservano variazioni del valore di \sensrandvar dipendenti da diversi chunk dell'entry...puo diventare simple


