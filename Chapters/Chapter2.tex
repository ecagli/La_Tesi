\chapter{Introduction to Side-Channel Attacks} % Main chapter title

\label{ChapterIntroductionSCA}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Notations and Probability and Statistics Recalls}\label{sec:notations}
\paragraph*{Basic Notations.}
In this thesis we use calligraphic letters as $\mathcal{X}$ to denote
sets, the corresponding upper-case letter $X$ to denote random variables (random
vectors $\vec{X}$ if with an arrow) over $\mathcal{X}$, and the corresponding
lower-case letter $x$ (resp. $\vec{x}$ for vectors) to denote realizations of
$X$ (resp. $\vec{X}$). The cardinality of a set $\mathcal{X}$ is denoted by $\lvert\mathcal{X}\rvert$. Matrices will be denoted with bold capital letters. The
$i$-th entry of a vector $\vec{x}$ is denoted by $\vec{x}[i]$, while the transposed of a vector $\vec{x}$ is denoted as $\vec{x}^\intercal$. In general the $i$th observation of a random vector $X$ will be denoted by $x_i$. Observations will sometimes be grouped into datasets $\setData=\{x_1,\dots, x_N\}$. Throughout this thesis, a finite set $\sensVarSet = \{z^1, \dots, z^\nbClasses\}$ will be often considered: it will always denote the possible values for a \emph{sensitive variable} $\sensRandVar$ (see later). Its elements are ordered by overwritten indeces $z^1, z^2,\dots$ and are sometimes encoded via a so-called \emph{one-hot-encoding}: to each element $z^j$ a $\numClasses$-dimensional vector  $\vec{\sensVar}^j$ is associated,
with all entries equal to $0$ and the $j$-th entry equal to $1$: $\sensVar^j
\rightarrow \vec{\sensVar}^j = (0,\ldots , 0,\underbrace{1}_{j},0,\dots,0)$.

\paragraph*{Probability Notations.}
The probability of a random variable $X$ taking value in a subset $\mathcal{U}\in\mathcal{X}$ is denoted by $\prob(X\in\mathcal{U})$, or simply by $\prob{\mathcal{U}}$ if not ambiguous When $\mathcal{U}$ is reduced to a singleton $\mathcal{U}=x$ the same probability is denoted by $\prob(X=x)$ or simply by $\prob{x}$ if not ambiguous. If $X$ is a discrete variable $\pdf_X$ denotes its probability mass function (pmf for short), such that $\prob(X\in\mathcal{U})=\sum_{x\in \mathcal{U}}\pdf_X(x)$. The same symbol $\pdf_X$ is used to denote the probability density function (pdf for short) if $X$ is a continuous variable, such that $\prob(X\in \mathcal{U}) = \int_{\mathcal{U}}\pdf_X(x)dx$, for $[a,b]\subset\mathcal{X}$. 
The symbol $\esper[f(X)]$, or equivalently $\esper_X[f(X)]$, denotes the expected value of a function $f$ of the random value $X$, under the distribution of $X$. In the same way, symbols $\var(X)$ and $\var_X(X)$ denote the variance of $X$.

When two random variables $X$ and $Y$ are considered, their joint probability is denoted by $\prob(X=x,Y=x)$, or simply by $\prob(x,y)$ if not ambiguous, and their joint probability density (or mass) function is denoted by $\pdf_{X,Y}(x,y)$. The conditional probability of $X$ assuming the value $x$ given an outcome $y$ for $Y$ is denoted by $\prob (\given{X=x}{Y=y})$, or simply by $\prob(\given{x}{y})$ if not ambiguous. The conditional probability density (or mass) function of $X$ given an outcome $y$ for $Y$ is denoted by $\pdf_{\given{X} {Y=y}}(x)$. Finally, the covariance of the two variables is denoted by $\cov(X,Y)$.\\


\paragraph*{Bayes' Theorem.}
We recall some basic probability rules. For every $x\in\mathcal{X}$ and for every $y\in\mathcal{Y}$ we have what follows:
\begin{itemize}
\item \emph{Symmetry of joint probabilities: } $\pdf_{X,Y}(x,y)=\pdf_{Y,X}(y,x)$;
\item \emph{Marginal probabilities from joint ones: } $\pdf_X(x) = \sum_{Y=y}\pdf_{X,Y}(x,y)$ (where the sum has to be intended as an integral if $Y$ is a continuous variable);
\item \emph{Product rule: } $\pdf_{X,Y}(x,y) = \pdf_{\given{Y}{X=x}}(y)\pdf_{X}(x)$;
\end{itemize} 
These rules are sufficient to demonstrate, in the case of discrete random variables $X,Y$, a key stone of probability theory, the Bayes' theorem: 
\begin{equation}
\pdf_{\given{X}{Y=y}}(x) = \frac{\pdf_{\given{Y}{X=x}}\pdf_{X}(x)}{\pdf_{Y}(y)} \mbox{ ;}
\end{equation}
the marginal probability function $\pdf_X$ is referred to as \emph{prior} probability of $X$, and describes the distribution of $X$ without taking into account the variable $Y$. The conditional probability $\pdf_{\given{X}{Y=y}}$ is referred to as \emph{posterior} probability of $X$, and gives the distribution of $X$ once the outcome $y$ of $Y$ is taken into account. Notions of measure's theory are needed to show that Bayes' theorem is valid and keeps unchanged in case of continuous random variables and in cases in which one of the two involved variables is discrete and the other one is continuous. The interested reader might refer to \cite{feller2008introduction}. 

\paragraph*{The Gaussian distribution.}
The Gaussian or normal distribution is a widely used model for the distribution of continuous variables. We use the symbol $X\sim\mathcal{N}(\mu,\sigma^2)$ to denote a random variable $X$ that follows a Gaussian distribution with parameters $\mu\in\mathbb{R}$ and $\sigma^2\in\mathbb{R}^{+}$. For a $D$-dimensional random vector $\vec{X}$ we use the symbol $X\sim\mathcal{N}(\mumumu,\Sigma)$ to denote a vector that follows a multivariate Gaussian distribution with parameter $\mumumu\in\mathbb{R}^D$ and $\Sigma\in\mathbb{R}^{D\times D}$ positive-definite. The density of the Gaussian distribution is completely determined by the value of its two parameters. It is given by the following expressions, in unidimensional and multidimensional case: 
\begin{equation}
\pdf_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}\mbox{ ,}
\end{equation}
\begin{equation}
\pdf_{\vec{X}}(\vec{x}) = \frac{1}{\sqrt{2\pi\det(\Sigma)}}\exp{-\frac{1}{2}\left(\vec{x}-\mumumu\right)^\intercal \Sigma^ {-1}(\vec{x}-\mumumu)} \mbox{ .}
\end{equation}
The expected value of a Gaussian distribution coincides with the parameter $\mu$ for the univariate case and with $\mumumu$ for the multivariate one. The parameter $\sigma^2$ coincides with the variance of the univariate distribution, while $\Sigma$ coincides with the covariance matrix of the multivariate one.

\paragraph*{Basics of Statistics.}
The word \emph{statistics}  refers to a branch of mathematics that aims to analyse, describe or interpret observed data. Differently, the word \emph{statistic} refers to any measure obtained applying a function to some observed data. Let $\setData = \{x_1,\dots,x_N\}$ be a dataset of observations of a random variable $X$. We might distinguish two sub-branches in statistics: the descriptive statistics, and the inferential statistics. In descriptive statistics data are described by means of more or less complex statistics (in the sense of measure), the most common of them being the \emph{arithmetic mean}: 
\begin{equation}
\overline{x} = \frac{1}{N}\sum_{i=1}^N x_i \mbox{ .}
\end{equation}

In inferential statistics data are considered as sample observation of random variables and the data analysis aims at modelling the distribution of such variables. Dealing with random variables, inferential statistics exploit the probability theory framework and theorems. Statistics of data (in the sense of measures) play an important role in inferential statistics as well, usually aiming to estimate some random variable parameters. In this case they are called \emph{estimators} and will be denoted by a hat: for example, $\esperEst[X]$  denotes an estimator for the expected value of $X$ and $\varEst(X)$ denotes an estimator for the variance of $X$. The most classical estimator for the expected value is the arithmetic mean $\overline{x}$. It has several valuable properties, for example it is \emph{unbiased}, in the sense that, considering it as a random variable, its expected value coincides with the true value of $\esper[X]$. Moreover, it is the \emph{maximum-likelihood} estimator under the Gaussian distribution: for data that are independent among each other and drawn from a Gaussian distribution,  the arithmetic mean of the observed data $\setData$ is the value for the parameter $\mu$ that maximises the probability of observing the data $\setData$. A common unbiased estimator for the variance is the following so-called sample variance: 
\begin{equation}
\varEst = \frac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x})^2 \mbox{ ;}
\end{equation}
when the observed random variable  follows a Gaussian distribution, such an estimator differs from the maximum-likelihood one, in which the factor $\frac{1}{N-1}$ is substituted by $\frac{1}{N}$. In the same way, acting with Gaussian random vectors, the maximum-likelihood estimator of the covariance matrix is biased, and differs from the common unbiased one for a multiplicative factor. \\

Various approaches exist to make statistical inference. The two main ones are the frequentist approach and the Bayesian one. The frequentist inference is an approach that draws conclusions exclusively from sample data. It makes use of methodologies like the statistical hypothesis testing and the confidence interval, and in this thesis it is sometimes referred to as classical statistics. In the frequentist approach, parameters that define the distribution of the analysed random variable are priorly considered as fixed and unknown, and are estimated or tested on the sole basis of the observation of the sample data $\setData$. A second approach is the Bayesian inference, for which parameters that describe the analysed random variable are admitted to be probabilistic: in Bayesian inference, before the observation of sample data, the parameters have a prior distribution that reflects the knowledge and belief of the data-scientist about them. The observation of data leads to an update procedure, based on the Bayes' theorem, that allows such probability distribution of parameters to become more and more appropriate, each time exploiting the new available information. For both approaches, the maximum-likelihood is an optimal statistical principle and is widely exploited to choose parameters, in the frequentist approach, or to update parameters' probability distributions in the Bayesian one. Up-to-now the Bayesian inference has never explicitly influenced the Side-Channel Attack literature, nor we will use such a framework in this thesis. We leave this track opened for future works, briefly discussing its suitability for Side-Channel Attacks domain in Chapter~\ref{ChapterConclusions}.


\section{Side-Channel Attacks}

Side-Channel Attacks (SCA) belong to the cryptanalysis domain, since they aim to breach cryptographic security systems. Usually their goal is to retrieve a secret variable of a cryptographic algorithm, typically a secret key. They distinguish from classic mathematical cryptanalysis techniques by the fact that they are based on information gained from the physical implementation of a cryptosystem, rather than theoretical weaknesses in the algorithms. 

\subsection{Attacks general strategy}
\subsubsection{Grey-box and divide-and-conquer}
Side-Channel Attacks go beyond the cryptographic complexity that assures resistance against classical cryptanalysis strategies.  Indeed, no matter the size of the secret variables manipulated by the algorithm and the algebraic complexity of the encrypting/decrypting operations, an hardware implementation of any algorithm always handles variables of a bounded size, that depends on the hardware architecture. For example, in an 8-bit architecture an AES with 128-bit-sized key will be necessarily implemented as multiple partial computations over 8-bit blocks of data. In classical cryptanalysis, the typical attacker model faces to a black-box that performs the cryptographic algorithm: an attacker may make queries to the black-box, asking for ciphertexts of given plaintexts or viceversa, but has no information about any partial computations. On the contrary, a side-channel attacker is said to face to a grey-box model: he has a way to obtain noised information about partial computations. This allows him to perform a \emph{divide-and-conquer} strategy: if his goal is to retrieve the full 128-bit AES key, he will smartly divide his problem into the recovery of small parts of such keys at time, called \emph{key chunks},\footnote{or \emph{subkeys} when they coincide to a byte of key for the AES algorithm} making the complexity of the attack significantly drop. 

%OLD WITH RSA EXAMPLE For example, in an 8-bit architecture an RSA with 1024-bit-sized key, modulus and plaintext will be somehow implemented as multiple partial computations over 8-bit blocks of observable data. In classical cryptanalysis an attacker has no information about such partial computations. 

%\begin{remark}
%The core of an RSA implementation is the computation of a modular exponentiation with base, modulus and exponent being some big numbers (\eg 1024 bits). A myriad of different algorithms has been proposed to implement in hardware the RSA, which differ from each other for many factors: the representation data (\eg Montgomery representation, binary representations with signed bits, Chinese Reminder Theorem representation \dots), the way of scanning the exponent (\eg scan the exponent bit by bit, or window by window, from right to left or left to right, priorly decomposing the exponent or not, \dots), the way of performing in hardware the multiplication (\eg schoolbook, Montgomery, Barrett, \dots). Each choice impacts the size of the needed hardware and the time of the computation. A quite exhaustive state-of-the-art of such implementations is provided in \cite{koc1994high}. 
%% aggiungi citazione di Sliding Right into Disaster: Left-to-Right Sliding Windows Leak per dire che ancora oggi la questione della resistenza delle implementazioni agli SCA Ã¨ soggetto di ricerche
%\end{remark}

\subsubsection{Sensitive Variable}\label{sec:sensVar}
The noised information an attacker accesses, comes in form of side-channels signals, \eg instantaneous power consumption or electromagnetic irradiation, that he acquires during the execution of the algorithm. Such signals are collected into vectors called \emph{traces} (or \emph{acquisitions}). They will be denoted by $\vLeakVec_i$ and considered as observations of a random real vector $\vaLeakVec$, where each coordinate corresponds to a time sample of the acquired signal. The goal of the side-channel analysis is to clear traces from noise, in such a way to determine with the highest possible precision the association between a trace (or a set of traces) and the value taken by a target \emph{sensitive variable} $\sensRandVar$ during its (their) acquisition. A sensitive variable is a quantity handled during the processing that tells something about a secret of the implementation. Actually, it would be better to call it \emph{sensitive target}, since it might not be variable. Some typical examples of sensitive variables include: 



\begin{itemize}
\item $\sensRandVar = \keyRandVar$ with $\keyRandVar$ a secret subkey - this is the most direct choice for a sensitive target, nevertheless it is often not variable, since in some cases a device always manipulates the same key for a given embedded primitive. When the target is not variable we are performing a \emph{simple attack} (see below);
\item a cryptographic variable that depends on a sufficiently small subkey and a part of a known input variable $\publicParRandVar$: $\sensRandVar = \sensFunction(\keyRandVar,\publicParRandVar)$ - this is the most classical choice to perform a so-called \emph{differential} or \emph{advanced} SCA (see below);
\item any function of a cryptographic variable (ex: $\HW(\sensFunction(\keyRandVar,\publicParRandVar))$), where $\HW(\cdot)$ represents the Hamming weight operation, \ie the operation that counts the number of 1's in the binary string representing the entry. Sometimes, as for example we will see in Chapter~\ref{ChapterKernel} (see Sec.~\ref{sec:multiclass}) it can be interesting not to target a variable but a non-injective function of a variable as the Hamming weight; when the identity function is applied we are in the previous case;
\item an operation (ex: $\sensRandVar \in \{\mathrm{square}, \mathrm{multiply}\}$)
\item a register (ex: $\sensRandVar$ is the register used to store results of intermediate operations in a Montgomery ladder implementation of RSA)
\end{itemize}
In this thesis we will try as much as possible to abstract from the form of the sensitive variable, thinking of any entity $\sensRandVar$ that assumes values in a finite set $\sensVarSet$ and whose value permits an attacker to make inference on a secret of the implemented algorithm. Anyway, the choice of the sensitive variable is crucial for the successfulness of an attack. Interestingly, many notions and a wide and still-evolving taxonomy of  SCAs' typologies is in facts related to the choice of the sensitive variables. Some of these notions are reported in next three paragraphs by way of example.


\paragraph*{Vertical vs Horizontal SCAs.}
Attacks in which sensitive information
is extracted from a single acquisition split into several parts are called \emph{horizontal}. Horizontal attacks may apply when several internal variables depend on the same sensitive variable (see for instance \cite{battistello2016horizontal}). Algebraic SCAs (see below) are horizontal attacks as well, in which inferences are not done over a single sensitive variable, but potentially over every cryptographic variable, followed by a deep analysis of the algebraic relations between these sensitive variables. Horizontal attacks differ from the so-called \emph{vertical} attacks where information is obtained from different algorithm executions. The nice notion of rectangle attack has been introduced in \cite{bauer2013horizontal} to refer to attacks that exploit both vertical and horizontal leakages.


\paragraph*{Simple vs Advanced SCAs.}
A simple attack is an attack that only needs one trace to be applied,  except for eventually profiling acquisitions (see profiling attacks later). Such a one-trace attack can be seen as a classification problem in machine learning language: an attacker guesses the value of the secret key from the observation of a single side-channel trace;  in other words, setting $\sensRandVar$ equal to the definitive secret the attacker is looking for (\eg the whole secret key), the attack consists in classifying one observation, \ie assigning to the single attack trace the corresponding value of $\sensRandVar$. In 2002 Mangard \etal  \cite{mangard2002simple} proposed for example a simple power analysis (SPA) strategy, improved by Clavier \etal in 2014 \cite{clavier2014simple}, to retrieve the whole AES key observing leakages from a single execution of the AES key expansion. When Algebraic SCAs (see below) appeared in literature in 2009, their aim was to be a strategy to perform simple attacks as well.  

\begin{remark}
Attacks for which many observations are acquired with fixed entry parameters and by consequence in which the observed leakage always corresponds to a fixed $\sensRandVar$ are still considered as simple attacks. The attacker may exploit the several acquisitions in mainly two ways: he computes their average before performing the classification, aiming to reduce the noise influence, or he performs the classification of each acquisition (expecting each gives the same outcome) and then applies a function to the several outcomes (\eg majority vote or computation of joint probabilities, if outcomes are probability distributions, see \ref{eq:joint_distr}) to guess the right label. We observe that this approach with several observations allows the attacker to reduce the noise impact, while observing many times the same variable. The relation between the variable and the key chunk being fixed, he will not exploit algebraic relations to ameliorate his inference over the latter.
\end{remark}
An advanced attack is a more powerful strategy: the attacker acquires several acquisitions making entry parameters vary, and by consequence observing leakages related to different values of $\sensRandVar$. This time the variation of the observed sensitive variable is interpretable as a raising of the amount of caught information. The attacker exploits synergistically the information coming from each acquisition: he evaluates each key hypothesis, taking advantage of the algebraic relation between (known) entries, keys and sensitive variables, to find out the one that would better justify the leakages he observed. Classical Differential Power Attacks (DPA) \cite{brier2004correlation} or Correlation Power Attacks (CPA) \cite{brier2004correlation}, as well as attacks based over Mutual Information Analysis (MIA) \cite{batina2011mutual} are advanced attacks. 

\paragraph*{Algebraic Side-Channel Attacks.}\label{sec:ASCA}
Algebraic SCAs (ASCA) were firstly proposed in 2009 \cite{ASCA,renauld2009algebraic}. Their aim was to combine profiling SCA strategies (see \ref{sec:profilingSCA}) to classical cryptanalysis techniques: in the latter seminal papers block ciphers implementations are attacked, and the authors added to plaintext and ciphertext knowledge, classically assumed in algebraic cryptanalysis, the access to the (exact) Hamming weights of  several intermediate computations, obtained by side-channel observation. Once recovered as many partial information as possible, it is expressed in the form of an equation system, then converted to a set of clauses treatable by a SAT solver. In opposition to classic SCAs, ASCA approach does not exploit a divide-and-conquer strategy: the whole secret key is retrieved at once, exploiting algebraic relations between intermediate variables. This means, in the case of block ciphers, that the attacker observes leakages occurring during each round of the algorithm, and not only those related to the first rounds, were the cryptographic diffusion is limited. This optimal exploitation of the information, allows the ASCA strategy to provide efficient simple attacks, \ie succeeding with a single attack trace.\\
This approach presents two main weaknesses. First, it does not tolerate errors, implying that it is weak to noise: a wrong side-channel information may put the right key out of the list of key candidates. This is why seminal ASCA papers proposed to equip the strategy with some techniques of detection of impossibilities and likelihood rating. Second, the use of SAT solvers asks to express relations between cipher variables at a bit level. Since general block ciphers are byte-oriented this produces very large and complex instances, challenging to construct and hard to debug.\\
Two works appeared in 2014 address these two weaknesses. In \cite{soft} a \emph{Soft Analytical Side Channel Attack} (SASCA) is proposed to make the ASCA strategy more tolerant to noise. The idea of such SASCAs is to replace the equation system representation of retrieved intermediate variables with a code, inspired by the low density parity check codes. This approach is still bit-oriented. Then the code is efficiently decoded by an algorithm known as \emph{Belief Propagation}, whose inputs are not the exact values of the retrieved intermediate variables, but their probability distributions, provided by the profiling phase. The noise tolerance is provided by the utilisation of such probabilities instead of exact values. \\
In \cite{Oren2014} a constraint solver is proposed to replace the bit-oriented SAT solver. Such a new solver is designed for side channel cryptanalysis of byte-oriented ciphers, and works in a probabilistic way, as well as the SASCA approach, tracking the likelihoods of values in the secret key. The likelihoods of observed intermediate variables are provided by a template approach, as well as proposed in \cite{Oren:2013} in 2013 under the name of Template-Algebraic SCA (TASCA). The main tool  of the constrained solver is the \emph{conflation operator} for reconciling multiple probability distributions for the same variable.\\

Since all these algebraic side-channel approaches are based over a preliminary profiling phase, they are all largely concerned by the dimensionality reduction issue, which is often not explicitly taken into account in the literature about this research axe. 

\subsubsection{Leakage Models}\label{sec:leakage_model}
%sistema, togli il modello gaussiano, semmai cita il modello con il bias e cita duc2014unifying
The underlying hypothesis of a SCA is that some information about  a sensitive variable of the implemented algorithm leaks during its execution through some observable \emph{side} channels. Such leakages are collected 
in the form of signal traces. Depending on the observed channel (\emph{e.g. power consumption, electromagnetic irradiation, time, \dots}), different properties might influence the form of the leakage, and should be taken into account for the construction of a leakage model.\\

If we allow a Side-Channel attacker to make use of a probing station to directly access the circuit wires and monitor the exact values of some intermediate values, this attacker will observe leakages following the so-called \emph{probing model}. To define this model no further hypothesis is needed, for example no noise is taken into account. As explained in \ref{sec:classification_attacks}, SCAs typically refer to non-invasive attacks, so in Side-Channel literature the probing model has been introduced as a worst-case abstract model, and is mainly considered in order to provide formal security proofs for some kinds of countermeasures. More precisely the $d$-probing model \cite{ishai2003private}, in which an attacker can probe $d$ different wires at a time, provides a good model to exhibit security proofs for $d$th-order masking schemes (cf. \ref{sec:masking}). \\
The most common passive leaking channel considered in literature is the power consumption. For such a physical quantity many efforts have been done to propose adherent leakage models. A detailed modelling for power consumption of CMOS circuits is proposed in the \emph{DPA book} \cite{mangard2008power}. After a description of the physical factors influencing the power consumption (divided into static and dynamic) of single logic cells, the authors propose to assume two different points of view to model and develop simulations of the power consumption: the designer point of view can bring to a quite accurate and detailed model, essentially based over his circuit transistor netlists. On the contrary an attacker would be satisfied by considering some easier model, often based over the \emph{Hamming distance} ($\HD$) or the \emph{Hamming-Weight} ($\HW$) of internal variables. Indeed these two functions well-fit the consumption behaviour of circuits registers and buses, which consume depending on how many bits set to 1 or 0 they store or transport (Hamming weight) or how many of them switch their value for 0 to 1 or vice-versa during computations (Hamming distance). \\
When an attacker has chosen its sensitive target $\sensRandVar$ and deals with concrete acquisitions, he does not need a complete power model, but only  a way to model the relative differences between leakages for different values of $\sensRandVar$, in order to distinguish traces related to different values of $\sensRandVar$, or in other terms, closer to the machine learning language, to classify traces depending on their associated value of $\sensRandVar$. A statistical model is then sufficient to him. Thus for an attacker, the wider considered model in Side-Channel community is the one sometimes called \emph{noisy leakage model}, firstly introduced in \cite{chari1999towards}, that considers that the physical observable is a deterministic function of the sensitive variable, corrupted by some noise. Some efforts to better specify the form of the noise in such a model have been done in literature, often with the aim of providing security formal proofs for side-channel countermeasures. This is the case for the seminal publication \cite{chari1999towards}, where noise is assumed following a Gaussian distribution, and is thus quantified by its standard deviation, and for the model generalization proposed in \cite{prouff2013masking} where noise is quantified as a statistical distance, called \emph{bias}, between the distribution of $\sensRandVar$ and the conditional distribution of $\sensRandVar$ given $\vaLeakVec$. In this thesis we do not need to consider a formal description of the noise, nor a precise concept to quantify it. Despite the fact that some of the proposed techniques present optimality features in presence of Gaussian hypothesis, we will not endorse the Gaussian model. To us, side-channel traces are informative, in the sense that the conditional probability densities of $\vaLeakVec$ given $\sensRandVar$ are different for a least two different values assumed by $\sensRandVar$. We assume in particular that such difference is probability density is observable in their first-order moment, \ie the expected value. The role of the attacker is to understand how to optimally take advantage of these statistical differences, minimizing the number of observations he needs to succeed, and maximizing the efficiency of the attack, in the sense formalized in next section. 


\subsection{Efficiency of the SCAs}\label{sec:metrics}
In order to measure the efficiency of a side-channel attack, different security metrics have been proposed, the most exploited being the \emph{success rate of order} $\orderRate$ ($\SR_\orderRate$) and the \emph{guessing entropy} (GE). Referring to the formalization proposed by \cite{unifiedFramework}, a key recovery side-channel attack outputs a vector of key candidates,\footnote{In this thesis we will always target a key chunk and we will use such metrics to evaluate the efficiency of an attack in recovering such key chunks. When a full-key recovery attack is run, some algorithms to merge key chunks' outcomes and obtain the full key enumeration and a complete key rank estimation are deployed. This domain is out the scope of this thesis.} called {\em guessing vector} $\guessingVector = \left[\guessingVector[1], \dots, \guessingVector[{\lvert \keyVarSet \rvert}]\right]$, in which such candidates are sorted in decreasing order with respect to their likelihood after the attack phase. Being $\keyVar^\star$ the right candidate, its \emph{rank} is given by:
\begin{equation}
\mathrm{Rank}(\keyVar^\star) = i \mbox{ such that } \guessingVector[i] = \keyVar^\star \mbox{.}
\end{equation}

Then, the success rate of order $\orderRate$ of an attack is given by the probability for the right key candidate to be ranked among the first $\orderRate$ candidates: 

\begin{equation}
\SR_\orderRate = \prob[\mathrm{Rank}(\keyVar^\star)\leq \orderRate] \mbox{ .}
\end{equation}

The success rate of an attack is usually estimated empirically: the attack is repeated a large number of times, and the empirical $\SR_\orderRate$ is given by the ratio between the number of successes (attacks for which the right key is ranked among the first $\orderRate$ ones) and the total number of attacks. \\

The guessing entropy \cite{massey1994guessing} is defined as the expected rank of the right key: 
\begin{equation}
\guessingEntropy = \esper[\mathrm{Rank}(\keyVar^\star)]\mbox{ .}
\end{equation}

This is also generally estimated in an empirical way, by performing the attack many times independently, then computing the average of the obtained ranks. 

%  OLD VERSION OF paragraph LEAKAGE MODELS
%Thus for an attacker, the wider considered model in Side-Channel community is the one sometimes called \emph{noisy leakage model}. firstly introduced in \cite{chari1999towards}. In this model the leakage is a random variable obtained by the sum of a deterministic function $\leakFunction$ of the sensitive variable $\sensRandVar$ and a random noise. In general the noise has a Gaussian distribution of null mean and variance $\sigma^2$:
%\begin{equation}\label{eq:noise_model}
%\vaLeak = \leakFunction(\sensRandVar) + \noise \mbox{ ,}
%\end{equation}
%with $\noise \sim \mathcal{N}(0,\sigma^2)$.
%\begin{remark}
%In this model the leakage is a real value $\vaLeak$. In general leakages are acquired as traces, 
%represented by column vectors $\vaLeakVec\in\mathbb{R}^\traceLength$ and model \eqref{eq:noise_model} refers to a single well-chosen \emph{point of interest} (see later for a definition).
%\end{remark}


\subsection{Profiling Side-Channel Attacks}\label{sec:profilingSCA}
As anticipated in Sec.~\ref{sec:this_thesis_objectives}, when an open sample of the attacked device is available to make a prior characterisation of the leaking signals of a device, we talk about \emph{profiling} attacks. When this is not the case, we talk about \emph{non-profiling} attacks. \\
A profiling attack is divided into two distinct phases. The first one, called \emph{profiling phase} or \emph{characterisation} phase exploits so-called \emph{profiling traces} to build a model of the leakages. Profiling traces are acquisitions taken under known values for the sensitive variable $\sensRandVar$, so are couples $(\vLeakVec_i, \sensVar_i)_{i=1, \dots , \nbProfilingTraces}$ for which the correct association trace/sensitive variable is known. The second phase of a profiling attack is the proper \emph{attack phase}, during which the attacker observes a new set of acquisitions, under unknown secret key, and takes advantage of the previous characterisation to infer over it. Throughout this thesis, and each time a profiling attack scenario is supposed,  we will refer to elements of $\sensVarSet$ as \emph{labels}, each one identifying a \emph{class} of traces. We will say that acquired traces associated to a same value $\sensVar\in\sensVarSet$ \emph{belong} to the same class, identified by the label $\sensVar$. We will say as well that such traces are  \emph{labelled} by the value $\sensVar$. By abuse we will also refer to the class $\sensVar$ to denote the class of traces labelled by $\sensVar$. In such a context $\nbTracesPerClass$ will denote the number of profiling traces belonging to the class $\sensVar$.



As we will see in Chapter~\ref{ChapterIntroML}, in machine learning domain the analogous of profiling attacks context is studied under the name of \emph{supervised machine learning}. In supervised machine learning, couples $(\vLeakVec_i, \sensVar_i)_{i=1, \dots , \nbProfilingTraces}$ are available and are called \emph{training examples}. The profiling phase is referred to as \emph{training} or \emph{learning} and the attack phase is assimilable to the so-called \emph{test phase}. The main difference between a machine learning test phase and a side-channel attack phase is that in the former one the examples are processed independently from each other, while in the latter the examples have something in common (typically a fixed secret key) and are used synergetically to guess it. If no example is available we talk about \emph{unsupervised machine learning}, that we can consider analogous to the non-profiling SCAs branch. 




\subsubsection{Template Attack}\label{sec:TA}
Introduced in 2002 by Chari \cite{Chari2003}, the so-called \emph{Template Attack} (TA) is the most well-established strategy to run a profiling SCA. It can be performed in a simple or advanced way. The idea of the TA is based over the construction of a so-called \emph{generative model}: in probability, statistics and machine learning \enquote{ ...approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.} \cite{christopher2006pattern}.
In TA the attacker observes the couples $(\vLeakVec_i, \sensVar_i)_{i=1, \dots , \nbProfilingTraces}$  and exploits them to estimate the class-conditional densities  
\begin{equation}\label{eq:class-conditional}
\pdf_{\given{\vaLeakVec}{\sensRandVar = \sensVar}}(\vLeakVec)\mbox{ ,}
\end{equation}
eventually the prior densities $p_{\vaLeakVec}(\vLeakVec)$, $p_{\sensRandVar}(\sensVar)$, and finally the \textit{a-posteriori} density, by means of Bayes' theorem:
\begin{equation}\label{eq:a-posteriori}
\pdf_{\given{\sensRandVar}{  \vaLeakVec = \vLeakVec}}(\sensVar) = \frac{\pdf_{\given{\vaLeakVec}{\sensRandVar = \sensVar}}(\vLeakVec)\pdf_{\sensRandVar}(\sensVar)} {\pdf_{\vaLeakVec}(\vLeakVec)}\mbox{ .}
\end{equation}
In the attack phase the attacker acquires new traces that he only can associate to the public parameter $\publicParRandVar$, obtaining couples  $(\vLeakVec_i, \publicParVar_i)_{i=1, \dots , \nbAttackTraces}$. Then he makes key hypothesis $\keyVar \in \keyVarSet$ and, making the assumption that each acquisition is an independent observation of $\vaLeakVec$, he associates to each hypothesis $\keyVar \in \keyVarSet$ a score $d_\keyVar$ given by the joint \textit{a-posteriori} probability that follows, and that he computes exploiting estimates \eqref{eq:a-posteriori}:

\begin{equation}\label{eq:joint_distr}
d_{\keyVar} = \prod_{i=1}^{\nbAttackTraces} \pdf_{\given{\sensRandVar}{\vaLeakVec = \vLeakVec_i}}(\sensFunction(\keyVar,\publicParVar_i) ) \mbox{ .}
\end{equation}

Finally, his best key candidate $\hat{\keyVar}$ is the one maximizing such a joint probability
\begin{equation}\label{eq:max_classifier}
\hat{\keyVar} = \argmax_{\keyVar} d_{\keyVar} \mbox{ .}
\end{equation}

\begin{remark}Since the marginal probability density $p_{\vaLeakVec}(\vLeakVec_i)$ of \eqref{eq:a-posteriori} does not depend on key hypothesis, it is usually neglected. Moreover, in many cases the variable $\sensRandVar$ follows a uniform distribution, so its probability mass function $p_{\sensRandVar}(\sensVar)$ appearing in \eqref{eq:a-posteriori}  does not influence the ranking of key hypothesis. It is often neglected as well. 
\end{remark}

\begin{remark}
In the special case of a simple attack, \ie $\nbAttackTraces = 1$, in which $\sensRandVar = \keyRandVar$, the problem becomes a classical machine learning classification problem (as we will discuss over in Chapter~\ref{ChapterIntroML}): the attacker wants to classify the unique attack trace, \ie assign to it a class label (the key). In such a case, the choice proposed by \eqref{eq:max_classifier} is known as \emph{Bayes (optimal) classifier}.\footnote{The term \emph{optimal} distinguishes it from the so-called \emph{Bayes naive classifier}, which introduces an independence assumption between data vector coordinates. The efficiency of a Bayes naive classifier has been analysed in SCA context in 2017 \cite{picek2017template}.} It is proven to be the optimal choice to reduce the misclassification error \cite{christopher2006pattern}.
\end{remark}

This approach has the theoretical optimality that comes from the maximum likelihood criterion. The crucial point is the estimation of the class-conditional densities \eqref{eq:class-conditional}: the efficiency of the attack strongly depends on the quality of such estimates. 

\paragraph*{The Gaussian Hypothesis.} A well-established choice to construct class-conditional densities estimations \ref{eq:class-conditional} is the one applied in Gaussian TA \cite{Chari2003}: it consists in making a class-conditional multivariate Gaussian distribution assumption
\begin{equation}\label{eq:gaussian_assumption}
\given{\vaLeakVec}{\sensRandVar} =\sensVar \sim \mathcal{N}(\mumumu_\sensVar, \Sigma_\sensVar)\mbox{ ,}
\end{equation} 
and exploits the profiling traces to estimate the  parameters $\mumumu_\sensVar$, \ie the mean vector of the Gaussian distributions, and $ \Sigma_\sensVar$, \ie the covariance matrices. \\

\begin{remark}This assumption is the same that is done for classification problems, bringing to the \emph{Quadratic Discriminant Analysis} technique, which we will describe in Chapter~\ref{ChapterIntroML}. 
\end{remark}

Many options and choices influence the implementation of a TA: the suppression or not of the marginal densities in \eqref{eq:a-posteriori}, the use of the unbiased estimator or the maximum likelihood estimator for the covariance matrices, the addition of an \emph{homoscedasticity} assumption (assume that all class-covariance matrices are equal). This last assumption, proposed in 2014 in SCA literature \cite{choudary2014efficient},  allows exploiting all profiling traces to estimate a unique so-called \emph{pooled} covariance matrix, instead of using traces belonging to each class to estimate each covariance matrix separately. The pooled estimation gains in accuracy. 

\begin{remark}
The homoscedasticity assumption is the same that is done for classification problems, bringing to the \emph{Linear Discriminant Analysis} technique, which we will introduce in Chapter~\ref{ChapterIntroML} and more deeply analyse in Chapter~\ref{ChapterLinear}.  
\end{remark}

Other choices that mainly influence the TA efficiency are those related to the PoI selection, or more generically to the dimensionality reduction issue.

\subsection{Points of Interest and Dimensionality Reduction}\label{sec:extractors}
Side channel traces are usually acquired by oscilloscopes with a very high sampling rate, which permits a powerful inspection of the component behaviour, but at the same time produces huge-dimensional data, consisting in thousands, or even millions of points. Nevertheless, often only a relatively small part of these time samples is informative, i.e. statistically depends, independently or jointly, on a sensitive target variable. These informative points are called \emph{Points of Interest} (PoI). The dimensionality reduction of the traces is a fundamental pre-processing phase to get efficient and effective SCAs, not too expensive in terms of memory and time consumption. The problem of performing an opportune dimensionality reduction goes hand in hand with the research of PoIs: a convenient dimensionality reduction should enhance the contribution of such PoIs while reducing or nullifying the one provided by non-interesting points. 
The goal of researches in this context is to study and develop techniques to characterise PoIs and to apply convenient dimensionality reduction techniques, that allow reducing the size of the acquisitions while keeping the exploitable information held by data high enough to allow an SCA to succeed.
Representing the side channel traces as column vectors ${\bf x}$ in $\mathbb{R}^\traceLength$, the compressing phase might be seen as the application of a function $\extract\colon \mathbb{R}^\traceLength\rightarrow \mathbb{R}^\newTraceLength$, with $\newTraceLength<\traceLength$, called {\em extractor} throughout this thesis. The first extractors proposed in SCA literature where actually some selectors of time samples, \ie functions that operate a simple subsampling of the traces on the base of the computation of some sample-wise statistics $\sPOI(t)$, whose aim is to quantify a sort of  signal strength. Several proposals exist for such a signal-strength estimate, among them the most deployed are the Difference of Means (DOM) \cite{Chari2003}, or the analogous but better specified Sum of Differences (SOD) \cite{Rechberger2005}, the Sum of Squared Differences (SOSD) \cite{gierlichs2006templates}, the Signal-to-Noise Ratio (SNR) \cite{mangard2008power,lomne2013behind} and  Sum of Squared $t$-differences SOST, corresponding to the $t$-test \cite{gierlichs2006templates}. All these statistics are similar, and exploit the sample mean per class of the traces, given by
\begin{equation}
\mmmXclass= \esperEst[\given{\vaLeakVec}{\sensRandVar = \sensVar}] = \frac{1}{\nbTracesPerClass}\sum_{i\colon \sensVar_i=\sensVar} \vLeakVec_i  \mbox{ .}
\end{equation} 
A notable difference among them is that only the last two, SNR and SOST, take also the variances per class of the traces into account, given by
\begin{equation}
\varXclass = \varEst(\given{\vaLeakVec}{\sensRandVar = \sensVar}) = \frac{1}{\nbTracesPerClass-1}\sum_{i\colon \sensVar_i=\sensVar} (\vLeakVec_i - \mmmXclass)^2 \mbox{ .}
\end{equation}

The Table~\ref{table:PoIselectors} gives explicit formulas to compute such state-of-the-art sample-wise statistics. Once the chosen signal strength estimate $\sPOI$ is computed, it can be used as in a hypothesis test to reject the hypothesis that the sample mean values at time $t$ are equal. The instants $t$ in which such a hypothesis is rejected correspond to the PoIs, since the variation of the signals in such instants seems depend on the class belongingness. The construction of the subsampling $\extract$ is done on the base of such a test, for example selecting all time samples for which $\sPOI(t)$ is higher than a certain threshold. \\

As anticipated in Sec.~\ref{sec:dim_red_objective}, in this thesis we did not go deeper in the study of such sample-wise PoI selection methods, exploring directly other dimensionality reduction approaches. Anyway, throughout the thesis, we will often refer to the SNR statistic, as a good indicator of sample-wise information.

\begin{table}[]
\centering
\caption{Statistics proposed as signal strength estimate to operate a selection of time samples.}
\label{table:PoIselectors}
\begin{tabular}{|c|c|}
\toprule
Name of the estimate    &  Definition\\
\midrule
SOD& \parbox{6cm}{\begin{equation*} \sPOI(t) = \sum_{\substack{\sensVar_1, \sensVar_2 \in \sensVarSet \\ \sensVar_1 \neq \sensVar_2}} (\mmmXclass[\sensVar_1](t) - \mmmXclass[\sensVar_2](t)) \end{equation*}} \\
\hline
SOSD & \parbox{6cm}{\begin{equation*} \sPOI(t) = \sum_{\substack{\sensVar_1, \sensVar_2 \in \sensVarSet \\ \sensVar_1 \neq \sensVar_2}} (\mmmXclass[\sensVar_1](t) - \mmmXclass[\sensVar_2](t))^2 \end{equation*}} \\
\hline
SOST (version \cite{gierlichs2006templates}) & \parbox{6cm}{\begin{equation*} \sPOI(t) = \frac{\sum_{\substack{\sensVar_1, \sensVar_2 \in \sensVarSet \\ \sensVar_1 \neq \sensVar_2}} (\mmmXclass[\sensVar_1](t) - \mmmXclass[\sensVar_2](t))^2}{\frac{\varXclass[\sensVar_1]}{\nbTracesPerClass[\sensVar_1]}+\frac{\varXclass[\sensVar_2]}{\nbTracesPerClass[\sensVar_2]}}  \end{equation*}}\\
\hline
SOST (version \cite{bar2010improved}) & \parbox{6cm}{\begin{equation*} \sPOI(t) = \frac{\sum_{\substack{\sensVar_1, \sensVar_2 \in \sensVarSet \\ \sensVar_1 \neq \sensVar_2}} (\mmmXclass[\sensVar_1](t) - \mmmXclass[\sensVar_2](t))^2}{\varXclass[\sensVar_1]+\varXclass[\sensVar_2]}  \end{equation*}}\\
\hline
SNR & \parbox{6cm}{\begin{equation} \sPOI(t) = \frac{\varEst(\mmmXclass[Z](t))}{\esperEst[\varXclass[Z](t)]}  \end{equation}\label{eq:SNR_formula}}\\
\hline
\end{tabular}

\end{table}




%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Main Side-Channel Countermeasures}\label{sec:countermeasures}
To counteract SCAs, strategies that aim at making leakages independent from the processed sensitive data have to be implemented. We can distinguish two broad groups of such countermeasures: 
those that aim at hiding the data and those that are designed to mask the data. The two approaches may even be combined.

\subsection{Hiding}
The main characteristic of a hiding countermeasure is that is does not change the intermediate data values that are processed in the cryptographic algorithm, but it only attempts in hiding its processing. Hiding is typically, but not only,\footnote{Strategies to attempting making power consumption constant, such as the use of dual-rail precharge logic cells, also belong to the hiding group of countermeasures \cite{popp2005masked}.} achieved in by randomising the power consumption. A random power consumption can be obtained by randomly changing the time at which the targeted sensitive variable is processed. In this way the attacker acquires side-channel traces that are desynchronised or misaligned with respect to their interesting part. This temporal misalignment reduces the effectiveness of an attacker's statistical analysis. Possible ways for randomising the power consumption are the random insertion of dummy instructions \cite{coron2009efficient,coron2010analysis} and the shuffling of the operations \cite{veyrat2012shuffling}, at a software level, or the randomization of the instruction stream by means of non deterministic processors \cite{irwin2002instruction,may2001non}, or the enhancement of a jittering effect over the clock via an asynchronous logic style at a hardware level \cite{moore2002improving,moore2003balanced}. Such methods may also be combined. \\

Applying realigning preprocessing techniques, such as integration \cite{mangard2004hardware,mangard2008power}, pattern matching \cite{nagashima2007dpa} or more sophisticated signal-processing techniques \cite{van2011improving}, is the most common approach an attacker usually chooses to face up temporal misalignment. Defeating differently misalignment countermeasures is one of the main motivations that lead us to investigate Convolutional Neural Networks, as we will discuss in Chapter~\ref{ChapterCNN}.

\subsection{Masking}\label{sec:masking}
Masking countermeasures derive from the idea of applying secret-sharing methods to counteract side-channel attacks. Secret-sharing methods consist in strategies to distribute a secret message amongst a group of participants. Each participant receives a piece of information, called \emph{share} and the original message can only be reconstructed if a sufficient number of participants collaborate, putting in common the knowledge of a sufficient number of shares. The idea of applying secret-sharing to counteract SCAs was first proposed by Chari \etal \cite{chari1999towards} and Goubin and Patarin \cite{goubin1999and}. In this case the sensitive variables of the cryptographic algorithm are considered as secret messages to distribute. Since 1999, several masking schemes have been proposed, attacked and ameliorated to protect various cryptographic algorithm, for example \cite{messerges2000securing,akkar2001implementation,ishai2003private,blomer2004provably,oswald2005side,schramm2006higher,rivain2010provably,moradi2011pushing,coron2013higher,bilgin2014more,de2015higher,goudarzi2017fast,journault2017very}. When a masking scheme is properly implemented, it guarantees that every sensitive variable $\sensRandVar$ is randomly split  into multiple shares $M_1,M_2,\dots,M_d$ in such a way that a relation 
\begin{equation}\label{eq:masking}
\sensRandVar = M_1 \star \dots \star M_d
\end{equation}  holds for a group operation $\star$ (\emph{e.g.} the exclusive or for the most popular Boolean masking already proposed in the seminal papers \cite{chari1999towards,goubin1999and}). The soundness of the masking countermeasure is implied by the fact that, in the noisy leakage model, the complexity of recovering information by SCA on a bit shared into several pieces grows exponentially with the number $d$ of shares.\footnote{The exponential basis being proportional to the noise standard deviation.} This fact was enlighten by Chari \etal in 1999  \cite{chari1999towards}, then complemented by Prouff and Rivain in 2013 \cite{prouff2013masking}. As a consequence of such an exponential complexity behaviour, the number $d$ of shares plays the role of a security parameter for a masking scheme and the method is usually referred to as $(d-1)$th-order masking (since it involves $(d-1)$ random values, called \emph{masks} and one value determined by the sensitive variable and the relation \eqref{eq:masking}, which is sometimes referred to as \emph{masked variable}). The shares are manipulated by distant parts of the circuit (especially if the countermeasure is implemented at a hardware level) or at different times (especially for software implementations of the countermeasure). In this way an attacker, who is obliged to retrieve information coming from a sufficient number of shares to obtain some $\sensRandVar$-dependent information, has to acquire many portions of signal to combine. \\

Attacks against the masking countermeasure are known as \emph{Higher-Order} Side-Channel Attacks (HO-SCA), where the order usually refers to the number of independent information an attacker has to join to succeed. In general, to defeat a $(d-1)$th-order masking countermeasure, a $d$th-order attack has to be run. In the first literature about HO-SCA (for instance \cite{messerges2000using,Waddle2004,joye2005second,oswald2006practical} the order corresponded to the number of time samples of the signal the attacker combined to mount the attack, and the common idea was to compute some combining function of the $d$ time samples and compare the outcome with some key-dependant predictions. Among the proposed combining functions, the centred product of the $d$ points were showed to be the most efficient, at least under a Hamming Weight power consumption model \cite{DBLP:journals/tc/ProuffRB09}. Actually, and for example when the countermeasure is implemented in hardware and shares are manipulated in parallel, sometimes the number of time samples to combine differs from the number $d$ of shares \cite{peeters2005improved,standaert2005masking}. So the definition of $d$th-order SCA has mutated in time (see for instance a different formalization in \cite{piret2008security}). Today it is most-widely accepted to define a $d$th-order attack as an attack that looks for key-discriminant information in some $d$th-order statistical moment of the signal, while the number of time samples of the signals that participate to such a statistic defines the \emph{multivariability} of the attack \cite{gierlichs2010revisiting,batina2011mutual,carlet2014achieving}. For example a $2$nd-order attack against a parallel implementation may be univariate if a single time sample is used to derive key-dependent information. In general for attacks against software implementations, a $d$th-order attack is usually $d$-variate. In such a case the research of interesting $d$-tuples of time samples still raises the complexity of the attacks. Even in the favourable case in which a profiling attack is allowed, two cases must be distinguished: the attacker has or not access to the masks values during profiling. In the former case the attacker can use the shares as target sensitive variables during the profiling phase, looking for PoIs for each one of them. Thus, the PoI research complexity grows only linearly with the number $d$ of shares. In the latter case the attacker cannot infer independently over each share and classical tools for PoIs research are inefficient. This issue is the main motivation that leads us to consider solutions based over the Kernel Discriminant Analysis (KDA) tool, as we will discuss in Chapter~\ref{ChapterKernel}.

