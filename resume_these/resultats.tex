\section{R\'{e}sultats principaux}\label{sec:res}

\subsection{Notations}
Dans la th\`{e}se, le symbole $X$ d\'{e}signe une variable al\'{e}atoire ($\vec{X}$ pour un vecteur colonne al\'{e}atoire) sur un ensemble $\mathcal{X}$, et $x$ (respectivement $\vec{x}$) d\'{e}signe une r\'{e}alisation de $X$ (respectivement $\vec{X}$). 
La $i$-\`{e}me coordonn\'{e}e d'un vecteur $\vec{x}$ est indiqu\'{e}e par $\vec{x}[i]$, et la transpos\'{e}e d'un vecteur $\vec{x}$ par $\vec{x}^\intercal$. Les matrices sont indiqu\'{e}es par des majuscules en gras, $\textbf{A}$ ou $\covmat$. Les traces acquises des canaux auxiliaires sont interprêt\'{e}es comme r\'{e}alisations $\vLeakVec_1, \dots , \vLeakVec_N$ d'un vecteur al\'{e}atoire r\'{e}el $\vaLeakVec\in\mathbb{R}^\traceLength$, o\`{u} $\traceLength$ est la longueur du signal. Quand une m\'{e}thode de r\'{e}duction de dimension est utilis\'{e}e comme pr\'{e}-traitement, celle-ci am\`{e}ne \`{a} la d\'{e}finition d'une fonction appel\'{e}e \emph{extracteur} et d\'{e}not\'{e}e par $\extract \colon \mathbb{R}^\traceLength \rightarrow \mathbb{R}^\newTraceLength$. La variable sensible manipul\'{e}e pendant l'acquisition des traces est not\'{e}e $\sensRandVar$. Celle-ci peut avoir diff\'{e}rentes formes, mais souvent dans cette th\`{e}se elle est d\'{e}finie comme $\sensRandVar = \sensFunction(\keyRandVar,\publicParRandVar)$, o\`{u} $\publicParRandVar$ d\'{e}note une variable publique, par exemple une partie de message en clair, et $\keyRandVar$ une partie d'une cl\'{e} secr\`{e}te que l'attaquant souhaite retrouver. Les valeurs acquises par la variable sensible sont vues comme r\'{e}alisations de la variable al\'{e}atoire $\sensRandVar$ en $\sensVarSet = \{\sensVarValue{1}, \dots, \sensVarValue{\nbClasses}\}$. Les \'{e}l\'{e}ments de $\sensRandVar$ sont parfois encod\'{e}s via le \emph{one-hot-encoding}: \`{a} chaque \'{e}l\'{e}ment $\sensVarValue{j}$ on associe un vecteur $\sensVarOneHot{j}$ de dimension $\numClasses$, avec toutes les entr\'{e}es nulles, sauf la $j$-\`{e}me qui est \'{e}gale \`{a}  $1$: $\sensVarValue{j}
\rightarrow \sensVarOneHot{j} = (0,\ldots , 0,\underbrace{1}_{j},0,\dots,0)$. Un \'{e}l\'{e}ment g\'{e}n\'{e}rique de $\sensVarSet$ sera not\'{e} $\sensVarGenValue$, si son indice  $i$ n'est pas n\'{e}cessaire.

\subsection{Techniques Lin\'{e}aires de R\'{e}duction de Dimension}\label{sec:lin}
Dans cette section sont d\'{e}crites les \'{e}tudes men\'{e}es autour des m\'{e}thodes lin\'{e}aires d'extraction de caract\'{e}ristiques, en particulier de l'Analyse aux Composantes Principales (PCA) et de l'Analyse Discriminante Lin\'{e}aire (LDA). 

\subsubsection{Analyse aux Composantes Principales, l'outil classique et le profil\'{e}e}
L'extracteur lin\'{e}aire $\extract^{\mathrm{PCA}}(\vLeakVec) = \textbf{A}\vLeakVec$ se d\'{e}duit des certains vecteurs propres $\AAlpha_1, \dots, \AAlpha_\newTraceLength$, appel\'{e}s \emph{Composantes Principales} (PCs), dont les transpos\'{e}s sont arrang\'{e} en tant que lignes dans la matrice de projection $\textbf{A}$. Classiquement la PCA intervient sur des donn\'{e}es non labellis\'{e}es $\vLeakVec_1, \dots , \vLeakVec_N$, suppos\'{e} ayant moyenne nulle et arrang\'{e} comme colonnes dans une matrice  $\measuresMatrix$ de dimension $\traceLength \times \nbTraces$, de tel sort que la matrice de covariance des donn\'{e}es est la suivante:
\begin{equation}\label{eq:covmat}
\covmat = \frac{1}{\nbTraces}\measuresMatrix\measuresMatrix^\intercal \mbox{ .}
\end{equation}

Dans ce cas, les vecteur propres $\AAlpha_1, \dots, \AAlpha_\newTraceLength$ correspondent aux vecteurs propres de la matrice $\covmat$ et leurs valeurs propres associ\'{e}es sont not\'{e}es $\lambda_1, \dots, \lambda_r$. La PCA est la projection qui maximise la variance globale des caract\'{e}ristiques extraites. La variance \'{e}tant li\'{e}e \`{a} la quantit\'{e} d'information des donn\'{e}es, cette transformation est cens\'{e}e r\'{e}duire la dimension des traces tout en renforçant l'information contenue. Une propri\'{e}t\'{e} remarquable de la PCA est que chaque $\lambda_i$ correspond \`{a} la variance empirique des donn\'{e}es projet\'{e}es sur la PC correspondante $\AAlpha_i$.\\

Dans un sc\'{e}nario d'attaque profil\'{e}e, cet outil classique est toutefois largement sous-optimal : il n'exploite pas une phase de caract\'{e}risation. Dans cette derni\`{e}re on suppose que l'attaquant est en possession d'un ensemble de donn\'{e}es étiquet'{e}es  $(\vLeakVec_i, \sensVar_i)_{i=1..\nbProfilingTraces}$, c'est-\`{a}-dire o\`{u} l'association \emph{trace-variable sensible} est connue. Dans la litt\'{e}rature SCA \cite{TAprincipal,choudaryefficient,choudary2014efficient,disassembler,Standaert2008} une version \emph{profil\'{e}e} de la PCA a \'{e}t\'{e} introduite. En introduisant les moyennes empiriques par classe
\begin{equation}\label{eq:mmmXclass}
\mmmXclass= \esperEst[\given{\vaLeakVec}{\sensRandVar = \sensVarGenValue}] = \frac{1}{\nbTracesPerClass}\sum_{i\colon \sensVar_i=\sensVarGenValue} \vLeakVec_i  \mbox{ ,}
\end{equation}
la PCA profil\'{e}e utilise la matrice des \emph{\'{e}carts inter-classes} suivante \`{a} la place de la matrice de covariance $\covmat$:
\begin{equation}\label{eq:SB}
\SB = \sum_{\sensVarGenValue\in\sensVarSet}\nbTracesPerClass(\mmmXclass-\mmmX)(\mmmXclass-\mmmX)^\intercal \mbox{ ,}
\end{equation}
o\`{u} $\mmmX$ est la moyenne empirique de toutes les donn\'{e}es confondues. L'extracteur obtenu de cette mani\`{e}re garantie que les centro\"ides par classes des donn\'{e}es projet\'{e}es sont \'{e}cart\'{e}s au maximum.


\subsubsection{L'enjeu du choix des composantes}\label{sec:ELV}
L'introduction de la PCA dans le contexte des SCA a levée les questions suivantes: \textit{combien} de composants et \textit{lesquelles} sont suffisantes/nécessaires pour réduire la dimension des traces sans perdre de l'information discriminante importante ? 
%
Une première réponse a été données dans \cite{choudary2014efficient}, en relation au concept de \emph{variance expliquée} (ou \emph{variance expliquée globale}, EGV) d'une PC $\AAlpha_i$:

\begin{equation}\label{eq:EGV}
\mathrm{EGV}(\AAlpha_i) =  \frac{\lambda_i}{\sum_{k=1}^r\lambda_k} \mbox{ .}
\end{equation}
Par définition, la somme de toutes les EGV est égal à  $1$. Le choix des composantes basé sur le EGV consiste à  fixer une valeur souhaité pour la \emph{variance expliquée cumulative} $\beta$ et à  garder $\newTraceLength$ PCs distinctes, où $\newTraceLength$ est l'entier minimum tel que: 
\begin{equation}
\mbox{EGV}(\AAlpha_1) +\mbox{EGV}(\AAlpha_2) + \dots +\mbox{EGV}(\AAlpha_\newTraceLength) \geq \beta \mbox{  .}
\end{equation}
Si l'attaquant e une contrainte pour la dimension $\newTraceLength$, le choix par EGV consiste simplement à  garder les $\newTraceLength$ premières composantes, dans l'ordre naturel des valeurs propres associées. Dans le contexte des SCA, ce type de choix ne semble pas être systématiquement le meilleur: des articles déclarent que les composantes leaders contiennent la plus grande quantité d'information, alors que d'autres suggèrent de défausser ces premières composantes \cite{Batina2012,specht}. 
% An example of this behaviour is provided in Fig.~\ref{fig:DPAcontest}. It may be noticed that the first component (plotted on the left) has high coefficients spread over the whole trace, while the sixth one  (on the right) has high coefficients localised in a small time interval, very likely to signalize the instants in which the target sensitive variable leaks.
%
%\begin{figure}
%\includegraphics[width=.45\textwidth]{figures/DPAcontestPC1_new.pdf} 
%\includegraphics[width=.45\textwidth]{figures/DPAcontestPC6_new.pdf} 
%\caption{First and sixth PCs in DPA contest v4 trace set (between time samples 198001 and 199000)}\label{fig:DPAcontest}
%\end{figure}
%
Dans le contexte des implémentations sécurisées, le but des développeurs est de minimiser les fuites, autrement dit, de réduire le nombre de points de fuite, d'où l'assomption suivante qui peut être raisonnablement faite: 

\begin{assumption}\label{assum:local}
L'information compromettante d'une trace acquis de canaux auxiliaires est localisée en peu de points. 
\end{assumption}
Sous cette assomption, les auteurs de \cite{SCAclassProbl} ont donné donnée une deuxième réponse au problème du choix des composantes: ils utilisent le \emph{ratio de participation inversé} (IPR) pour évaluer la \emph{localisation} des vecteurs propres. Le IPR est ainsi défini:
\begin{equation}
\mathrm{IPR}(\AAlpha_i) = \sum_{j=1}^\traceLength \AAlpha_i[j]^4 \mbox{ .}
\end{equation}
Les auteurs de \cite{SCAclassProbl} suggèrent de sélectionner les PCs en ordre inverse par rapport à  leur IPR. \\

%
\subsubsection{La méthode par variance expliquée locale} 
\begin{figure}
\includegraphics[width=0.5\textwidth]{figures/cumulativeELVallRectangle.pdf} 
\includegraphics[width=0.5\textwidth]{figures/cumulativeELVzoomedRectangle.pdf} 
\caption{Cumulative ELV trend of principal components. On the right a zoom of the plot on the left. Acquisition campaign on an 8-bit AVR Atmega328P.}\label{fig:ELVcumulative}
\end{figure}
%
Les méthodes de sélection par EGV et IPR sont d'une certaine façon complémentaires : la première se base uniquement sur les valeurs propres associées aux PC et ne considère pas la forme des PC elles-mêmes; la seconde ne considère pas l'information donnée pas le valeurs propres, mais seulement la distribution des coefficients des PCs. Dans ce panorama nous avons proposé une nouvelle méthode se sélection qui fait un pont entre la EGV et la IPR : nous avons introduit la \emph{variance expliquée locale} (ELV) d'une PC $\AAlpha_i$ en un point $j$, définie par:
\begin{equation}
\mathrm{ELV}(\AAlpha_i,j) = \frac{\lambda_i \AAlpha_i[j]^2}{\sum_{k=1}^r\lambda_k} = \mathrm{EGV}(\AAlpha_i) \AAlpha_i[j]^2  \mbox{ .}
\end{equation}
Soit $\mathcal{J}=\{j^i_1, j^i_2, \dots, j^i_{\traceLength}\}\subset\{1,2,\dots,\traceLength\}$  un ensemble  d'indices ordonnés de tel sort que $\mathrm{ELV}(\AAlpha_i,j^i_1)\geq \mathrm{ELV}(\AAlpha_i,j^i_2)\geq \dots \geq \mathrm{ELV}(\AAlpha_i,j^i_\traceLength)$. On observe que la somme sur tous les $\mathrm{ELV}(\AAlpha_i,j)$, pour tout $j\in[1,\dots,\traceLength],$  est égal à  $\mathrm{EGV}(\AAlpha_i)$. Si on opère cette somme en manière cumulative en suivant l'ordre donnée par l'ensemble ordonné $\mathcal{J}$, on obtient une description complète de la tendance suivie par la composante $\AAlpha_i$ pour atteindre son EGV. Comme montré in Fig.~\ref{fig:ELVcumulative}, où ces ELV cumulatives sont représentées, les trois premières composantes atteignent leur EGV finale très lentement, alors que la $4$ème, $5$ème et $6$ème atteignent une large partie de leur EGV très rapidement, c'est-à -dire an sommant les contributions ELV d'une moindre nombre de points. La sélection par ELV, en analogie avec EGV, demande de fixer une taille pour la dimension réduite du signal $\newTraceLength$, ou une seuil $\beta$ pour la ELV cumulative. Dans le premier cas les valeurs maximales de ELV de chaque PC sont comparées, et les $\newTraceLength$ montantes les valeurs plus élevées sont sélectionnées. Dans le second cas, tous les couple (PC, point temporel) sont ordonnés en ordre décroissant de ELV, et sommés jusqu'à  atteindre le seuil $\beta$. Les PC qui contribuent à  la somme sont sélectionnées.\\




\subsubsection{LDA et le probl\`{e}me du petit échantillonnage}
L'extracteur $\extract^{\mathrm{LDA}}$ est une réduction de dimension optimale, sous certaines conditions, pour résoudre un problème de classification, c'est-à -dire un problème d'apprentissage où l'on vise à  assigner à  une donnée (une trace dans notre cas) un label (donné ici par la valeur de la variable $\sensRandVar$ manipulée lors de l'acquisition). Il a pour but non seulement d'écarter les centro\"ides par classe, mais aussi de reprocher au mieux les données appartenant à  une même classe. La forte analogie entre les SCA et la tâche de classification en apprentissage automatique rend la LDA beaucoup plus adaptée de la PCA dans ce contexte, mais plus coûteux. Comme la PCA, la LDA construit une matrice de projection $\textbf{A}$ en juxtaposant des vecteurs propres, les composantes linéaires discriminantes (LDC), qui sont ceux de la matrice $\SW^{-1} \SB$, où $\SB$ est définie en \eqref{eq:SB} et $\SW$ est la matrice des \emph{écarts intra-classe}:

\begin{equation}
\SW = \sum_{\sensVarGenValue\in\sensVarSet}\sum_{i=1\colon \sensVar_i=\sensVarGenValue}(\vLeakVec_i-\mmmXclass)(\vLeakVec_i-\mmmXclass)^\intercal \mbox{.}
\end{equation}

Le désavantage principal de la LDA est appelé le \emph{problème du petit échantillonnage} et se réalise quand le nombre des traces $\nbTraces$ est inférieur ou égal à leur taille $\traceLength$. Ceci implique que la matrice $\SW$ n'est pas inversible. Si la LDA a été introduite relativement récemment dans la littérature SCA, la communauté de reconnaissance de patterns et apprentissage automatique cherche une solution à ce problème depuis les premières années quatre-vingt-dix. Nous avons exploré les solutions proposées, et les avons testés sur des traces compromettantes. 

%
\subsubsection{Résultats et conclusions}
\begin{figure}[t]
\subfigure[]{\label{fig:1.1}
\includegraphics[width=0.5\textwidth]{figures/Criterion1.pdf}}
\subfigure[]{\label{fig:1.2}
\includegraphics[width=0.5\textwidth]{figures/Criterion1Good.pdf}}
\caption{Guessing Entropy (rang moyen de la bonne hypothèse de clé) en fonction du nombre de traces d'attaque, pour différentes méthodes d'extraction. Les valeurs sont estimées en moyennant le rang sur 100  expériences indépendantes.}\label{fig:scenario1}
\end{figure}

\begin{figure}
\subfigure[]{\label{fig:direct_PCA}
\includegraphics[width=0.5\textwidth]{figures/SSS.pdf}}
\subfigure[]{\label{fig:notSSS}
\includegraphics[width=0.5\textwidth]{figures/Criterion2notSSS.pdf}}
\caption{Guessing Entropy en fonction du nombre de traces de profilage. Figure \subref{fig:direct_PCA}: la meilleure extension de LDA et la PCA par profilage en présence du problème du petit échantillonnage; Figure \subref{fig:notSSS}: une comparaison en absence de problème du petit échantillonnage.}\label{fig:scenario2}
\end{figure}

Nous avons mené diverses analyses expérimentales pour évaluer la méthode de sélection de composantes par ELV et les techniques d'extension de la LDA en présence du petit échantillonnage. Dans cette section nous reportons les plus significatifs, en les extrayant du chapitre 4 de la thèse. Nous considérons ici deux scénarios: dans le premier (Scenario 1) l'attaquant a un large nombre de trace de profilage ($\nbProfilingTraces$) et veut minimiser le nombre de traces d'attaque ($nbAttackTraces$) pour obtenir une attaque réussie. En observant la Fig.~\ref{fig:scenario1} nous pouvons tirer trois conclusions: 
\begin{itemize}
\item nous confirmons que la PCA standard est bien sous-optimale par rapport à la PCA par profilage
\item nous confirmons que la LDA est plus adaptée de la PCA pour pré-traiter le traces par canaux auxiliaires
\item en équipant la PCA profilée avec la méthode de sélection par ELV (à la place de la EGV classique) les performances de l'attaque augmentent significativement, en se rapprochant de celles obtenues à travers la LDA.
\end{itemize}
Dans le Scenario 2, l'attaquant veut au contraire minimiser le nombre de traces de profilage nécessaires pour obtenir une bonne méthode de pré-traitement et profilage. Dans ce cas, le problème du petit échantillonnage peut se vérifier. En observant la Fig.~\ref{fig:scenario2} nous concluons que encore une fois, en présence ou pas du petite échantillonnage, la PCA équipé de la sélection par ELV a performances proches de celles de la LDA ou de sa meilleure alternative en présence du petit échantillonnage.


\subsection{Analyse Discriminante par Noyau}\label{sec:kda}
Quand une contre-mesure de masquage d'ordre $(d-1)$ est mise en place, la targette $\sensRandVar$ est représentée par une $d$-uple de partie $M_i$ manipulées à différents instants temporels $t_1,\dots,t_d$ et l'information sensible réside dans le moment $\esper[\vaLeakVec[t_1]\vaLeakVec[t_2]\cdots \vaLeakVec[t_d]]$ d'ordre $d$, c'est-à-dire que la fonction  $f(z) = \esper[\vaLeakVec[t_1]\vaLeakVec[t_2]\cdots \vaLeakVec[t_d]\vert Z=z]$ n'est pas constante. Pour exploiter cette information cela a été montré  \cite{carlet2014achieving} qu'il est nécessaire de considérer une statistique des donnée qui, vu comme polynôme multivarié dans les coordonnées de $\vaLeakVec$, contient le monôme de degré $d$ $\prod_{i=1,\dots,d}\vaLeakVec[t_i]$. Comme en pratique les points $t_1,\dots,t_d$ sont inconnus à l'attaquant, une idée naïve pour obtenir un extracteur effectif et de chercher parmi les combinaisons linéaires des produits de tous les possibles $d$-uples de points. Ceci revient à immerger les données à travers une fonction non-linéaire $\Phi$ dans un espace de grand dimension $\featureSpace = \mathbb{R}^{D\choose{d}}$, et ici appliquer les méthodes d'extraction linéaires (LDA et PCA, par exemple). L'espace $\featureSpace$ est appelé dans \emph{espace des caractéristiques}, car c'est l'espace qui contient les caractéristiques informatives des traces. La croissance combinatoire de la taille de $\featureSpace$ est un obstacle d'un point de vu calculatoire, car ce demande de manipuler et stocker des traces de taille ${D\choose{d}}$. L'algorithme KDA permet à un attaquant d'utiliser l'extracteur LDA dans $\featureSpace$  sans effectuer de calculs dans l'espace $\featureSpace$, comme schématisé dans la Fig.~\ref{fig:scheme2}.\\

L'outil central de l'astuce du noyau est la \emph{fonction noyau} $K \colon \mathbb{R}^\traceLength \times \mathbb{R}^\traceLength \rightarrow \mathbb{R}$, qui doit satisfaire la propriété suivante, en rélation avec la fonction $\Phi$:

\begin{equation}\label{eq:kernelProperty}
K(\sss[]{i},\sss[]{j}) = \Phi(\sss[]{i})\cdot \Phi(\sss[]{j}) \mbox{ ,}
\end{equation}
pour tout $i,j= 1,\dots, \NPoI$, où $\cdot$ est le produit scalaire.

Toute fonction $\Phi$ a une fonction noyau associée, donnée \eqref{eq:kernelProperty}, pour un ensemble de données. Au contraire, seulement les fonctions $K\colon\mathbb{R}^D\times \mathbb{R}^D \rightarrow \mathbb{R}$ qui satisfont une condition de convergence connue comme {\em condition de Mercer} sont associées à une map $\Phi:\mathbb{R}^D \rightarrow \mathbb{R}^F$, pour quelque $F$. Les fonctions noyau éligibles pour une astuce du noyau sont celles qui sont calculables directement à partir des données brutes $\vLeakVec_i$, sans évaluer la fonction $\Phi$. \\
%
%The notion of kernel function is illustrated in the following example.
%
%\begin{example}\label{ex:polyKernel}
%Let $D=2$. Consider the function
%\begin{align}
%&K\colon\mathbb{R}^2\times \mathbb{R}^2 \rightarrow \mathbb{R} \nonumber \\ 
%&K\colon(\sss[]{i},\sss[]{j}) \mapsto ( \sss[]{i}\cdot \sss[]{j})^2 \mbox{ ,} \label{eq:example1}
%\end{align}
%
%After defining $\sss[]{i} = [a,b]$ and $\sss[]{j} = [c,d]$, we get the following development of K:
%\begin{equation}
%K(\sss[]{i},\sss[]{j}) = (ac + bd)^2 = a^2c^2 + 2abcd + b^2d^2 \mbox{ ,}
%\end{equation}
%
%which is associated to the following map from $\Bbb{R}^2$ to $\Bbb{R}^3$:
%
%\begin{equation}
%\Phi(u,v) =  [u^2, \sqrt{2}uv, v^2]
%\end{equation}
%
%Indeed $\Phi(\sss[]{i})\cdot \Phi(\sss[]{j}) = a^2c^2 + 2abcd + b^2d^2 = K(\sss[]{i},\sss[]{j})$\enspace. This means that to compute the dot product between some data mapped into the $3$-dimensional space $\featureSpace$ there is no need to apply $\Phi$: applying $K$ over the $2$-dimensional space is equivalent. 
%
%\end{example}
%
%
\begin{figure}
\centering
{
\begin{tikzpicture}
\matrix (m) [matrix of math nodes, row sep=3em,
column sep=3em, text height=1.5ex, text depth=0.25ex]
{ \mathbb{R}^\traceLength & \featureSpace & \mathbb{R}^\newTraceLength \\};
\path[->]
(m-1-1) edge node[above] {$\Phi$} (m-1-2);
         %edge [bend left=30] (m-2-2)
         %edge [bend right=15] (m-2-2);
\path[->]
($(m-1-2.north east)-(0,0.1)$) edge node[above] {$\extract^{\mathrm{PCA}}$} ($(m-1-3.north west)-(0,0.1)$);
\path[->]
($(m-1-2.south east)+(0,0.15)$) edge node[below] {$\extract^{\mathrm{LDA}}$} ($(m-1-3.south west)+(0,0.15)$);

\path[->]
(m-1-1) edge [bend left=50] node[above] {$\extract^{\mathrm{KPCA}}$} (m-1-3)
(m-1-1) edge [bend right=50] node[below] {$\extract^{\mathrm{KDA}}$} (m-1-3);

\end{tikzpicture} 
}
\caption{Applying KDA and KPCA permits to by-pass computations in $\featureSpace$.}\label{fig:scheme2}
\end{figure}
%
La fonction noyau \emph{polynomiale de degré $d$} fait partie de ce type de fonctions. Elle est ainsi définie: 
\begin{equation}
K(\vLeakVec_i,\vLeakVec_i) = (\vLeakVec_i \cdot \vLeakVec_j)^d \mbox{ ,}
\end{equation}
et corresponde à la fonction $\Phi$ qui porte les coordonnées en entrée dans l'espace des caractéristiques qui contient tous les possibles monômes de degré $d$ dans ces coordonnées, à constants près. Elle est donc, à constants près, la fonction $\Phi$ du schéma de Fig.~\ref{fig:scheme2}. L'algorithme pour obtenir $\extract^\mathrm{KDA}$ est le suivant.
%
\subsubsection*{KDA pour traces masquées à l'ordre $d$}\label{procedure:KDA}
Étant donné un ensemble de traces de profilage $(\vLeakVec{i},\sensVar_i)_{i=1,\dots, \nbTrainingTraces}$ et la fonction noyau $K(\vLeakVec,\yyy)= (\vLeakVec\cdot \yyy)^d$:
\begin{itemize}
\item[1)] Construire une matrice $\MMM$ (elle agit comme \emph{matrice des écarts inter-classe}):

\begin{equation}
\MMM = \sum_{\sensVar\in\sensVarSet}\numTraces(\MMMclass - \MMMT)(\MMMclass-\MMMT)^\intercal\mbox{ ,}
\end{equation}

où $\MMMclass$ et $\MMMT$ sont deux vecteurs colonne de taille $N$, avec coordonnées données par:
\begin{align}
\MMMclass[\sensVarGenValue][j] = \frac{1}{\nbTracesPerClass}\sum_{i:\sensVar_i=\sensVarGenValue}K(\vLeakVec_j,\vLeakVec_i)\\
\MMMT[j] = \frac{1}{\nbTrainingTraces}\sum_{i=1}^{\nbTrainingTraces}K(\vLeakVec_{j},\vLeakVec_{i}) \mbox{ .}
\end{align}

\item[2)] Construire une matrice $\NNN$ (elle agit comme \emph{matrice des écarts intra-classe}):
\begin{equation}\label{eq:N}
\NNN = \sum_{\sensVarGenValue\in\sensVarSet}\kernelMatrix_\sensVarGenValue(\III - \III_{\nbTracesPerClass})\kernelMatrix_\sensVarGenValue^\intercal\mbox{ ,}
\end{equation}
où $\III$ est la matrice identité de taille $\nbTracesPerClass \times \nbTracesPerClass$, $\III_{\nbTracesPerClass}$ est la matrice de taille $\nbTracesPerClass \times \nbTracesPerClass$ avec tous les éléments égals à $\frac{1}{\nbTracesPerClass}$ et $\kernelMatrix_{\sensVarGenValue}$ est la sous-matrice de $\kernelMatrix = (K(\vLeakVec_i,\vLeakVec_j))_{\substack{i=1,\dots,\nbTrainingTraces \\ j=1,\dots,\nbTrainingTraces}}$ de taille $\nbTrainingTraces \times \nbTracesPerClass$ qui contients toutes les colonnes indexées par les $i$ tels que $\sensVar_i=\sensVarGenValue$. 

\item[3)] Régulariser la matrice $\NNN$ pour garantir la stabilité calculatoire et gérer le \emph{surapprentissage}:
\begin{equation}\label{eq:mu}
\NNN = \NNN + \mu  \III
\end{equation}

\item[4)] Trouver les valeurs propres non nulles $\lambda_1, \dots, \lambda_\numEigenvectors$  de $\NNN^{-1}\MMM$ et le vecteurs propres correspondant $\nununu_1, \dots, \nununu_\numEigenvectors$; 


\item[5)] Finalement, la projection d'une nouvelle trace $\vLeakVec$ sur la $\ell$-ième composante discriminante d'ordre $d$ se calcule ainsi:
\begin{equation}\label{eq:projectionKDA}
\extract^{\mathrm{KDA}}_{\ell}(\vLeakVec) = \sum_{i=1}^{\nbTrainingTraces}{\nununu}_\ell[i]K(\vLeakVec_i, \vLeakVec) \mbox{ .}
\end{equation} 

\end{itemize}

\subsubsection{Analyse and résultats}
\begin{figure}[t]
\subfigure[]{\label{fig:numClasses-2order}
\includegraphics[width=.5\textwidth]{figures/2order_classes_TA.pdf}}
\subfigure[]{\label{fig:numClasses-3order}
\includegraphics[width=.5\textwidth]{figures/3order_new.pdf}}
\caption{Comparaison entre KDA sous 2 classes, 3 classes, 9 classes and 256 classes KDA dans un contexte d'attaque de $2$nd ordre \subref{fig:numClasses-2order} et de $3$ème ordre \subref{fig:numClasses-3order}. Pour le $2$nd ordre, la KDA est efficace à dissocier les données en 256 classes, permettant une caractérisation optimale. Pour le $3$ème ordre les données de profilage ne sont pas en nombre suffisant pour réussir une phase de caractérisation en 256 classes distinguées. Diminuer le nombre de classes à dissocier, fait améliorer l'efficacité du pré-traitement et, en conséquence, de l'attaque. }\label{fig:numClasses}
\end{figure}

Le premier aspect de l'application de la KDA sur lequel nous nous sommes concentré est l'importance d'une bonne régularisation (obtenue par le choix du paramètre$\mu$ in \eqref{eq:N}) : cette régularisation est la réponse au fait que les méthodes par noyau sont souvent porté au surapprentissage: ceci signifie, dans le cas de la KDA, que $\extract^{\mathrm{KDA}}$ risque de grouper parfaitement les traces d'apprentissage dans leur classe, mais de ne pas réussir à dissocier les traces d'attaque. La régularisation consiste à rajouter une contrainte à la phase d'apprentissage, dans le but de créer un modèle qui ait le plus possible le même comportement avec les traces d'apprentissage et les nouvelles données sur lequel s'applique. Nous avons observé que le extracteurs issus d'une bonne régularisation tiraient leurs projections de zones des traces très localisées, ce qui est encore signe (comme pour les extracteurs linéaires) d'une bonne détection implicite des PoIs. 

%
Le deuxième aspect sur lequel nous nous sommes concentré est le rôle de la forme de la targette dans le compromis précision-efficacité. Du fait que la complexité calculatoire de la KDA est $O(\nbTrainingTraces^3)$, avec $\nbTrainingTraces$ est le nombre des traces d'apprentissage,  il peut être intéressant de diminuer ce nombre $\nbTrainingTraces$ pour augmenter l'efficacité du calcul. Cependant, borner $\nbTrainingTraces$ réduit la précision de la KDA. Une manière de pallier à cette perte de précision est de réduire le nombre de classes à dissocier, en choisissant comme targette une fonction non-injective d'une variable interne du calcul cryptographique $m(\sensRandVar)$. Par example, les valeurs de $\sensRandVar$ peuvent être groupés selon leur poids d'Hamming: un modèle à 2 classes est donné par ($m(z) =0$ if $\HW(z)<4$, $m(z) =1$ if $\HW(z)\geq4$). Une fois réalisé un pré-traitement basé sur un certain modèle non-injectif, il parait adéquat réaliser l'attaque en visant la même targette. Ayant fixé le nombre $\nbTrainingTraces$ pour les traces d'apprentissage, les résultats de ce type d'attaque sont montrés en Fig.~\ref{fig:numClasses} : si dans un contexte d'attaque de $2$nd ordre, on peut observer que la KDA s'entraîne sur un nombre suffisant de données pour réussir une séparation en 256 classes, pour des contextes d'ordre supérieur la conversion à un problème à 2 classes devient une meilleure stratégie.

\begin{figure}
\includegraphics[width=.5\textwidth]{figures/3order_2_9.pdf} 
\includegraphics[width=.5\textwidth]{figures/4order_2_9.pdf} 
\caption{Gauche: guessing entropy  pour des attaques \emph{template} d'ordre 3, à 2 classes et à 3 classes. Droite: rang de la bonne clé pour une attaque d'ordre 4 à 2 classes et à 9 classes.}\label{fig:3-4}
\end{figure}

%Third, I evaluated the soundness of an asymmetric preprocessing-attack approach. The success of a 2-class extractor always relies on a good exploitation of the PoIs, whose position does not depend on the chosen target model. For this reason, even if an extractor has been trained to separate $W$ classes, it does not mean that a finest characterization is useless. Results depicted in Fig.~\ref{fig:3-4} come from this asymmetric approach: the extractors used have been trained over $2$ classes, but the attacks that exploit a $9$-class characterization are more efficient.  
%
%Finally, I effectuated a comparison between the KDA and the PP approach. In particular I compared their performances under a fixed training set size, concluding that the effectiveness of the KDA is much less affected by the increase of the order $d$ than the PP. Indeed, the latter failed the PoI detection at orders 3 and 4.


\subsection{R\'{e}seau Neuronal Convolutif}\label{sec:cnn}